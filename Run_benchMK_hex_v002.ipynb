{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f621791-a034-4ae7-adff-d317fc74a95c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c322b0-c3b4-4afd-a133-853108c19d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages']\n",
      "5.29.5\n",
      "/opt/conda/lib/python3.10/site-packages/google/protobuf/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import google.protobuf\n",
    "print(google.protobuf.__version__)\n",
    "print(google.protobuf.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d23990-6c8a-479c-acd6-09682290c3d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.10/site-packages (1.96.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59010d48-0323-4f97-8b4c-8813cb2dbd10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U google-cloud-aiplatform tensorflow \n",
    "# !pip install --upgrade google-cloud-aiplatform google-api-core protobuf grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8e2175-8417-4455-b0f4-331c175862d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 16:33:36.539138: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-15 16:33:36.547056: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-15 16:33:36.564659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752597216.594528   50959 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752597216.602960   50959 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752597216.623829   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752597216.623864   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752597216.623867   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752597216.623870   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-15 16:33:36.632015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-9273242a-17cc-4036-9dc3-0ff15e095d65\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"europe-west4\" #\"us-south1\" #\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = LOCATION # \"us-south1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c29f83-504a-4ae3-883b-c155bc92a3e3",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed657dc-3152-40b9-a555-b9b00120cf38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Endpoint initialized: <google.cloud.aiplatform.models.Endpoint object at 0x7f8ea7219a20> \n",
      "resource name: projects/87995179092/locations/europe-west4/endpoints/8393005462395551744\n",
      "🌐 DNS: 8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog\n",
      "📋 Resource: projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive TPU Endpoint Benchmark Suite\n",
    "Based on working TPU endpoint code with full benchmarking capabilities\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, List, Dict, Union, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Google Cloud imports\n",
    "import google.auth\n",
    "import openai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Configuration - Set your actual values here\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-project-id\")\n",
    "REGION = \"europe-west4\"\n",
    "endpoint_name = \"8393005462395551744\"\n",
    "\n",
    "# Initialize endpoint\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "use_dedicated_endpoint = True\n",
    "\n",
    "print('🔧 Endpoint initialized:', endpoint)\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoint.gca_resource.dedicated_endpoint_dns\n",
    "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
    "    PROJECT_ID, REGION, endpoint.name\n",
    ")\n",
    "print(f\"🌐 DNS: {DEDICATED_ENDPOINT_DNS}\")\n",
    "print(f\"📋 Resource: {ENDPOINT_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827ca80-21f7-4e80-8ac2-8364e1c629fd",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de733ed-07e9-4d3f-93a9-75880becf0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 TPU Endpoint Benchmark Suite - Examples\n",
      "\n",
      "This benchmark suite uses your exact working TPU endpoint code pattern with comprehensive performance analysis.\n",
      "\n",
      "# Basic Examples:\n",
      "\n",
      "1. Simple Test (like your original):\n",
      "   run_simple_test(\"Write a poem about AI\", max_tokens=200, temperature=0.8)\n",
      "\n",
      "2. Token Length Experiments:\n",
      "   run_token_length_experiment(\"small\")     # 250 tokens\n",
      "   run_token_length_experiment(\"medium\")    # 465 tokens\n",
      "   run_token_length_experiment(\"large\")     # 1500 tokens\n",
      "   run_token_length_experiment(\"xlarge\")    # 3250 tokens\n",
      "\n",
      "3. Comprehensive Study:\n",
      "   run_comprehensive_token_length_study(model_name=\"llama3.3_tpuv6e\")\n",
      "\n",
      "4. Quick Test (reduced sizes):\n",
      "   quick_token_length_test(model_name=\"llama3.3_tpuv6e\")\n",
      "\n",
      "5. Custom Benchmark:\n",
      "   run_custom_benchmark(input_tokens=1000, output_tokens=500, num_requests=50, concurrency=5)\n",
      "\n",
      "6. Throughput Scaling Test:\n",
      "   run_throughput_scaling_test(base_concurrency=5, max_concurrency=50, step=5)\n",
      "\n",
      "🔧 Current Configuration:\n",
      "   📋 Project: tpu-launchpad-playground\n",
      "   🌍 Region: europe-west4\n",
      "   🖥️ Endpoint: 8393005462395551744\n",
      "   🔗 DNS: 8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog\n",
      "\n",
      "🎯 Running Demo Test...\n",
      "🚀 Running Simple Test\n",
      "   📝 Prompt: Explain the benefits of TPU for machine learning in 3 sentences.\n",
      "   🎯 Max tokens: 100\n",
      "   🌡️ Temperature: 0.7\n",
      "   🔄 Stream: True\n",
      "------------------------------------------------------------\n",
      "📡 Streaming response:\n",
      "----------------------------------------\n",
      "⚡ TTFT: 0.464s\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "📊 Performance Metrics:\n",
      "----------------------------------------\n",
      "✅ Request completed successfully\n",
      "⏱️  E2E Latency: 2.744s\n",
      "⚡ TTFT: 0.464s\n",
      "🔄 Average TPOT: 0.023s\n",
      "📈 Min/Max ITL: 0.012s / 0.030s\n",
      "🚀 Tokens/second: 36.45\n",
      "🔢 Total tokens: 100\n",
      "📏 Total characters: 100\n",
      "💾 Usage: CompletionUsage(completion_tokens=100, prompt_tokens=50, total_tokens=150, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))\n",
      "✅ Demo test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkRequest:\n",
    "    \"\"\"Request data structure for benchmarking\"\"\"\n",
    "    prompt: str\n",
    "    prompt_len: int\n",
    "    expected_output_len: int\n",
    "    request_id: int\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Result of a single benchmark request\"\"\"\n",
    "    request_id: int\n",
    "    success: bool\n",
    "    prompt_len: int\n",
    "    output_len: int\n",
    "    ttft: float  # Time to first token\n",
    "    tpot: float  # Time per output token\n",
    "    itl: float   # Inter-token latency\n",
    "    e2e_latency: float  # End-to-end latency\n",
    "    error_msg: str = \"\"\n",
    "    timestamp: float = 0.0\n",
    "    full_response: str = \"\"\n",
    "\n",
    "class RandomDataset:\n",
    "    \"\"\"Generate random prompts for benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int, range_ratio: float = 0.0):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "        self.range_ratio = range_ratio\n",
    "        \n",
    "    def _generate_random_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate a random prompt of specified length\"\"\"\n",
    "        words = [\n",
    "            \"analyze\", \"consider\", \"evaluate\", \"examine\", \"investigate\", \"review\", \"assess\", \"study\",\n",
    "            \"business\", \"technology\", \"strategy\", \"development\", \"innovation\", \"implementation\", \"solution\",\n",
    "            \"market\", \"customer\", \"product\", \"service\", \"quality\", \"performance\", \"efficiency\", \"growth\",\n",
    "            \"data\", \"information\", \"process\", \"system\", \"method\", \"approach\", \"framework\", \"model\",\n",
    "            \"challenge\", \"opportunity\", \"risk\", \"benefit\", \"advantage\", \"improvement\", \"optimization\",\n",
    "            \"research\", \"analysis\", \"report\", \"recommendation\", \"conclusion\", \"insight\", \"finding\",\n",
    "            \"artificial\", \"intelligence\", \"machine\", \"learning\", \"neural\", \"network\", \"algorithm\",\n",
    "            \"compute\", \"processor\", \"memory\", \"storage\", \"bandwidth\", \"latency\", \"throughput\"\n",
    "        ]\n",
    "        \n",
    "        prompt_words = []\n",
    "        target_words = int(length * 0.75)  # Rough token-to-word conversion\n",
    "        \n",
    "        while len(prompt_words) < target_words:\n",
    "            prompt_words.append(random.choice(words))\n",
    "        \n",
    "        # Add a question or instruction to make it more realistic\n",
    "        prompt_base = \" \".join(prompt_words)\n",
    "        prompts = [\n",
    "            f\"Explain the following concepts in detail: {prompt_base}\",\n",
    "            f\"Write a comprehensive analysis of: {prompt_base}\",\n",
    "            f\"Describe the relationship between: {prompt_base}\",\n",
    "            f\"Provide insights about: {prompt_base}\",\n",
    "            f\"Create a detailed report on: {prompt_base}\"\n",
    "        ]\n",
    "        \n",
    "        return random.choice(prompts)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate benchmark requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            if self.range_ratio > 0:\n",
    "                input_variance = int(self.input_len * self.range_ratio)\n",
    "                output_variance = int(self.output_len * self.range_ratio)\n",
    "                \n",
    "                actual_input_len = random.randint(\n",
    "                    max(1, self.input_len - input_variance),\n",
    "                    self.input_len + input_variance\n",
    "                )\n",
    "                actual_output_len = random.randint(\n",
    "                    max(1, self.output_len - output_variance),\n",
    "                    self.output_len + output_variance\n",
    "                )\n",
    "            else:\n",
    "                actual_input_len = self.input_len\n",
    "                actual_output_len = self.output_len\n",
    "            \n",
    "            prompt = self._generate_random_prompt(actual_input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=actual_input_len,\n",
    "                expected_output_len=actual_output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class TPUBenchmarkEngine:\n",
    "    \"\"\"Benchmark engine using your exact working TPU endpoint pattern\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_lock = threading.Lock()\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "        \n",
    "        # Initialize authentication using your exact pattern\n",
    "        self.creds, self.project = google.auth.default()\n",
    "        self.auth_req = google.auth.transport.requests.Request()\n",
    "        \n",
    "        # Setup BASE_URL using your exact logic\n",
    "        self.BASE_URL = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "        \n",
    "        if use_dedicated_endpoint:\n",
    "            self.BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "        \n",
    "        print(f\"🔗 Base URL: {self.BASE_URL}\")\n",
    "    \n",
    "    def _refresh_auth(self):\n",
    "        \"\"\"Refresh authentication token\"\"\"\n",
    "        self.creds.refresh(self.auth_req)\n",
    "    \n",
    "    def _make_streaming_request(self, request: BenchmarkRequest, \n",
    "                              temperature: float = 0.7, \n",
    "                              max_tokens: int = None,\n",
    "                              stream: bool = True) -> BenchmarkResult:\n",
    "        \"\"\"Make a streaming request using your exact working pattern\"\"\"\n",
    "        \n",
    "        if max_tokens is None:\n",
    "            max_tokens = request.expected_output_len + 50\n",
    "        \n",
    "        try:\n",
    "            # Refresh auth token\n",
    "            self._refresh_auth()\n",
    "            \n",
    "            # Create OpenAI client using your exact setup\n",
    "            client = openai.OpenAI(base_url=self.BASE_URL, api_key=self.creds.token)\n",
    "            \n",
    "            # Start timing\n",
    "            request_start = time.time()\n",
    "            ttft = None\n",
    "            last_token_time = request_start\n",
    "            inter_token_latencies = []\n",
    "            \n",
    "            # Make request using your exact model request pattern\n",
    "            model_response = client.chat.completions.create(\n",
    "                model=\"\",  # Your exact model parameter\n",
    "                messages=[{\"role\": \"user\", \"content\": request.prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "            )\n",
    "            \n",
    "            # Process streaming response using your exact logic\n",
    "            if stream:\n",
    "                usage = None\n",
    "                contents = []\n",
    "                token_count = 0\n",
    "                \n",
    "                for chunk in model_response:\n",
    "                    current_time = time.time()\n",
    "                    \n",
    "                    if chunk.usage is not None:\n",
    "                        usage = chunk.usage\n",
    "                        continue\n",
    "                    \n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    if content:  # Only process if there's actual content\n",
    "                        # Timing measurements using your exact pattern\n",
    "                        if ttft is None:\n",
    "                            ttft = current_time - request_start\n",
    "                        else:\n",
    "                            itl = current_time - last_token_time\n",
    "                            inter_token_latencies.append(itl)\n",
    "                        \n",
    "                        contents.append(content)\n",
    "                        token_count += 1\n",
    "                        last_token_time = current_time\n",
    "                \n",
    "                # Final measurements\n",
    "                e2e_latency = time.time() - request_start\n",
    "                full_text = ''.join(contents)\n",
    "                \n",
    "                # Calculate TPOT\n",
    "                if inter_token_latencies:\n",
    "                    avg_tpot = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "                    avg_itl = avg_tpot\n",
    "                else:\n",
    "                    avg_tpot = e2e_latency / max(1, token_count) if token_count > 0 else 0\n",
    "                    avg_itl = avg_tpot\n",
    "                \n",
    "                return BenchmarkResult(\n",
    "                    request_id=request.request_id,\n",
    "                    success=True,\n",
    "                    prompt_len=len(request.prompt.split()) * 1.3,  # Rough token estimate\n",
    "                    output_len=token_count,\n",
    "                    ttft=ttft if ttft else e2e_latency,\n",
    "                    tpot=avg_tpot,\n",
    "                    itl=avg_itl,\n",
    "                    e2e_latency=e2e_latency,\n",
    "                    timestamp=request_start,\n",
    "                    full_response=full_text\n",
    "                )\n",
    "            else:\n",
    "                # Non-streaming response\n",
    "                e2e_latency = time.time() - request_start\n",
    "                response_text = model_response.choices[0].message.content\n",
    "                token_count = len(response_text.split()) * 1.3  # Rough estimate\n",
    "                \n",
    "                # Estimate TTFT and TPOT for non-streaming\n",
    "                estimated_ttft = e2e_latency * 0.2  # 20% for processing\n",
    "                estimated_tpot = (e2e_latency - estimated_ttft) / max(1, token_count)\n",
    "                \n",
    "                return BenchmarkResult(\n",
    "                    request_id=request.request_id,\n",
    "                    success=True,\n",
    "                    prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                    output_len=int(token_count),\n",
    "                    ttft=estimated_ttft,\n",
    "                    tpot=estimated_tpot,\n",
    "                    itl=estimated_tpot,\n",
    "                    e2e_latency=e2e_latency,\n",
    "                    timestamp=request_start,\n",
    "                    full_response=response_text\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - request_start if 'request_start' in locals() else 0\n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=False,\n",
    "                prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                output_len=0,\n",
    "                ttft=0.0,\n",
    "                tpot=0.0,\n",
    "                itl=0.0,\n",
    "                e2e_latency=error_time,\n",
    "                error_msg=str(e),\n",
    "                timestamp=time.time(),\n",
    "                full_response=\"\"\n",
    "            )\n",
    "    \n",
    "    def run_benchmark(self, \n",
    "                      requests: List[BenchmarkRequest],\n",
    "                      max_concurrency: int = 100,\n",
    "                      temperature: float = 0.7,\n",
    "                      max_tokens: int = None,\n",
    "                      stream: bool = True,\n",
    "                      request_rate: float = float('inf')) -> List[BenchmarkResult]:\n",
    "        \"\"\"Run benchmark with specified parameters\"\"\"\n",
    "        \n",
    "        print(f\"🚀 Starting benchmark with {len(requests)} requests...\")\n",
    "        print(f\"👥 Max concurrency: {max_concurrency}\")\n",
    "        print(f\"🌡️ Temperature: {temperature}\")\n",
    "        print(f\"🔄 Streaming: {stream}\")\n",
    "        \n",
    "        self.results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Rate limiting setup\n",
    "        if request_rate != float('inf'):\n",
    "            request_interval = 1.0 / request_rate\n",
    "            print(f\"⏱️ Request rate limit: {request_rate} req/s\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "            future_to_request = {}\n",
    "            \n",
    "            for i, req in enumerate(requests):\n",
    "                # Rate limiting\n",
    "                if request_rate != float('inf') and i > 0:\n",
    "                    time.sleep(request_interval)\n",
    "                \n",
    "                future = executor.submit(\n",
    "                    self._make_streaming_request, \n",
    "                    req, \n",
    "                    temperature, \n",
    "                    max_tokens, \n",
    "                    stream\n",
    "                )\n",
    "                future_to_request[future] = req\n",
    "            \n",
    "            with tqdm(total=len(requests), desc=\"📊 Processing requests\") as pbar:\n",
    "                for future in as_completed(future_to_request):\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        with self.results_lock:\n",
    "                            self.results.append(result)\n",
    "                    except Exception as e:\n",
    "                        request = future_to_request[future]\n",
    "                        error_result = BenchmarkResult(\n",
    "                            request_id=request.request_id,\n",
    "                            success=False,\n",
    "                            prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                            output_len=0,\n",
    "                            ttft=0.0,\n",
    "                            tpot=0.0,\n",
    "                            itl=0.0,\n",
    "                            e2e_latency=0.0,\n",
    "                            error_msg=f\"Future execution failed: {str(e)}\",\n",
    "                            timestamp=time.time(),\n",
    "                            full_response=\"\"\n",
    "                        )\n",
    "                        with self.results_lock:\n",
    "                            self.results.append(error_result)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        successful_count = len([r for r in self.results if r.success])\n",
    "        print(f\"✅ Benchmark completed in {total_time:.2f} seconds\")\n",
    "        print(f\"📈 Success rate: {successful_count}/{len(requests)} ({successful_count/len(requests)*100:.1f}%)\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "class BenchmarkAnalyzer:\n",
    "    \"\"\"Analyze and report benchmark results\"\"\"\n",
    "    \n",
    "    def __init__(self, results: List[BenchmarkResult]):\n",
    "        self.results = results\n",
    "        self.successful_results = [r for r in results if r.success]\n",
    "        self.failed_results = [r for r in results if not r.success]\n",
    "    \n",
    "    def calculate_percentiles(self, values: List[float], percentiles: List[int]) -> Dict[int, float]:\n",
    "        \"\"\"Calculate percentiles for a list of values\"\"\"\n",
    "        if not values:\n",
    "            return {p: 0.0 for p in percentiles}\n",
    "        return {p: np.percentile(values, p) for p in percentiles}\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate benchmark summary statistics\"\"\"\n",
    "        if not self.successful_results:\n",
    "            return {\n",
    "                \"error\": \"No successful requests\",\n",
    "                \"total_requests\": len(self.results),\n",
    "                \"failed_requests\": len(self.failed_results)\n",
    "            }\n",
    "        \n",
    "        ttfts = [r.ttft for r in self.successful_results]\n",
    "        tpots = [r.tpot * 1000 for r in self.successful_results]  # Convert to ms\n",
    "        e2e_latencies = [r.e2e_latency for r in self.successful_results]\n",
    "        \n",
    "        total_input_tokens = sum(r.prompt_len for r in self.successful_results)\n",
    "        total_output_tokens = sum(r.output_len for r in self.successful_results)\n",
    "        \n",
    "        if self.successful_results:\n",
    "            timestamps = [r.timestamp for r in self.successful_results]\n",
    "            benchmark_duration = max(timestamps) - min(timestamps) + max(e2e_latencies)\n",
    "        else:\n",
    "            benchmark_duration = 1.0\n",
    "        \n",
    "        request_throughput = len(self.successful_results) / benchmark_duration\n",
    "        input_token_throughput = total_input_tokens / benchmark_duration\n",
    "        output_token_throughput = total_output_tokens / benchmark_duration\n",
    "        overall_token_throughput = (total_input_tokens + total_output_tokens) / benchmark_duration\n",
    "        \n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        \n",
    "        summary = {\n",
    "            \"successful_requests\": len(self.successful_results),\n",
    "            \"failed_requests\": len(self.failed_results),\n",
    "            \"total_requests\": len(self.results),\n",
    "            \"benchmark_duration\": benchmark_duration,\n",
    "            \"request_throughput\": request_throughput,\n",
    "            \"input_token_throughput\": input_token_throughput,\n",
    "            \"output_token_throughput\": output_token_throughput,\n",
    "            \"overall_token_throughput\": overall_token_throughput,\n",
    "            \"total_input_tokens\": total_input_tokens,\n",
    "            \"total_output_tokens\": total_output_tokens,\n",
    "            \"ttft_percentiles\": self.calculate_percentiles(ttfts, percentiles),\n",
    "            \"tpot_percentiles\": self.calculate_percentiles(tpots, percentiles),\n",
    "            \"e2e_latency_percentiles\": self.calculate_percentiles(e2e_latencies, percentiles)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_detailed_summary(self):\n",
    "        \"\"\"Print detailed performance summary\"\"\"\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            print(f\"❌ Error: {summary['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 DETAILED PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\n📋 REQUEST STATISTICS:\")\n",
    "        print(f\"   ✅ Successful: {summary['successful_requests']:,}\")\n",
    "        print(f\"   ❌ Failed: {summary['failed_requests']:,}\")\n",
    "        print(f\"   📊 Success Rate: {summary['successful_requests']/summary['total_requests']*100:.1f}%\")\n",
    "        print(f\"   ⏱️ Duration: {summary['benchmark_duration']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n⚡ LATENCY METRICS:\")\n",
    "        print(f\"   🚀 TTFT p50: {summary['ttft_percentiles'][50]*1000:.1f}ms\")\n",
    "        print(f\"   🚀 TTFT p95: {summary['ttft_percentiles'][95]*1000:.1f}ms\")\n",
    "        print(f\"   🚀 TTFT p99: {summary['ttft_percentiles'][99]*1000:.1f}ms\")\n",
    "        \n",
    "        print(f\"\\n🔄 TIME PER OUTPUT TOKEN:\")\n",
    "        print(f\"   ⚡ TPOT p50: {summary['tpot_percentiles'][50]:.1f}ms\")\n",
    "        print(f\"   ⚡ TPOT p95: {summary['tpot_percentiles'][95]:.1f}ms\")\n",
    "        print(f\"   ⚡ TPOT p99: {summary['tpot_percentiles'][99]:.1f}ms\")\n",
    "        \n",
    "        print(f\"\\n⏱️ END-TO-END LATENCY:\")\n",
    "        print(f\"   📈 E2E p50: {summary['e2e_latency_percentiles'][50]:.2f}s\")\n",
    "        print(f\"   📈 E2E p95: {summary['e2e_latency_percentiles'][95]:.2f}s\")\n",
    "        print(f\"   📈 E2E p99: {summary['e2e_latency_percentiles'][99]:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n🚀 THROUGHPUT METRICS:\")\n",
    "        print(f\"   📊 Requests/sec: {summary['request_throughput']:.2f}\")\n",
    "        print(f\"   📤 Output tokens/sec: {summary['output_token_throughput']:.2f}\")\n",
    "        print(f\"   📊 Overall tokens/sec: {summary['overall_token_throughput']:.0f}\")\n",
    "        \n",
    "        print(f\"\\n📊 TOKEN STATISTICS:\")\n",
    "        print(f\"   📥 Total input tokens: {summary['total_input_tokens']:,}\")\n",
    "        print(f\"   📤 Total output tokens: {summary['total_output_tokens']:,}\")\n",
    "        print(f\"   📊 Avg input/request: {summary['total_input_tokens']/summary['successful_requests']:.1f}\")\n",
    "        print(f\"   📊 Avg output/request: {summary['total_output_tokens']/summary['successful_requests']:.1f}\")\n",
    "\n",
    "# Token Length Test Configurations\n",
    "TOKEN_LENGTH_EXPERIMENTS = {\n",
    "    \"small\": {\n",
    "        \"name\": \"Token Length - Small(250)\",\n",
    "        \"input_tokens\": 250,\n",
    "        \"output_tokens\": 250,\n",
    "        \"total_tokens\": 500,\n",
    "        \"concurrency\": 250,\n",
    "        \"num_requests\": 2000,\n",
    "        \"description\": \"Small token length test\"\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"name\": \"Token Length - Medium(350 - 580)\",\n",
    "        \"input_tokens\": 465,  # Average of 350-580\n",
    "        \"output_tokens\": 465,\n",
    "        \"total_tokens\": 930,\n",
    "        \"concurrency\": 150,\n",
    "        \"num_requests\": 1500,\n",
    "        \"description\": \"Medium token length test\"\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"name\": \"Token Length - Large(1200 - 1800)\",\n",
    "        \"input_tokens\": 1500,  # Average of 1200-1800\n",
    "        \"output_tokens\": 1500,\n",
    "        \"total_tokens\": 3000,\n",
    "        \"concurrency\": 100,\n",
    "        \"num_requests\": 1000,\n",
    "        \"description\": \"Large token length test\"\n",
    "    },\n",
    "    \"xlarge\": {\n",
    "        \"name\": \"Token Length - Xlarge(2.5k - 4k)\",\n",
    "        \"input_tokens\": 3250,  # Average of 2.5k-4k\n",
    "        \"output_tokens\": 1000,  # Reasonable output for very long input\n",
    "        \"total_tokens\": 4250,\n",
    "        \"concurrency\": 30,\n",
    "        \"num_requests\": 500,\n",
    "        \"description\": \"Extra large token length test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_simple_test(prompt: str = \"Write a short poem about artificial intelligence\",\n",
    "                   max_tokens: int = 200,\n",
    "                   temperature: float = 0.8,\n",
    "                   stream: bool = True):\n",
    "    \"\"\"Run a simple test using your exact working pattern\"\"\"\n",
    "    \n",
    "    print(f\"🚀 Running Simple Test\")\n",
    "    print(f\"   📝 Prompt: {prompt}\")\n",
    "    print(f\"   🎯 Max tokens: {max_tokens}\")\n",
    "    print(f\"   🌡️ Temperature: {temperature}\")\n",
    "    print(f\"   🔄 Stream: {stream}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Use your exact authentication setup\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Use your exact BASE_URL setup\n",
    "    BASE_URL = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "    \n",
    "    if use_dedicated_endpoint:\n",
    "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "    \n",
    "    # Use your exact client setup\n",
    "    client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
    "    \n",
    "    # Start timing\n",
    "    request_start = time.time()\n",
    "    ttft = None\n",
    "    last_token_time = request_start\n",
    "    inter_token_latencies = []\n",
    "    \n",
    "    # Your exact model request\n",
    "    model_response = client.chat.completions.create(\n",
    "        model=\"\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    print(\"📡 Streaming response:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Your exact streaming logic with timing added\n",
    "    if stream:\n",
    "        usage = None\n",
    "        contents = []\n",
    "        token_count = 0\n",
    "        \n",
    "        for chunk in model_response:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if chunk.usage is not None:\n",
    "                usage = chunk.usage\n",
    "                continue\n",
    "            \n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Only process if there's actual content\n",
    "                # Timing measurements\n",
    "                if ttft is None:\n",
    "                    ttft = current_time - request_start\n",
    "                    print(f\"⚡ TTFT: {ttft:.3f}s\")\n",
    "                else:\n",
    "                    itl = current_time - last_token_time\n",
    "                    inter_token_latencies.append(itl)\n",
    "                \n",
    "                print(content, end=\"\", flush=True)\n",
    "                contents.append(content)\n",
    "                token_count += 1\n",
    "                last_token_time = current_time\n",
    "        \n",
    "        # Final measurements\n",
    "        e2e_latency = time.time() - request_start\n",
    "        \n",
    "        print(f\"\\n\\n📊 Performance Metrics:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"✅ Request completed successfully\")\n",
    "        print(f\"⏱️  E2E Latency: {e2e_latency:.3f}s\")\n",
    "        print(f\"⚡ TTFT: {ttft:.3f}s\" if ttft else \"⚡ TTFT: N/A\")\n",
    "        \n",
    "        if inter_token_latencies:\n",
    "            avg_tpot = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "            print(f\"🔄 Average TPOT: {avg_tpot:.3f}s\")\n",
    "            print(f\"📈 Min/Max ITL: {min(inter_token_latencies):.3f}s / {max(inter_token_latencies):.3f}s\")\n",
    "            print(f\"🚀 Tokens/second: {token_count / e2e_latency:.2f}\")\n",
    "        \n",
    "        print(f\"🔢 Total tokens: {token_count}\")\n",
    "        print(f\"📏 Total characters: {len(''.join(contents))}\")\n",
    "        \n",
    "        if usage:\n",
    "            print(f\"💾 Usage: {usage}\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'ttft': ttft,\n",
    "            'e2e_latency': e2e_latency,\n",
    "            'token_count': token_count,\n",
    "            'inter_token_latencies': inter_token_latencies,\n",
    "            'full_text': ''.join(contents),\n",
    "            'usage': usage\n",
    "        }\n",
    "    else:\n",
    "        response_text = model_response.choices[0].message.content\n",
    "        e2e_latency = time.time() - request_start\n",
    "        print(f\"📄 Response: {response_text}\")\n",
    "        print(f\"⏱️ E2E Latency: {e2e_latency:.3f}s\")\n",
    "        return {'success': True, 'response': response_text, 'e2e_latency': e2e_latency}\n",
    "\n",
    "def run_token_length_experiment(experiment_name: str,\n",
    "                               model_name: str = \"llama3.3_tpuv6e\",\n",
    "                               device_type: str = \"TPU v6e\") -> Dict[str, Any]:\n",
    "    \"\"\"Run a specific token length experiment\"\"\"\n",
    "    \n",
    "    if experiment_name not in TOKEN_LENGTH_EXPERIMENTS:\n",
    "        available = \", \".join(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "        raise ValueError(f\"Unknown experiment '{experiment_name}'. Available: {available}\")\n",
    "    \n",
    "    config = TOKEN_LENGTH_EXPERIMENTS[experiment_name]\n",
    "    \n",
    "    print(f\"\\n🔬 Running Token Length Experiment: {config['name']}\")\n",
    "    print(f\"📊 Input tokens: {config['input_tokens']}\")\n",
    "    print(f\"📊 Output tokens: {config['output_tokens']}\")\n",
    "    print(f\"📊 Total tokens: {config['total_tokens']}\")\n",
    "    print(f\"🚀 Concurrency: {config['concurrency']}\")\n",
    "    print(f\"📝 Requests: {config['num_requests']}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = RandomDataset(\n",
    "        input_len=config['input_tokens'],\n",
    "        output_len=config['output_tokens'],\n",
    "        num_requests=config['num_requests']\n",
    "    )\n",
    "    \n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine()\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=config['concurrency'],\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_detailed_summary()\n",
    "    summary = analyzer.generate_summary()\n",
    "    \n",
    "    if \"error\" not in summary:\n",
    "        print(f\"\\n✅ Experiment Complete: {config['name']}\")\n",
    "        print(f\"📈 TTFT-P95: {summary['ttft_percentiles'][95]:.2f}s\")\n",
    "        print(f\"📈 Token Output Throughput: {summary['output_token_throughput']:.2f} tok/s\")\n",
    "        print(f\"📈 Overall Token Throughput: {summary['overall_token_throughput']:.0f} tok/s\")\n",
    "        print(f\"👥 Concurrent Users: {config['concurrency']}\")\n",
    "    else:\n",
    "        print(f\"❌ Experiment Failed: {summary['error']}\")\n",
    "    \n",
    "    # Add experiment metadata\n",
    "    summary.update({\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"experiment_config\": config,\n",
    "        \"model_name\": model_name,\n",
    "        \"device_type\": device_type,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def run_comprehensive_token_length_study(model_name: str = \"llama3.3_tpuv6e\",\n",
    "                                        device_type: str = \"v6e-8 TPU (256 GB HBM)\",\n",
    "                                        experiments: List[str] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Run comprehensive token length study across all experiments\"\"\"\n",
    "    \n",
    "    if experiments is None:\n",
    "        experiments = list(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "    \n",
    "    print(f\"🚀 Starting Comprehensive Token Length Study\")\n",
    "    print(f\"📋 Model: {model_name}\")\n",
    "    print(f\"🖥️ Device: {device_type}\")\n",
    "    print(f\"🧪 Experiments: {len(experiments)}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_name in experiments:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name,\n",
    "                device_type=device_type\n",
    "            )\n",
    "            all_results[exp_name] = result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Experiment {exp_name} failed: {e}\")\n",
    "            all_results[exp_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"experiment_name\": exp_name\n",
    "            }\n",
    "    \n",
    "    # Generate comparison report\n",
    "    generate_token_length_comparison_report(all_results, model_name, device_type)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_token_length_comparison_report(all_results: Dict[str, Dict[str, Any]], \n",
    "                                          model_name: str,\n",
    "                                          device_type: str):\n",
    "    \"\"\"Generate comparison report in CSV format matching your screenshot with ALL metrics\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"token_length_benchmarks\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate comprehensive CSV report with ALL metrics\n",
    "    comprehensive_csv_data = []\n",
    "    \n",
    "    for exp_name, result in all_results.items():\n",
    "        if \"error\" not in result:\n",
    "            config = TOKEN_LENGTH_EXPERIMENTS[exp_name]\n",
    "            \n",
    "            # Base row data\n",
    "            base_row = {\n",
    "                \"Token_Length_Category\": config['name'],\n",
    "                \"Device_Type\": device_type,\n",
    "                \"Model_Name\": result['model_name'],\n",
    "                \"Input_Tokens\": config['input_tokens'],\n",
    "                \"Output_Tokens\": config['output_tokens'],\n",
    "                \"Total_Tokens\": config['total_tokens'],\n",
    "                \"Concurrent_Users\": config['concurrency'],\n",
    "                \"Total_Requests\": config['num_requests'],\n",
    "                \"Successful_Requests\": result['successful_requests'],\n",
    "                \"Failed_Requests\": result['failed_requests'],\n",
    "                \"Success_Rate_Percent\": f\"{result['successful_requests'] / result['total_requests'] * 100:.1f}%\",\n",
    "                \"Test_Duration_Seconds\": f\"{result['benchmark_duration']:.2f}\",\n",
    "                \n",
    "                # Key Performance Metrics (Your 4 main ones)\n",
    "                \"TTFT_P95_Seconds\": f\"{result['ttft_percentiles'][95]:.3f}\",\n",
    "                \"Token_Output_Throughput_Per_Second\": f\"{result['output_token_throughput']:.2f}\",\n",
    "                \"Overall_Token_Throughput\": f\"{result['overall_token_throughput']:.0f}\",\n",
    "                \n",
    "                # Complete TTFT Metrics\n",
    "                \"TTFT_P50_Seconds\": f\"{result['ttft_percentiles'][50]:.3f}\",\n",
    "                \"TTFT_P90_Seconds\": f\"{result['ttft_percentiles'][90]:.3f}\",\n",
    "                \"TTFT_P99_Seconds\": f\"{result['ttft_percentiles'][99]:.3f}\",\n",
    "                \n",
    "                # Complete TPOT Metrics\n",
    "                \"TPOT_P50_ms\": f\"{result['tpot_percentiles'][50]:.1f}\",\n",
    "                \"TPOT_P90_ms\": f\"{result['tpot_percentiles'][90]:.1f}\",\n",
    "                \"TPOT_P95_ms\": f\"{result['tpot_percentiles'][95]:.1f}\",\n",
    "                \"TPOT_P99_ms\": f\"{result['tpot_percentiles'][99]:.1f}\",\n",
    "                \n",
    "                # Complete End-to-End Latency Metrics\n",
    "                \"E2E_Latency_P50_Seconds\": f\"{result['e2e_latency_percentiles'][50]:.2f}\",\n",
    "                \"E2E_Latency_P90_Seconds\": f\"{result['e2e_latency_percentiles'][90]:.2f}\",\n",
    "                \"E2E_Latency_P95_Seconds\": f\"{result['e2e_latency_percentiles'][95]:.2f}\",\n",
    "                \"E2E_Latency_P99_Seconds\": f\"{result['e2e_latency_percentiles'][99]:.2f}\",\n",
    "                \n",
    "                # Throughput Metrics\n",
    "                \"Request_Throughput_Per_Second\": f\"{result['request_throughput']:.2f}\",\n",
    "                \"Input_Token_Throughput_Per_Second\": f\"{result['input_token_throughput']:.2f}\",\n",
    "                \n",
    "                # Token Statistics\n",
    "                \"Total_Input_Tokens_Processed\": f\"{result['total_input_tokens']:,}\",\n",
    "                \"Total_Output_Tokens_Generated\": f\"{result['total_output_tokens']:,}\",\n",
    "                \"Avg_Input_Tokens_Per_Request\": f\"{result['total_input_tokens'] / result['successful_requests']:.1f}\",\n",
    "                \"Avg_Output_Tokens_Per_Request\": f\"{result['total_output_tokens'] / result['successful_requests']:.1f}\",\n",
    "                \n",
    "                # Efficiency Metrics\n",
    "                \"Tokens_Per_Second_Per_User\": f\"{result['overall_token_throughput'] / config['concurrency']:.2f}\",\n",
    "                \"Requests_Per_Second_Per_User\": f\"{result['request_throughput'] / config['concurrency']:.3f}\",\n",
    "                \"Output_Input_Token_Ratio\": f\"{(result['total_output_tokens'] / result['total_input_tokens']):.2f}\",\n",
    "                \n",
    "                # Test Configuration\n",
    "                \"Temperature\": \"0.7\",\n",
    "                \"Streaming\": \"True\",\n",
    "                \"Test_Timestamp\": result['timestamp']\n",
    "            }\n",
    "            \n",
    "            comprehensive_csv_data.append(base_row)\n",
    "    \n",
    "    # Save comprehensive CSV\n",
    "    comprehensive_csv_file = os.path.join(output_dir, f\"comprehensive_token_length_analysis_{model_name.replace('.', '_')}_{timestamp}.csv\")\n",
    "    comprehensive_df = pd.DataFrame(comprehensive_csv_data)\n",
    "    comprehensive_df.to_csv(comprehensive_csv_file, index=False)\n",
    "    \n",
    "    # Generate formatted comparison table (key metrics only - like your screenshot)\n",
    "    key_metrics_data = []\n",
    "    \n",
    "    for exp_name, result in all_results.items():\n",
    "        if \"error\" not in result:\n",
    "            config = TOKEN_LENGTH_EXPERIMENTS[exp_name]\n",
    "            \n",
    "            key_metrics_data.append({\n",
    "                \"Token_Length_Category\": config['name'],\n",
    "                \"TTFT_P95_Seconds\": f\"{result['ttft_percentiles'][95]:.3f}\",\n",
    "                \"Token_Output_Throughput_Per_Second\": f\"{result['output_token_throughput']:.0f}\",\n",
    "                \"Overall_Token_Throughput\": f\"{result['overall_token_throughput']:.0f}\",\n",
    "                \"Concurrent_Users\": f\"{config['concurrency']}\"\n",
    "            })\n",
    "    \n",
    "    # Save key metrics comparison (matching your screenshot format)\n",
    "    key_metrics_file = os.path.join(output_dir, f\"key_metrics_comparison_{model_name.replace('.', '_')}_{timestamp}.csv\")\n",
    "    key_metrics_df = pd.DataFrame(key_metrics_data)\n",
    "    key_metrics_df.to_csv(key_metrics_file, index=False)\n",
    "    \n",
    "    print(f\"\\n📊 Comprehensive Reports Generated:\")\n",
    "    print(f\"   🎯 Key Metrics CSV (Your Format): {key_metrics_file}\")\n",
    "    print(f\"   📊 Comprehensive Analysis CSV: {comprehensive_csv_file}\")\n",
    "\n",
    "def quick_token_length_test(model_name: str = \"llama3.3_tpuv6e\"):\n",
    "    \"\"\"Run a quick test across all token length categories with reduced sizes\"\"\"\n",
    "    \n",
    "    # Reduced test sizes for quick evaluation\n",
    "    quick_experiments = {\n",
    "        \"small\": {\"num_requests\": 100, \"concurrency\": 25},\n",
    "        \"medium\": {\"num_requests\": 75, \"concurrency\": 15},\n",
    "        \"large\": {\"num_requests\": 50, \"concurrency\": 10},\n",
    "        \"xlarge\": {\"num_requests\": 25, \"concurrency\": 5}\n",
    "    }\n",
    "    \n",
    "    print(\"🚀 Running Quick Token Length Test...\")\n",
    "    \n",
    "    results = {}\n",
    "    for exp_name in quick_experiments:\n",
    "        # Temporarily modify config\n",
    "        original_config = TOKEN_LENGTH_EXPERIMENTS[exp_name].copy()\n",
    "        TOKEN_LENGTH_EXPERIMENTS[exp_name].update(quick_experiments[exp_name])\n",
    "        \n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name\n",
    "            )\n",
    "            results[exp_name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Quick test {exp_name} failed: {e}\")\n",
    "            results[exp_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Restore original config\n",
    "        TOKEN_LENGTH_EXPERIMENTS[exp_name] = original_config\n",
    "    \n",
    "    # Generate quick report\n",
    "    generate_token_length_comparison_report(results, model_name, \"v6e-8 TPU (256 GB HBM)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_custom_benchmark(input_tokens: int = 500,\n",
    "                        output_tokens: int = 500,\n",
    "                        num_requests: int = 100,\n",
    "                        concurrency: int = 10,\n",
    "                        temperature: float = 0.7,\n",
    "                        stream: bool = True,\n",
    "                        model_name: str = \"custom_test\"):\n",
    "    \"\"\"Run a custom benchmark with specified parameters\"\"\"\n",
    "    \n",
    "    print(f\"🧪 Running Custom Benchmark\")\n",
    "    print(f\"📊 Input tokens: {input_tokens}\")\n",
    "    print(f\"📊 Output tokens: {output_tokens}\")\n",
    "    print(f\"📝 Requests: {num_requests}\")\n",
    "    print(f\"🚀 Concurrency: {concurrency}\")\n",
    "    print(f\"🌡️ Temperature: {temperature}\")\n",
    "    print(f\"🔄 Streaming: {stream}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = RandomDataset(\n",
    "        input_len=input_tokens,\n",
    "        output_len=output_tokens,\n",
    "        num_requests=num_requests\n",
    "    )\n",
    "    \n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine()\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=concurrency,\n",
    "        temperature=temperature,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_detailed_summary()\n",
    "    \n",
    "    return analyzer.generate_summary()\n",
    "\n",
    "def run_throughput_scaling_test(base_concurrency: int = 10,\n",
    "                              max_concurrency: int = 100,\n",
    "                              step: int = 10,\n",
    "                              num_requests: int = 200):\n",
    "    \"\"\"Test how throughput scales with concurrency\"\"\"\n",
    "    \n",
    "    print(f\"📈 Running Throughput Scaling Test\")\n",
    "    print(f\"🚀 Concurrency range: {base_concurrency} to {max_concurrency} (step {step})\")\n",
    "    print(f\"📝 Requests per test: {num_requests}\")\n",
    "    \n",
    "    scaling_results = []\n",
    "    \n",
    "    for concurrency in range(base_concurrency, max_concurrency + 1, step):\n",
    "        print(f\"\\n🧪 Testing concurrency: {concurrency}\")\n",
    "        \n",
    "        # Generate small dataset for quick testing\n",
    "        dataset = RandomDataset(\n",
    "            input_len=250,\n",
    "            output_len=250,\n",
    "            num_requests=num_requests\n",
    "        )\n",
    "        \n",
    "        requests = dataset.generate_requests()\n",
    "        \n",
    "        # Run benchmark\n",
    "        engine = TPUBenchmarkEngine()\n",
    "        results = engine.run_benchmark(\n",
    "            requests=requests,\n",
    "            max_concurrency=concurrency,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Analyze results\n",
    "        analyzer = BenchmarkAnalyzer(results)\n",
    "        summary = analyzer.generate_summary()\n",
    "        \n",
    "        if \"error\" not in summary:\n",
    "            scaling_results.append({\n",
    "                \"concurrency\": concurrency,\n",
    "                \"request_throughput\": summary['request_throughput'],\n",
    "                \"output_token_throughput\": summary['output_token_throughput'],\n",
    "                \"overall_token_throughput\": summary['overall_token_throughput'],\n",
    "                \"ttft_p95\": summary['ttft_percentiles'][95],\n",
    "                \"tpot_p95\": summary['tpot_percentiles'][95],\n",
    "                \"success_rate\": summary['successful_requests'] / summary['total_requests']\n",
    "            })\n",
    "            \n",
    "            print(f\"   📈 Throughput: {summary['overall_token_throughput']:.0f} tok/s\")\n",
    "            print(f\"   ⚡ TTFT p95: {summary['ttft_percentiles'][95]*1000:.1f}ms\")\n",
    "        else:\n",
    "            print(f\"   ❌ Failed: {summary['error']}\")\n",
    "    \n",
    "    # Save scaling results\n",
    "    if scaling_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = \"token_length_benchmarks\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        scaling_df = pd.DataFrame(scaling_results)\n",
    "        scaling_file = os.path.join(output_dir, f\"throughput_scaling_{timestamp}.csv\")\n",
    "        scaling_df.to_csv(scaling_file, index=False)\n",
    "        \n",
    "        print(f\"\\n📊 Scaling Results Saved: {scaling_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n📈 THROUGHPUT SCALING SUMMARY:\")\n",
    "        print(f\"   🏆 Peak throughput: {max(r['overall_token_throughput'] for r in scaling_results):.0f} tok/s\")\n",
    "        print(f\"   🎯 Optimal concurrency: {max(scaling_results, key=lambda x: x['overall_token_throughput'])['concurrency']}\")\n",
    "    \n",
    "    return scaling_results\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Show example usage and run basic tests\"\"\"\n",
    "    print(\"\"\"\n",
    "🚀 TPU Endpoint Benchmark Suite - Examples\n",
    "\n",
    "This benchmark suite uses your exact working TPU endpoint code pattern with comprehensive performance analysis.\n",
    "\n",
    "# Basic Examples:\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"1. Simple Test (like your original):\")\n",
    "    print('   run_simple_test(\"Write a poem about AI\", max_tokens=200, temperature=0.8)')\n",
    "    \n",
    "    print(\"\\n2. Token Length Experiments:\")\n",
    "    print('   run_token_length_experiment(\"small\")     # 250 tokens')\n",
    "    print('   run_token_length_experiment(\"medium\")    # 465 tokens') \n",
    "    print('   run_token_length_experiment(\"large\")     # 1500 tokens')\n",
    "    print('   run_token_length_experiment(\"xlarge\")    # 3250 tokens')\n",
    "    \n",
    "    print(\"\\n3. Comprehensive Study:\")\n",
    "    print('   run_comprehensive_token_length_study(model_name=\"llama3.3_tpuv6e\")')\n",
    "    \n",
    "    print(\"\\n4. Quick Test (reduced sizes):\")\n",
    "    print('   quick_token_length_test(model_name=\"llama3.3_tpuv6e\")')\n",
    "    \n",
    "    print(\"\\n5. Custom Benchmark:\")\n",
    "    print('   run_custom_benchmark(input_tokens=1000, output_tokens=500, num_requests=50, concurrency=5)')\n",
    "    \n",
    "    print(\"\\n6. Throughput Scaling Test:\")\n",
    "    print('   run_throughput_scaling_test(base_concurrency=5, max_concurrency=50, step=5)')\n",
    "    \n",
    "    print(f\"\\n🔧 Current Configuration:\")\n",
    "    print(f\"   📋 Project: {PROJECT_ID}\")\n",
    "    print(f\"   🌍 Region: {REGION}\")\n",
    "    print(f\"   🖥️ Endpoint: {endpoint_name}\")\n",
    "    print(f\"   🔗 DNS: {DEDICATED_ENDPOINT_DNS if use_dedicated_endpoint else 'Standard'}\")\n",
    "    \n",
    "    # Run a simple demo\n",
    "    print(f\"\\n🎯 Running Demo Test...\")\n",
    "    try:\n",
    "        demo_result = run_simple_test(\n",
    "            prompt=\"Explain the benefits of TPU for machine learning in 3 sentences.\",\n",
    "            max_tokens=100,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        if demo_result['success']:\n",
    "            print(\"✅ Demo test completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Demo test failed: {e}\")\n",
    "        print(\"Please check your PROJECT_ID, REGION, and endpoint_name configuration.\")\n",
    "\n",
    "# Predefined test suites\n",
    "def production_readiness_test():\n",
    "    \"\"\"Comprehensive production readiness test\"\"\"\n",
    "    print(\"🏭 Running Production Readiness Test Suite...\")\n",
    "    \n",
    "    tests = [\n",
    "        (\"Latency Test\", lambda: run_token_length_experiment(\"small\")),\n",
    "        (\"Medium Load Test\", lambda: run_token_length_experiment(\"medium\")),\n",
    "        (\"High Load Test\", lambda: run_token_length_experiment(\"large\")),\n",
    "        (\"Scaling Test\", lambda: run_throughput_scaling_test(10, 50, 10, 100))\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for test_name, test_func in tests:\n",
    "        print(f\"\\n🧪 {test_name}...\")\n",
    "        try:\n",
    "            results[test_name] = test_func()\n",
    "            print(f\"✅ {test_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {test_name} failed: {e}\")\n",
    "            results[test_name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def stress_test():\n",
    "    \"\"\"High concurrency stress test\"\"\"\n",
    "    print(\"💪 Running Stress Test...\")\n",
    "    \n",
    "    return run_custom_benchmark(\n",
    "        input_tokens=500,\n",
    "        output_tokens=500, \n",
    "        num_requests=500,\n",
    "        concurrency=100,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "        model_name=\"stress_test\"\n",
    "    )\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If in notebook, show examples\n",
    "        example_usage()\n",
    "    except NameError:\n",
    "        # If script execution, show usage\n",
    "        import argparse\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description=\"TPU Endpoint Benchmark Suite\")\n",
    "        parser.add_argument(\"--test\", choices=[\"simple\", \"small\", \"medium\", \"large\", \"xlarge\", \"comprehensive\", \"quick\", \"scaling\", \"stress\", \"production\"], \n",
    "                           default=\"simple\", help=\"Test type to run\")\n",
    "        parser.add_argument(\"--model\", default=\"llama3.3_tpuv6e\", help=\"Model name\")\n",
    "        parser.add_argument(\"--requests\", type=int, default=100, help=\"Number of requests\")\n",
    "        parser.add_argument(\"--concurrency\", type=int, default=10, help=\"Concurrency level\")\n",
    "        parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Temperature\")\n",
    "        parser.add_argument(\"--max-tokens\", type=int, default=200, help=\"Max tokens\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        if args.test == \"simple\":\n",
    "            run_simple_test(max_tokens=args.max_tokens, temperature=args.temperature)\n",
    "        elif args.test in [\"small\", \"medium\", \"large\", \"xlarge\"]:\n",
    "            run_token_length_experiment(args.test, args.model)\n",
    "        elif args.test == \"comprehensive\":\n",
    "            run_comprehensive_token_length_study(args.model)\n",
    "        elif args.test == \"quick\":\n",
    "            quick_token_length_test(args.model)\n",
    "        elif args.test == \"scaling\":\n",
    "            run_throughput_scaling_test()\n",
    "        elif args.test == \"stress\":\n",
    "            stress_test()\n",
    "        elif args.test == \"production\":\n",
    "            production_readiness_test()\n",
    "        else:\n",
    "            example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44971e-ace1-4ed8-a6b1-3af5d463b35e",
   "metadata": {},
   "source": [
    "### 1. Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57b89d1-b322-4b2a-8eb9-8713ececc5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Simple Test\n",
      "   📝 Prompt: Write a poem about AI\n",
      "   🎯 Max tokens: 200\n",
      "   🌡️ Temperature: 0.8\n",
      "   🔄 Stream: True\n",
      "------------------------------------------------------------\n",
      "📡 Streaming response:\n",
      "----------------------------------------\n",
      "⚡ TTFT: 0.121s\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "📊 Performance Metrics:\n",
      "----------------------------------------\n",
      "✅ Request completed successfully\n",
      "⏱️  E2E Latency: 4.802s\n",
      "⚡ TTFT: 0.121s\n",
      "🔄 Average TPOT: 0.024s\n",
      "📈 Min/Max ITL: 0.001s / 0.134s\n",
      "🚀 Tokens/second: 41.65\n",
      "🔢 Total tokens: 200\n",
      "📏 Total characters: 200\n",
      "💾 Usage: CompletionUsage(completion_tokens=200, prompt_tokens=40, total_tokens=240, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'ttft': 0.12057781219482422,\n",
       " 'e2e_latency': 4.801581859588623,\n",
       " 'token_count': 200,\n",
       " 'inter_token_latencies': [0.0223085880279541,\n",
       "  0.022749662399291992,\n",
       "  0.023308277130126953,\n",
       "  0.02325892448425293,\n",
       "  0.0234677791595459,\n",
       "  0.022517919540405273,\n",
       "  0.02348160743713379,\n",
       "  0.024016141891479492,\n",
       "  0.022310256958007812,\n",
       "  0.02321791648864746,\n",
       "  0.023424625396728516,\n",
       "  0.022989988327026367,\n",
       "  0.022957563400268555,\n",
       "  0.023400068283081055,\n",
       "  0.023659467697143555,\n",
       "  0.02379012107849121,\n",
       "  0.023547649383544922,\n",
       "  0.02373504638671875,\n",
       "  0.023845195770263672,\n",
       "  0.0230715274810791,\n",
       "  0.022527456283569336,\n",
       "  0.022658109664916992,\n",
       "  0.023657560348510742,\n",
       "  0.022643327713012695,\n",
       "  0.023045778274536133,\n",
       "  0.02321171760559082,\n",
       "  0.023291587829589844,\n",
       "  0.022841930389404297,\n",
       "  0.02321600914001465,\n",
       "  0.023213863372802734,\n",
       "  0.02335953712463379,\n",
       "  0.023038625717163086,\n",
       "  0.022670507431030273,\n",
       "  0.023227691650390625,\n",
       "  0.023264169692993164,\n",
       "  0.023616313934326172,\n",
       "  0.023816823959350586,\n",
       "  0.022714853286743164,\n",
       "  0.02273416519165039,\n",
       "  0.022533655166625977,\n",
       "  0.022999286651611328,\n",
       "  0.02313089370727539,\n",
       "  0.023537397384643555,\n",
       "  0.023272275924682617,\n",
       "  0.02317023277282715,\n",
       "  0.023664236068725586,\n",
       "  0.022730350494384766,\n",
       "  0.02399897575378418,\n",
       "  0.02280259132385254,\n",
       "  0.024295806884765625,\n",
       "  0.023052453994750977,\n",
       "  0.023506641387939453,\n",
       "  0.023493051528930664,\n",
       "  0.02286076545715332,\n",
       "  0.023128986358642578,\n",
       "  0.022578716278076172,\n",
       "  0.023035764694213867,\n",
       "  0.022717714309692383,\n",
       "  0.022962093353271484,\n",
       "  0.02311563491821289,\n",
       "  0.02337789535522461,\n",
       "  0.0236208438873291,\n",
       "  0.023404598236083984,\n",
       "  0.023080110549926758,\n",
       "  0.023487329483032227,\n",
       "  0.022138357162475586,\n",
       "  0.0238800048828125,\n",
       "  0.023172855377197266,\n",
       "  0.022600412368774414,\n",
       "  0.023698091506958008,\n",
       "  0.0226593017578125,\n",
       "  0.0231170654296875,\n",
       "  0.023506641387939453,\n",
       "  0.023071765899658203,\n",
       "  0.02313995361328125,\n",
       "  0.022825241088867188,\n",
       "  0.02369832992553711,\n",
       "  0.023446321487426758,\n",
       "  0.022121906280517578,\n",
       "  0.022739887237548828,\n",
       "  0.022910118103027344,\n",
       "  0.023359060287475586,\n",
       "  0.022745370864868164,\n",
       "  0.02252960205078125,\n",
       "  0.022853612899780273,\n",
       "  0.024408817291259766,\n",
       "  0.023209571838378906,\n",
       "  0.023334503173828125,\n",
       "  0.023123502731323242,\n",
       "  0.023253679275512695,\n",
       "  0.024141311645507812,\n",
       "  0.023409366607666016,\n",
       "  0.023401737213134766,\n",
       "  0.022796630859375,\n",
       "  0.02332329750061035,\n",
       "  0.022515058517456055,\n",
       "  0.02317953109741211,\n",
       "  0.02292919158935547,\n",
       "  0.0230863094329834,\n",
       "  0.02300429344177246,\n",
       "  0.022887706756591797,\n",
       "  0.023011445999145508,\n",
       "  0.02285003662109375,\n",
       "  0.022472381591796875,\n",
       "  0.023641347885131836,\n",
       "  0.02512955665588379,\n",
       "  0.02287912368774414,\n",
       "  0.023578643798828125,\n",
       "  0.023411989212036133,\n",
       "  0.022745370864868164,\n",
       "  0.02286505699157715,\n",
       "  0.023339271545410156,\n",
       "  0.023560523986816406,\n",
       "  0.022875547409057617,\n",
       "  0.023137807846069336,\n",
       "  0.024282455444335938,\n",
       "  0.02307748794555664,\n",
       "  0.022621631622314453,\n",
       "  0.023389339447021484,\n",
       "  0.023119688034057617,\n",
       "  0.022421598434448242,\n",
       "  0.0241391658782959,\n",
       "  0.02320241928100586,\n",
       "  0.022884845733642578,\n",
       "  0.022977828979492188,\n",
       "  0.02288079261779785,\n",
       "  0.022984981536865234,\n",
       "  0.023688077926635742,\n",
       "  0.023749113082885742,\n",
       "  0.02340221405029297,\n",
       "  0.022975444793701172,\n",
       "  0.022584199905395508,\n",
       "  0.023993492126464844,\n",
       "  0.023589372634887695,\n",
       "  0.023029804229736328,\n",
       "  0.023497581481933594,\n",
       "  0.02289748191833496,\n",
       "  0.022982358932495117,\n",
       "  0.022818326950073242,\n",
       "  0.02327585220336914,\n",
       "  0.023285627365112305,\n",
       "  0.023729562759399414,\n",
       "  0.022639989852905273,\n",
       "  0.022966623306274414,\n",
       "  0.023831844329833984,\n",
       "  0.022875547409057617,\n",
       "  0.0229339599609375,\n",
       "  0.024266481399536133,\n",
       "  0.02295684814453125,\n",
       "  0.023138761520385742,\n",
       "  0.02359771728515625,\n",
       "  0.023410320281982422,\n",
       "  0.023030519485473633,\n",
       "  0.02303028106689453,\n",
       "  0.022919893264770508,\n",
       "  0.022726774215698242,\n",
       "  0.0232391357421875,\n",
       "  0.023647546768188477,\n",
       "  0.02314162254333496,\n",
       "  0.023579120635986328,\n",
       "  0.022667884826660156,\n",
       "  0.024139404296875,\n",
       "  0.02281022071838379,\n",
       "  0.022897005081176758,\n",
       "  0.023622512817382812,\n",
       "  0.02396559715270996,\n",
       "  0.023160457611083984,\n",
       "  0.02312016487121582,\n",
       "  0.023331165313720703,\n",
       "  0.02293229103088379,\n",
       "  0.02299332618713379,\n",
       "  0.02275562286376953,\n",
       "  0.023467063903808594,\n",
       "  0.021723508834838867,\n",
       "  0.022617340087890625,\n",
       "  0.022328615188598633,\n",
       "  0.022789478302001953,\n",
       "  0.023164033889770508,\n",
       "  0.02356576919555664,\n",
       "  0.02275824546813965,\n",
       "  0.023259639739990234,\n",
       "  0.022397756576538086,\n",
       "  0.02544260025024414,\n",
       "  0.02051568031311035,\n",
       "  0.022409915924072266,\n",
       "  0.02342677116394043,\n",
       "  0.023738622665405273,\n",
       "  0.022797822952270508,\n",
       "  0.07003521919250488,\n",
       "  0.002538919448852539,\n",
       "  0.001466512680053711,\n",
       "  0.01784229278564453,\n",
       "  0.02402353286743164,\n",
       "  0.02317023277282715,\n",
       "  0.024071693420410156,\n",
       "  0.022911548614501953,\n",
       "  0.13380837440490723,\n",
       "  0.0016529560089111328,\n",
       "  0.002141714096069336],\n",
       " 'full_text': '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " 'usage': CompletionUsage(completion_tokens=200, prompt_tokens=40, total_tokens=240, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_test(\n",
    "    prompt=\"Write a poem about AI\", \n",
    "    max_tokens=200, \n",
    "    temperature=0.8, \n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d05167-ed3d-41fd-977e-9f8e0a013a1f",
   "metadata": {},
   "source": [
    "### 2. Run various token length experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d270f2a-65c8-449a-ac09-2126c30c1090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Comprehensive Token Length Study\n",
      "📋 Model: llama3.3_tpuv6e\n",
      "🖥️ Device: v6e-8 TPU (256 GB HBM)\n",
      "🧪 Experiments: 4\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔬 Running Token Length Experiment: Token Length - Small(250)\n",
      "📊 Input tokens: 250\n",
      "📊 Output tokens: 250\n",
      "📊 Total tokens: 500\n",
      "🚀 Concurrency: 250\n",
      "📝 Requests: 2000\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 2000 requests...\n",
      "👥 Max concurrency: 250\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 2000/2000 [08:01<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 498.88 seconds\n",
      "📈 Success rate: 2000/2000 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "📊 DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📋 REQUEST STATISTICS:\n",
      "   ✅ Successful: 2,000\n",
      "   ❌ Failed: 0\n",
      "   📊 Success Rate: 100.0%\n",
      "   ⏱️ Duration: 590.89s\n",
      "\n",
      "⚡ LATENCY METRICS:\n",
      "   🚀 TTFT p50: 2608.6ms\n",
      "   🚀 TTFT p95: 5060.1ms\n",
      "   🚀 TTFT p99: 6264.6ms\n",
      "\n",
      "🔄 TIME PER OUTPUT TOKEN:\n",
      "   ⚡ TPOT p50: 165.0ms\n",
      "   ⚡ TPOT p95: 198.0ms\n",
      "   ⚡ TPOT p99: 212.1ms\n",
      "\n",
      "⏱️ END-TO-END LATENCY:\n",
      "   📈 E2E p50: 51.85s\n",
      "   📈 E2E p95: 61.90s\n",
      "   📈 E2E p99: 65.56s\n",
      "\n",
      "🚀 THROUGHPUT METRICS:\n",
      "   📊 Requests/sec: 3.38\n",
      "   📤 Output tokens/sec: 1015.02\n",
      "   📊 Overall tokens/sec: 1858\n",
      "\n",
      "📊 TOKEN STATISTICS:\n",
      "   📥 Total input tokens: 498,145.699999994\n",
      "   📤 Total output tokens: 599,766\n",
      "   📊 Avg input/request: 249.1\n",
      "   📊 Avg output/request: 299.9\n",
      "\n",
      "✅ Experiment Complete: Token Length - Small(250)\n",
      "📈 TTFT-P95: 5.06s\n",
      "📈 Token Output Throughput: 1015.02 tok/s\n",
      "📈 Overall Token Throughput: 1858 tok/s\n",
      "👥 Concurrent Users: 250\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔬 Running Token Length Experiment: Token Length - Medium(350 - 580)\n",
      "📊 Input tokens: 465\n",
      "📊 Output tokens: 465\n",
      "📊 Total tokens: 930\n",
      "🚀 Concurrency: 150\n",
      "📝 Requests: 1500\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 1500 requests...\n",
      "👥 Max concurrency: 150\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests:  93%|█████████▎| 926/1000 [20:20<00:11,  6.51it/s]  "
     ]
    }
   ],
   "source": [
    "# run_token_length_experiment(\"small\")    # 250 tokens, 250 concurrency\n",
    "# run_token_length_experiment(\"medium\")   # 465 tokens, 150 concurrency  \n",
    "# run_token_length_experiment(\"large\")    # 1500 tokens, 100 concurrency\n",
    "# run_token_length_experiment(\"xlarge\")   # 3250 tokens, 30 concurrency\n",
    "\n",
    "run_comprehensive_token_length_study(experiments=[\"small\", \"medium\", \"large\", \"xlarge\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae8cb7-10ed-4d69-a27d-33203321d2d6",
   "metadata": {},
   "source": [
    "### 3. Comprehensive Study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3b1016-6464-49e9-ba55-1813df9760d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 2000/2000 [06:59<00:00,  4.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 451.14 seconds\n",
      "📈 Success rate: 2000/2000 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "📊 DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📋 REQUEST STATISTICS:\n",
      "   ✅ Successful: 2,000\n",
      "   ❌ Failed: 0\n",
      "   📊 Success Rate: 100.0%\n",
      "   ⏱️ Duration: 589.08s\n",
      "\n",
      "⚡ LATENCY METRICS:\n",
      "   🚀 TTFT p50: 3178.8ms\n",
      "   🚀 TTFT p95: 5205.2ms\n",
      "   🚀 TTFT p99: 6367.6ms\n",
      "\n",
      "🔄 TIME PER OUTPUT TOKEN:\n",
      "   ⚡ TPOT p50: 166.5ms\n",
      "   ⚡ TPOT p95: 192.1ms\n",
      "   ⚡ TPOT p99: 201.4ms\n",
      "\n",
      "⏱️ END-TO-END LATENCY:\n",
      "   📈 E2E p50: 53.20s\n",
      "   📈 E2E p95: 61.11s\n",
      "   📈 E2E p99: 64.14s\n",
      "\n",
      "🚀 THROUGHPUT METRICS:\n",
      "   📊 Requests/sec: 3.40\n",
      "   📤 Output tokens/sec: 1018.26\n",
      "   📊 Overall tokens/sec: 1864\n",
      "\n",
      "📊 TOKEN STATISTICS:\n",
      "   📥 Total input tokens: 498,178.1999999926\n",
      "   📤 Total output tokens: 599,833\n",
      "   📊 Avg input/request: 249.1\n",
      "   📊 Avg output/request: 299.9\n",
      "\n",
      "✅ Experiment Complete: Token Length - Small(250)\n",
      "📈 TTFT-P95: 5.21s\n",
      "📈 Token Output Throughput: 1018.26 tok/s\n",
      "📈 Overall Token Throughput: 1864 tok/s\n",
      "👥 Concurrent Users: 250\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔬 Running Token Length Experiment: Token Length - Medium(350 - 580)\n",
      "📊 Input tokens: 465\n",
      "📊 Output tokens: 465\n",
      "📊 Total tokens: 930\n",
      "🚀 Concurrency: 150\n",
      "📝 Requests: 1500\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 1500 requests...\n",
      "👥 Max concurrency: 150\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 1500/1500 [08:52<00:00,  2.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 535.13 seconds\n",
      "📈 Success rate: 1500/1500 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "📊 DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📋 REQUEST STATISTICS:\n",
      "   ✅ Successful: 1,500\n",
      "   ❌ Failed: 0\n",
      "   📊 Success Rate: 100.0%\n",
      "   ⏱️ Duration: 629.87s\n",
      "\n",
      "⚡ LATENCY METRICS:\n",
      "   🚀 TTFT p50: 1871.9ms\n",
      "   🚀 TTFT p95: 3693.7ms\n",
      "   🚀 TTFT p99: 5352.4ms\n",
      "\n",
      "🔄 TIME PER OUTPUT TOKEN:\n",
      "   ⚡ TPOT p50: 96.4ms\n",
      "   ⚡ TPOT p95: 105.9ms\n",
      "   ⚡ TPOT p99: 109.9ms\n",
      "\n",
      "⏱️ END-TO-END LATENCY:\n",
      "   📈 E2E p50: 51.69s\n",
      "   📈 E2E p95: 56.51s\n",
      "   📈 E2E p99: 58.59s\n",
      "\n",
      "🚀 THROUGHPUT METRICS:\n",
      "   📊 Requests/sec: 2.38\n",
      "   📤 Output tokens/sec: 1225.90\n",
      "   📊 Overall tokens/sec: 2317\n",
      "\n",
      "📊 TOKEN STATISTICS:\n",
      "   📥 Total input tokens: 687,544.0000000022\n",
      "   📤 Total output tokens: 772,158\n",
      "   📊 Avg input/request: 458.4\n",
      "   📊 Avg output/request: 514.8\n",
      "\n",
      "✅ Experiment Complete: Token Length - Medium(350 - 580)\n",
      "📈 TTFT-P95: 3.69s\n",
      "📈 Token Output Throughput: 1225.90 tok/s\n",
      "📈 Overall Token Throughput: 2317 tok/s\n",
      "👥 Concurrent Users: 150\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔬 Running Token Length Experiment: Token Length - Large(1200 - 1800)\n",
      "📊 Input tokens: 1500\n",
      "📊 Output tokens: 1500\n",
      "📊 Total tokens: 3000\n",
      "🚀 Concurrency: 100\n",
      "📝 Requests: 1000\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 1000 requests...\n",
      "👥 Max concurrency: 100\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 500/500 [14:28<00:00,  1.74s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 869.02 seconds\n",
      "📈 Success rate: 500/500 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "📊 DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📋 REQUEST STATISTICS:\n",
      "   ✅ Successful: 500\n",
      "   ❌ Failed: 0\n",
      "   📊 Success Rate: 100.0%\n",
      "   ⏱️ Duration: 872.16s\n",
      "\n",
      "⚡ LATENCY METRICS:\n",
      "   🚀 TTFT p50: 764.3ms\n",
      "   🚀 TTFT p95: 1857.1ms\n",
      "   🚀 TTFT p99: 7016.5ms\n",
      "\n",
      "🔄 TIME PER OUTPUT TOKEN:\n",
      "   ⚡ TPOT p50: 47.6ms\n",
      "   ⚡ TPOT p95: 48.0ms\n",
      "   ⚡ TPOT p99: 55.2ms\n",
      "\n",
      "⏱️ END-TO-END LATENCY:\n",
      "   📈 E2E p50: 50.72s\n",
      "   📈 E2E p95: 55.97s\n",
      "   📈 E2E p99: 58.51s\n",
      "\n",
      "🚀 THROUGHPUT METRICS:\n",
      "   📊 Requests/sec: 0.57\n",
      "   📤 Output tokens/sec: 601.92\n",
      "   📊 Overall tokens/sec: 2422\n",
      "\n",
      "📊 TOKEN STATISTICS:\n",
      "   📥 Total input tokens: 1,587,067.300000005\n",
      "   📤 Total output tokens: 524,970\n",
      "   📊 Avg input/request: 3174.1\n",
      "   📊 Avg output/request: 1049.9\n",
      "\n",
      "✅ Experiment Complete: Token Length - Xlarge(2.5k - 4k)\n",
      "📈 TTFT-P95: 1.86s\n",
      "📈 Token Output Throughput: 601.92 tok/s\n",
      "📈 Overall Token Throughput: 2422 tok/s\n",
      "👥 Concurrent Users: 30\n",
      "\n",
      "📊 Comprehensive Reports Generated:\n",
      "   🎯 Key Metrics CSV (Your Format): token_length_benchmarks/key_metrics_comparison_llama3_3_tpuv6e_20250715_181747.csv\n",
      "   📊 Comprehensive Analysis CSV: token_length_benchmarks/comprehensive_token_length_analysis_llama3_3_tpuv6e_20250715_181747.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'small': {'successful_requests': 2000,\n",
       "  'failed_requests': 0,\n",
       "  'total_requests': 2000,\n",
       "  'benchmark_duration': 589.0778615474701,\n",
       "  'request_throughput': 3.3951369259508875,\n",
       "  'input_token_throughput': 845.6916012618607,\n",
       "  'output_token_throughput': 1018.2575838519493,\n",
       "  'overall_token_throughput': 1863.94918511381,\n",
       "  'total_input_tokens': 498178.1999999926,\n",
       "  'total_output_tokens': 599833,\n",
       "  'ttft_percentiles': {50: np.float64(3.178810954093933),\n",
       "   90: np.float64(4.638809752464295),\n",
       "   95: np.float64(5.205248308181763),\n",
       "   99: np.float64(6.3676453614234925)},\n",
       "  'tpot_percentiles': {50: np.float64(166.5489518875869),\n",
       "   90: np.float64(186.44861210150063),\n",
       "   95: np.float64(192.11059362194612),\n",
       "   99: np.float64(201.43812152454285)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(53.20250082015991),\n",
       "   90: np.float64(59.260724258422854),\n",
       "   95: np.float64(61.10888644456863),\n",
       "   99: np.float64(64.13655402898789)},\n",
       "  'experiment_name': 'small',\n",
       "  'experiment_config': {'name': 'Token Length - Small(250)',\n",
       "   'input_tokens': 250,\n",
       "   'output_tokens': 250,\n",
       "   'total_tokens': 500,\n",
       "   'concurrency': 250,\n",
       "   'num_requests': 2000,\n",
       "   'description': 'Small token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T17:33:40.142583'},\n",
       " 'medium': {'successful_requests': 1500,\n",
       "  'failed_requests': 0,\n",
       "  'total_requests': 1500,\n",
       "  'benchmark_duration': 629.8719720840454,\n",
       "  'request_throughput': 2.3814363338584164,\n",
       "  'input_token_throughput': 1091.5615084842377,\n",
       "  'output_token_throughput': 1225.8967444529649,\n",
       "  'overall_token_throughput': 2317.458252937203,\n",
       "  'total_input_tokens': 687544.0000000022,\n",
       "  'total_output_tokens': 772158,\n",
       "  'ttft_percentiles': {50: np.float64(1.8718584775924683),\n",
       "   90: np.float64(2.874673795700074),\n",
       "   95: np.float64(3.6936592578887932),\n",
       "   99: np.float64(5.352431359291076)},\n",
       "  'tpot_percentiles': {50: np.float64(96.44748937295105),\n",
       "   90: np.float64(103.77714142261313),\n",
       "   95: np.float64(105.88593691703412),\n",
       "   99: np.float64(109.87056585137482)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(51.69079852104187),\n",
       "   90: np.float64(55.49904773235321),\n",
       "   95: np.float64(56.508831632137294),\n",
       "   99: np.float64(58.58802018642425)},\n",
       "  'experiment_name': 'medium',\n",
       "  'experiment_config': {'name': 'Token Length - Medium(350 - 580)',\n",
       "   'input_tokens': 465,\n",
       "   'output_tokens': 465,\n",
       "   'total_tokens': 930,\n",
       "   'concurrency': 150,\n",
       "   'num_requests': 1500,\n",
       "   'description': 'Medium token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T17:42:35.648058'},\n",
       " 'large': {'successful_requests': 999,\n",
       "  'failed_requests': 1,\n",
       "  'total_requests': 1000,\n",
       "  'benchmark_duration': 1264.4612381458282,\n",
       "  'request_throughput': 0.7900598056014011,\n",
       "  'input_token_throughput': 1160.1855839813495,\n",
       "  'output_token_throughput': 1224.4289925963265,\n",
       "  'overall_token_throughput': 2384.614576577676,\n",
       "  'total_input_tokens': 1467009.699999998,\n",
       "  'total_output_tokens': 1548243,\n",
       "  'ttft_percentiles': {50: np.float64(1.350999116897583),\n",
       "   90: np.float64(2.775886344909669),\n",
       "   95: np.float64(6.248661971092206),\n",
       "   99: np.float64(12.731653532981872)},\n",
       "  'tpot_percentiles': {50: np.float64(78.84154640118896),\n",
       "   90: np.float64(79.53633760313129),\n",
       "   95: np.float64(79.63380524233281),\n",
       "   99: np.float64(79.77085293609147)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(123.7315046787262),\n",
       "   90: np.float64(124.93169393539429),\n",
       "   95: np.float64(126.0218659877777),\n",
       "   99: np.float64(133.1352851009369)},\n",
       "  'experiment_name': 'large',\n",
       "  'experiment_config': {'name': 'Token Length - Large(1200 - 1800)',\n",
       "   'input_tokens': 1500,\n",
       "   'output_tokens': 1500,\n",
       "   'total_tokens': 3000,\n",
       "   'concurrency': 100,\n",
       "   'num_requests': 1000,\n",
       "   'description': 'Large token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T18:03:17.369639'},\n",
       " 'xlarge': {'successful_requests': 500,\n",
       "  'failed_requests': 0,\n",
       "  'total_requests': 500,\n",
       "  'benchmark_duration': 872.1621716022491,\n",
       "  'request_throughput': 0.5732878772779723,\n",
       "  'input_token_throughput': 1819.6928870285712,\n",
       "  'output_token_throughput': 601.9178738692342,\n",
       "  'overall_token_throughput': 2421.6107608978054,\n",
       "  'total_input_tokens': 1587067.300000005,\n",
       "  'total_output_tokens': 524970,\n",
       "  'ttft_percentiles': {50: np.float64(0.7643232345581055),\n",
       "   90: np.float64(1.6921887397766133),\n",
       "   95: np.float64(1.857050287723541),\n",
       "   99: np.float64(7.01654859781265)},\n",
       "  'tpot_percentiles': {50: np.float64(47.60750926484143),\n",
       "   90: np.float64(47.82994773071987),\n",
       "   95: np.float64(47.96382778854527),\n",
       "   99: np.float64(55.16062678099815)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(50.71910536289215),\n",
       "   90: np.float64(51.43657557964325),\n",
       "   95: np.float64(55.9717831134796),\n",
       "   99: np.float64(58.51055370807647)},\n",
       "  'experiment_name': 'xlarge',\n",
       "  'experiment_config': {'name': 'Token Length - Xlarge(2.5k - 4k)',\n",
       "   'input_tokens': 3250,\n",
       "   'output_tokens': 1000,\n",
       "   'total_tokens': 4250,\n",
       "   'concurrency': 30,\n",
       "   'num_requests': 500,\n",
       "   'description': 'Extra large token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T18:17:47.088353'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_comprehensive_token_length_study(\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    device_type=\"v6e-8 TPU (256 GB HBM)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8571f-0b00-47b1-b019-78d40ec47ec5",
   "metadata": {},
   "source": [
    "### 4. Custom Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a508977f-4080-4d44-acb4-925e331a5486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Running Custom Benchmark\n",
      "📊 Input tokens: 1000\n",
      "📊 Output tokens: 500\n",
      "📝 Requests: 50\n",
      "🚀 Concurrency: 5\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 50 requests...\n",
      "👥 Max concurrency: 5\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 50/50 [03:19<00:00,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 199.24 seconds\n",
      "📈 Success rate: 50/50 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "📊 DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📋 REQUEST STATISTICS:\n",
      "   ✅ Successful: 50\n",
      "   ❌ Failed: 0\n",
      "   📊 Success Rate: 100.0%\n",
      "   ⏱️ Duration: 199.58s\n",
      "\n",
      "⚡ LATENCY METRICS:\n",
      "   🚀 TTFT p50: 251.7ms\n",
      "   🚀 TTFT p95: 421.5ms\n",
      "   🚀 TTFT p99: 465.8ms\n",
      "\n",
      "🔄 TIME PER OUTPUT TOKEN:\n",
      "   ⚡ TPOT p50: 35.8ms\n",
      "   ⚡ TPOT p95: 35.9ms\n",
      "   ⚡ TPOT p99: 36.0ms\n",
      "\n",
      "⏱️ END-TO-END LATENCY:\n",
      "   📈 E2E p50: 19.88s\n",
      "   📈 E2E p95: 20.09s\n",
      "   📈 E2E p99: 20.11s\n",
      "\n",
      "🚀 THROUGHPUT METRICS:\n",
      "   📊 Requests/sec: 0.25\n",
      "   📤 Output tokens/sec: 137.79\n",
      "   📊 Overall tokens/sec: 384\n",
      "\n",
      "📊 TOKEN STATISTICS:\n",
      "   📥 Total input tokens: 49,051.60000000001\n",
      "   📤 Total output tokens: 27,500\n",
      "   📊 Avg input/request: 981.0\n",
      "   📊 Avg output/request: 550.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'successful_requests': 50,\n",
       " 'failed_requests': 0,\n",
       " 'total_requests': 50,\n",
       " 'benchmark_duration': 199.5771927833557,\n",
       " 'request_throughput': 0.2505296286749349,\n",
       " 'input_token_throughput': 245.77758267822878,\n",
       " 'output_token_throughput': 137.79129577121418,\n",
       " 'overall_token_throughput': 383.56887844944293,\n",
       " 'total_input_tokens': 49051.60000000001,\n",
       " 'total_output_tokens': 27500,\n",
       " 'ttft_percentiles': {50: np.float64(0.25173211097717285),\n",
       "  90: np.float64(0.370452094078064),\n",
       "  95: np.float64(0.42148737907409656),\n",
       "  99: np.float64(0.46575789213180535)},\n",
       " 'tpot_percentiles': {50: np.float64(35.75101576216234),\n",
       "  90: np.float64(35.84772202920827),\n",
       "  95: np.float64(35.8981689468759),\n",
       "  99: np.float64(35.96962899675352)},\n",
       " 'e2e_latency_percentiles': {50: np.float64(19.87656855583191),\n",
       "  90: np.float64(20.00148813724518),\n",
       "  95: np.float64(20.09250659942627),\n",
       "  99: np.float64(20.106535928249357)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_custom_benchmark(\n",
    "    input_tokens=1000,\n",
    "    output_tokens=500, \n",
    "    num_requests=50,\n",
    "    concurrency=5,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c45ce-2fff-482a-839c-a3e54b88c2c6",
   "metadata": {},
   "source": [
    "### 5. Scaling Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a122d66e-bd9e-4988-b08d-e0c7d70c809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Running Throughput Scaling Test\n",
      "🚀 Concurrency range: 10 to 100 (step 10)\n",
      "📝 Requests per test: 200\n",
      "\n",
      "🧪 Testing concurrency: 10\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 10\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [03:42<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 222.44 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 493 tok/s\n",
      "   ⚡ TTFT p95: 234.4ms\n",
      "\n",
      "🧪 Testing concurrency: 20\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 20\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [01:55<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 115.52 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 948 tok/s\n",
      "   ⚡ TTFT p95: 326.1ms\n",
      "\n",
      "🧪 Testing concurrency: 30\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 30\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [01:04<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 64.70 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 1692 tok/s\n",
      "   ⚡ TTFT p95: 502.2ms\n",
      "\n",
      "🧪 Testing concurrency: 40\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 40\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:48<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 48.73 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 2226 tok/s\n",
      "   ⚡ TTFT p95: 682.3ms\n",
      "\n",
      "🧪 Testing concurrency: 50\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 50\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:44<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 44.82 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 2358 tok/s\n",
      "   ⚡ TTFT p95: 900.8ms\n",
      "\n",
      "🧪 Testing concurrency: 60\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 60\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:47<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 49.08 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 2214 tok/s\n",
      "   ⚡ TTFT p95: 1937.2ms\n",
      "\n",
      "🧪 Testing concurrency: 70\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 70\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:38<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 39.42 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 2564 tok/s\n",
      "   ⚡ TTFT p95: 1144.6ms\n",
      "\n",
      "🧪 Testing concurrency: 80\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 80\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:40<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 41.55 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 2244 tok/s\n",
      "   ⚡ TTFT p95: 1150.0ms\n",
      "\n",
      "🧪 Testing concurrency: 90\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 90\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:46<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 48.16 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 1941 tok/s\n",
      "   ⚡ TTFT p95: 1378.8ms\n",
      "\n",
      "🧪 Testing concurrency: 100\n",
      "🔗 Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "🚀 Starting benchmark with 200 requests...\n",
      "👥 Max concurrency: 100\n",
      "🌡️ Temperature: 0.7\n",
      "🔄 Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Processing requests: 100%|██████████| 200/200 [00:38<00:00,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark completed in 39.63 seconds\n",
      "📈 Success rate: 200/200 (100.0%)\n",
      "   📈 Throughput: 2536 tok/s\n",
      "   ⚡ TTFT p95: 1414.8ms\n",
      "\n",
      "📊 Scaling Results Saved: token_length_benchmarks/throughput_scaling_20250715_183300.csv\n",
      "\n",
      "📈 THROUGHPUT SCALING SUMMARY:\n",
      "   🏆 Peak throughput: 2564 tok/s\n",
      "   🎯 Optimal concurrency: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'concurrency': 10,\n",
       "  'request_throughput': 0.8973291696540795,\n",
       "  'output_token_throughput': 269.1987508962238,\n",
       "  'overall_token_throughput': 492.70550047366214,\n",
       "  'ttft_p95': np.float64(0.2343828439712524),\n",
       "  'tpot_p95': np.float64(36.97146568011281),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 20,\n",
       "  'request_throughput': 1.726308639522166,\n",
       "  'output_token_throughput': 517.8925918566498,\n",
       "  'overall_token_throughput': 947.7244537026344,\n",
       "  'ttft_p95': np.float64(0.3261077880859375),\n",
       "  'tpot_p95': np.float64(37.92974657836965),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 30,\n",
       "  'request_throughput': 3.0815879403177884,\n",
       "  'output_token_throughput': 924.4763820953365,\n",
       "  'overall_token_throughput': 1692.0983972345273,\n",
       "  'ttft_p95': np.float64(0.5022170305252074),\n",
       "  'tpot_p95': np.float64(37.12986687752714),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 40,\n",
       "  'request_throughput': 4.054141327320815,\n",
       "  'output_token_throughput': 1216.2423981962447,\n",
       "  'overall_token_throughput': 2225.942512330803,\n",
       "  'ttft_p95': np.float64(0.6823026537895202),\n",
       "  'tpot_p95': np.float64(31.193702635557756),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 50,\n",
       "  'request_throughput': 4.295416427490725,\n",
       "  'output_token_throughput': 1287.7873220438569,\n",
       "  'overall_token_throughput': 2357.9409276642546,\n",
       "  'ttft_p95': np.float64(0.9008141875267026),\n",
       "  'tpot_p95': np.float64(35.795600757152336),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 60,\n",
       "  'request_throughput': 4.0333449598528315,\n",
       "  'output_token_throughput': 1209.5396532854663,\n",
       "  'overall_token_throughput': 2213.7981814942627,\n",
       "  'ttft_p95': np.float64(1.9372004032135008),\n",
       "  'tpot_p95': np.float64(38.8952347793327),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 70,\n",
       "  'request_throughput': 4.672992240765772,\n",
       "  'output_token_throughput': 1400.2153950230559,\n",
       "  'overall_token_throughput': 2564.376923499949,\n",
       "  'ttft_p95': np.float64(1.1446186900138853),\n",
       "  'tpot_p95': np.float64(44.16661581466828),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 80,\n",
       "  'request_throughput': 4.087246925213982,\n",
       "  'output_token_throughput': 1225.6018629946645,\n",
       "  'overall_token_throughput': 2244.0518337021713,\n",
       "  'ttft_p95': np.float64(1.1500409841537471),\n",
       "  'tpot_p95': np.float64(52.75865047671723),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 90,\n",
       "  'request_throughput': 3.5354424011736243,\n",
       "  'output_token_throughput': 1060.6327203520873,\n",
       "  'overall_token_throughput': 1940.9649491291225,\n",
       "  'ttft_p95': np.float64(1.378824138641357),\n",
       "  'tpot_p95': np.float64(60.16815685508242),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 100,\n",
       "  'request_throughput': 4.622715601401743,\n",
       "  'output_token_throughput': 1385.0580484919901,\n",
       "  'overall_token_throughput': 2535.8230021581353,\n",
       "  'ttft_p95': np.float64(1.4148171305656427),\n",
       "  'tpot_p95': np.float64(65.87223860879757),\n",
       "  'success_rate': 1.0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_throughput_scaling_test(\n",
    "    base_concurrency=10,\n",
    "    max_concurrency=100, \n",
    "    step=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1977ca-6017-41f5-8844-3a355cfd0925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
