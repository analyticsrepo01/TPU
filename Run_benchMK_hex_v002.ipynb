{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f621791-a034-4ae7-adff-d317fc74a95c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c322b0-c3b4-4afd-a133-853108c19d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages']\n",
      "5.29.5\n",
      "/opt/conda/lib/python3.10/site-packages/google/protobuf/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import google.protobuf\n",
    "print(google.protobuf.__version__)\n",
    "print(google.protobuf.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d23990-6c8a-479c-acd6-09682290c3d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.10/site-packages (1.96.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59010d48-0323-4f97-8b4c-8813cb2dbd10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U google-cloud-aiplatform tensorflow \n",
    "# !pip install --upgrade google-cloud-aiplatform google-api-core protobuf grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8e2175-8417-4455-b0f4-331c175862d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 16:33:36.539138: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-15 16:33:36.547056: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-15 16:33:36.564659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752597216.594528   50959 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752597216.602960   50959 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752597216.623829   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752597216.623864   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752597216.623867   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752597216.623870   50959 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-15 16:33:36.632015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-9273242a-17cc-4036-9dc3-0ff15e095d65\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"europe-west4\" #\"us-south1\" #\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = LOCATION # \"us-south1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c29f83-504a-4ae3-883b-c155bc92a3e3",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed657dc-3152-40b9-a555-b9b00120cf38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Endpoint initialized: <google.cloud.aiplatform.models.Endpoint object at 0x7f8ea7219a20> \n",
      "resource name: projects/87995179092/locations/europe-west4/endpoints/8393005462395551744\n",
      "üåê DNS: 8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog\n",
      "üìã Resource: projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive TPU Endpoint Benchmark Suite\n",
    "Based on working TPU endpoint code with full benchmarking capabilities\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, List, Dict, Union, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Google Cloud imports\n",
    "import google.auth\n",
    "import openai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Configuration - Set your actual values here\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-project-id\")\n",
    "REGION = \"europe-west4\"\n",
    "endpoint_name = \"8393005462395551744\"\n",
    "\n",
    "# Initialize endpoint\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "use_dedicated_endpoint = True\n",
    "\n",
    "print('üîß Endpoint initialized:', endpoint)\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoint.gca_resource.dedicated_endpoint_dns\n",
    "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
    "    PROJECT_ID, REGION, endpoint.name\n",
    ")\n",
    "print(f\"üåê DNS: {DEDICATED_ENDPOINT_DNS}\")\n",
    "print(f\"üìã Resource: {ENDPOINT_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827ca80-21f7-4e80-8ac2-8364e1c629fd",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de733ed-07e9-4d3f-93a9-75880becf0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ TPU Endpoint Benchmark Suite - Examples\n",
      "\n",
      "This benchmark suite uses your exact working TPU endpoint code pattern with comprehensive performance analysis.\n",
      "\n",
      "# Basic Examples:\n",
      "\n",
      "1. Simple Test (like your original):\n",
      "   run_simple_test(\"Write a poem about AI\", max_tokens=200, temperature=0.8)\n",
      "\n",
      "2. Token Length Experiments:\n",
      "   run_token_length_experiment(\"small\")     # 250 tokens\n",
      "   run_token_length_experiment(\"medium\")    # 465 tokens\n",
      "   run_token_length_experiment(\"large\")     # 1500 tokens\n",
      "   run_token_length_experiment(\"xlarge\")    # 3250 tokens\n",
      "\n",
      "3. Comprehensive Study:\n",
      "   run_comprehensive_token_length_study(model_name=\"llama3.3_tpuv6e\")\n",
      "\n",
      "4. Quick Test (reduced sizes):\n",
      "   quick_token_length_test(model_name=\"llama3.3_tpuv6e\")\n",
      "\n",
      "5. Custom Benchmark:\n",
      "   run_custom_benchmark(input_tokens=1000, output_tokens=500, num_requests=50, concurrency=5)\n",
      "\n",
      "6. Throughput Scaling Test:\n",
      "   run_throughput_scaling_test(base_concurrency=5, max_concurrency=50, step=5)\n",
      "\n",
      "üîß Current Configuration:\n",
      "   üìã Project: tpu-launchpad-playground\n",
      "   üåç Region: europe-west4\n",
      "   üñ•Ô∏è Endpoint: 8393005462395551744\n",
      "   üîó DNS: 8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog\n",
      "\n",
      "üéØ Running Demo Test...\n",
      "üöÄ Running Simple Test\n",
      "   üìù Prompt: Explain the benefits of TPU for machine learning in 3 sentences.\n",
      "   üéØ Max tokens: 100\n",
      "   üå°Ô∏è Temperature: 0.7\n",
      "   üîÑ Stream: True\n",
      "------------------------------------------------------------\n",
      "üì° Streaming response:\n",
      "----------------------------------------\n",
      "‚ö° TTFT: 0.464s\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "üìä Performance Metrics:\n",
      "----------------------------------------\n",
      "‚úÖ Request completed successfully\n",
      "‚è±Ô∏è  E2E Latency: 2.744s\n",
      "‚ö° TTFT: 0.464s\n",
      "üîÑ Average TPOT: 0.023s\n",
      "üìà Min/Max ITL: 0.012s / 0.030s\n",
      "üöÄ Tokens/second: 36.45\n",
      "üî¢ Total tokens: 100\n",
      "üìè Total characters: 100\n",
      "üíæ Usage: CompletionUsage(completion_tokens=100, prompt_tokens=50, total_tokens=150, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))\n",
      "‚úÖ Demo test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkRequest:\n",
    "    \"\"\"Request data structure for benchmarking\"\"\"\n",
    "    prompt: str\n",
    "    prompt_len: int\n",
    "    expected_output_len: int\n",
    "    request_id: int\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Result of a single benchmark request\"\"\"\n",
    "    request_id: int\n",
    "    success: bool\n",
    "    prompt_len: int\n",
    "    output_len: int\n",
    "    ttft: float  # Time to first token\n",
    "    tpot: float  # Time per output token\n",
    "    itl: float   # Inter-token latency\n",
    "    e2e_latency: float  # End-to-end latency\n",
    "    error_msg: str = \"\"\n",
    "    timestamp: float = 0.0\n",
    "    full_response: str = \"\"\n",
    "\n",
    "class RandomDataset:\n",
    "    \"\"\"Generate random prompts for benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int, range_ratio: float = 0.0):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "        self.range_ratio = range_ratio\n",
    "        \n",
    "    def _generate_random_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate a random prompt of specified length\"\"\"\n",
    "        words = [\n",
    "            \"analyze\", \"consider\", \"evaluate\", \"examine\", \"investigate\", \"review\", \"assess\", \"study\",\n",
    "            \"business\", \"technology\", \"strategy\", \"development\", \"innovation\", \"implementation\", \"solution\",\n",
    "            \"market\", \"customer\", \"product\", \"service\", \"quality\", \"performance\", \"efficiency\", \"growth\",\n",
    "            \"data\", \"information\", \"process\", \"system\", \"method\", \"approach\", \"framework\", \"model\",\n",
    "            \"challenge\", \"opportunity\", \"risk\", \"benefit\", \"advantage\", \"improvement\", \"optimization\",\n",
    "            \"research\", \"analysis\", \"report\", \"recommendation\", \"conclusion\", \"insight\", \"finding\",\n",
    "            \"artificial\", \"intelligence\", \"machine\", \"learning\", \"neural\", \"network\", \"algorithm\",\n",
    "            \"compute\", \"processor\", \"memory\", \"storage\", \"bandwidth\", \"latency\", \"throughput\"\n",
    "        ]\n",
    "        \n",
    "        prompt_words = []\n",
    "        target_words = int(length * 0.75)  # Rough token-to-word conversion\n",
    "        \n",
    "        while len(prompt_words) < target_words:\n",
    "            prompt_words.append(random.choice(words))\n",
    "        \n",
    "        # Add a question or instruction to make it more realistic\n",
    "        prompt_base = \" \".join(prompt_words)\n",
    "        prompts = [\n",
    "            f\"Explain the following concepts in detail: {prompt_base}\",\n",
    "            f\"Write a comprehensive analysis of: {prompt_base}\",\n",
    "            f\"Describe the relationship between: {prompt_base}\",\n",
    "            f\"Provide insights about: {prompt_base}\",\n",
    "            f\"Create a detailed report on: {prompt_base}\"\n",
    "        ]\n",
    "        \n",
    "        return random.choice(prompts)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate benchmark requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            if self.range_ratio > 0:\n",
    "                input_variance = int(self.input_len * self.range_ratio)\n",
    "                output_variance = int(self.output_len * self.range_ratio)\n",
    "                \n",
    "                actual_input_len = random.randint(\n",
    "                    max(1, self.input_len - input_variance),\n",
    "                    self.input_len + input_variance\n",
    "                )\n",
    "                actual_output_len = random.randint(\n",
    "                    max(1, self.output_len - output_variance),\n",
    "                    self.output_len + output_variance\n",
    "                )\n",
    "            else:\n",
    "                actual_input_len = self.input_len\n",
    "                actual_output_len = self.output_len\n",
    "            \n",
    "            prompt = self._generate_random_prompt(actual_input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=actual_input_len,\n",
    "                expected_output_len=actual_output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class TPUBenchmarkEngine:\n",
    "    \"\"\"Benchmark engine using your exact working TPU endpoint pattern\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_lock = threading.Lock()\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "        \n",
    "        # Initialize authentication using your exact pattern\n",
    "        self.creds, self.project = google.auth.default()\n",
    "        self.auth_req = google.auth.transport.requests.Request()\n",
    "        \n",
    "        # Setup BASE_URL using your exact logic\n",
    "        self.BASE_URL = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "        \n",
    "        if use_dedicated_endpoint:\n",
    "            self.BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "        \n",
    "        print(f\"üîó Base URL: {self.BASE_URL}\")\n",
    "    \n",
    "    def _refresh_auth(self):\n",
    "        \"\"\"Refresh authentication token\"\"\"\n",
    "        self.creds.refresh(self.auth_req)\n",
    "    \n",
    "    def _make_streaming_request(self, request: BenchmarkRequest, \n",
    "                              temperature: float = 0.7, \n",
    "                              max_tokens: int = None,\n",
    "                              stream: bool = True) -> BenchmarkResult:\n",
    "        \"\"\"Make a streaming request using your exact working pattern\"\"\"\n",
    "        \n",
    "        if max_tokens is None:\n",
    "            max_tokens = request.expected_output_len + 50\n",
    "        \n",
    "        try:\n",
    "            # Refresh auth token\n",
    "            self._refresh_auth()\n",
    "            \n",
    "            # Create OpenAI client using your exact setup\n",
    "            client = openai.OpenAI(base_url=self.BASE_URL, api_key=self.creds.token)\n",
    "            \n",
    "            # Start timing\n",
    "            request_start = time.time()\n",
    "            ttft = None\n",
    "            last_token_time = request_start\n",
    "            inter_token_latencies = []\n",
    "            \n",
    "            # Make request using your exact model request pattern\n",
    "            model_response = client.chat.completions.create(\n",
    "                model=\"\",  # Your exact model parameter\n",
    "                messages=[{\"role\": \"user\", \"content\": request.prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "            )\n",
    "            \n",
    "            # Process streaming response using your exact logic\n",
    "            if stream:\n",
    "                usage = None\n",
    "                contents = []\n",
    "                token_count = 0\n",
    "                \n",
    "                for chunk in model_response:\n",
    "                    current_time = time.time()\n",
    "                    \n",
    "                    if chunk.usage is not None:\n",
    "                        usage = chunk.usage\n",
    "                        continue\n",
    "                    \n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    if content:  # Only process if there's actual content\n",
    "                        # Timing measurements using your exact pattern\n",
    "                        if ttft is None:\n",
    "                            ttft = current_time - request_start\n",
    "                        else:\n",
    "                            itl = current_time - last_token_time\n",
    "                            inter_token_latencies.append(itl)\n",
    "                        \n",
    "                        contents.append(content)\n",
    "                        token_count += 1\n",
    "                        last_token_time = current_time\n",
    "                \n",
    "                # Final measurements\n",
    "                e2e_latency = time.time() - request_start\n",
    "                full_text = ''.join(contents)\n",
    "                \n",
    "                # Calculate TPOT\n",
    "                if inter_token_latencies:\n",
    "                    avg_tpot = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "                    avg_itl = avg_tpot\n",
    "                else:\n",
    "                    avg_tpot = e2e_latency / max(1, token_count) if token_count > 0 else 0\n",
    "                    avg_itl = avg_tpot\n",
    "                \n",
    "                return BenchmarkResult(\n",
    "                    request_id=request.request_id,\n",
    "                    success=True,\n",
    "                    prompt_len=len(request.prompt.split()) * 1.3,  # Rough token estimate\n",
    "                    output_len=token_count,\n",
    "                    ttft=ttft if ttft else e2e_latency,\n",
    "                    tpot=avg_tpot,\n",
    "                    itl=avg_itl,\n",
    "                    e2e_latency=e2e_latency,\n",
    "                    timestamp=request_start,\n",
    "                    full_response=full_text\n",
    "                )\n",
    "            else:\n",
    "                # Non-streaming response\n",
    "                e2e_latency = time.time() - request_start\n",
    "                response_text = model_response.choices[0].message.content\n",
    "                token_count = len(response_text.split()) * 1.3  # Rough estimate\n",
    "                \n",
    "                # Estimate TTFT and TPOT for non-streaming\n",
    "                estimated_ttft = e2e_latency * 0.2  # 20% for processing\n",
    "                estimated_tpot = (e2e_latency - estimated_ttft) / max(1, token_count)\n",
    "                \n",
    "                return BenchmarkResult(\n",
    "                    request_id=request.request_id,\n",
    "                    success=True,\n",
    "                    prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                    output_len=int(token_count),\n",
    "                    ttft=estimated_ttft,\n",
    "                    tpot=estimated_tpot,\n",
    "                    itl=estimated_tpot,\n",
    "                    e2e_latency=e2e_latency,\n",
    "                    timestamp=request_start,\n",
    "                    full_response=response_text\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - request_start if 'request_start' in locals() else 0\n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=False,\n",
    "                prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                output_len=0,\n",
    "                ttft=0.0,\n",
    "                tpot=0.0,\n",
    "                itl=0.0,\n",
    "                e2e_latency=error_time,\n",
    "                error_msg=str(e),\n",
    "                timestamp=time.time(),\n",
    "                full_response=\"\"\n",
    "            )\n",
    "    \n",
    "    def run_benchmark(self, \n",
    "                      requests: List[BenchmarkRequest],\n",
    "                      max_concurrency: int = 100,\n",
    "                      temperature: float = 0.7,\n",
    "                      max_tokens: int = None,\n",
    "                      stream: bool = True,\n",
    "                      request_rate: float = float('inf')) -> List[BenchmarkResult]:\n",
    "        \"\"\"Run benchmark with specified parameters\"\"\"\n",
    "        \n",
    "        print(f\"üöÄ Starting benchmark with {len(requests)} requests...\")\n",
    "        print(f\"üë• Max concurrency: {max_concurrency}\")\n",
    "        print(f\"üå°Ô∏è Temperature: {temperature}\")\n",
    "        print(f\"üîÑ Streaming: {stream}\")\n",
    "        \n",
    "        self.results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Rate limiting setup\n",
    "        if request_rate != float('inf'):\n",
    "            request_interval = 1.0 / request_rate\n",
    "            print(f\"‚è±Ô∏è Request rate limit: {request_rate} req/s\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "            future_to_request = {}\n",
    "            \n",
    "            for i, req in enumerate(requests):\n",
    "                # Rate limiting\n",
    "                if request_rate != float('inf') and i > 0:\n",
    "                    time.sleep(request_interval)\n",
    "                \n",
    "                future = executor.submit(\n",
    "                    self._make_streaming_request, \n",
    "                    req, \n",
    "                    temperature, \n",
    "                    max_tokens, \n",
    "                    stream\n",
    "                )\n",
    "                future_to_request[future] = req\n",
    "            \n",
    "            with tqdm(total=len(requests), desc=\"üìä Processing requests\") as pbar:\n",
    "                for future in as_completed(future_to_request):\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        with self.results_lock:\n",
    "                            self.results.append(result)\n",
    "                    except Exception as e:\n",
    "                        request = future_to_request[future]\n",
    "                        error_result = BenchmarkResult(\n",
    "                            request_id=request.request_id,\n",
    "                            success=False,\n",
    "                            prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                            output_len=0,\n",
    "                            ttft=0.0,\n",
    "                            tpot=0.0,\n",
    "                            itl=0.0,\n",
    "                            e2e_latency=0.0,\n",
    "                            error_msg=f\"Future execution failed: {str(e)}\",\n",
    "                            timestamp=time.time(),\n",
    "                            full_response=\"\"\n",
    "                        )\n",
    "                        with self.results_lock:\n",
    "                            self.results.append(error_result)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        successful_count = len([r for r in self.results if r.success])\n",
    "        print(f\"‚úÖ Benchmark completed in {total_time:.2f} seconds\")\n",
    "        print(f\"üìà Success rate: {successful_count}/{len(requests)} ({successful_count/len(requests)*100:.1f}%)\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "class BenchmarkAnalyzer:\n",
    "    \"\"\"Analyze and report benchmark results\"\"\"\n",
    "    \n",
    "    def __init__(self, results: List[BenchmarkResult]):\n",
    "        self.results = results\n",
    "        self.successful_results = [r for r in results if r.success]\n",
    "        self.failed_results = [r for r in results if not r.success]\n",
    "    \n",
    "    def calculate_percentiles(self, values: List[float], percentiles: List[int]) -> Dict[int, float]:\n",
    "        \"\"\"Calculate percentiles for a list of values\"\"\"\n",
    "        if not values:\n",
    "            return {p: 0.0 for p in percentiles}\n",
    "        return {p: np.percentile(values, p) for p in percentiles}\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate benchmark summary statistics\"\"\"\n",
    "        if not self.successful_results:\n",
    "            return {\n",
    "                \"error\": \"No successful requests\",\n",
    "                \"total_requests\": len(self.results),\n",
    "                \"failed_requests\": len(self.failed_results)\n",
    "            }\n",
    "        \n",
    "        ttfts = [r.ttft for r in self.successful_results]\n",
    "        tpots = [r.tpot * 1000 for r in self.successful_results]  # Convert to ms\n",
    "        e2e_latencies = [r.e2e_latency for r in self.successful_results]\n",
    "        \n",
    "        total_input_tokens = sum(r.prompt_len for r in self.successful_results)\n",
    "        total_output_tokens = sum(r.output_len for r in self.successful_results)\n",
    "        \n",
    "        if self.successful_results:\n",
    "            timestamps = [r.timestamp for r in self.successful_results]\n",
    "            benchmark_duration = max(timestamps) - min(timestamps) + max(e2e_latencies)\n",
    "        else:\n",
    "            benchmark_duration = 1.0\n",
    "        \n",
    "        request_throughput = len(self.successful_results) / benchmark_duration\n",
    "        input_token_throughput = total_input_tokens / benchmark_duration\n",
    "        output_token_throughput = total_output_tokens / benchmark_duration\n",
    "        overall_token_throughput = (total_input_tokens + total_output_tokens) / benchmark_duration\n",
    "        \n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        \n",
    "        summary = {\n",
    "            \"successful_requests\": len(self.successful_results),\n",
    "            \"failed_requests\": len(self.failed_results),\n",
    "            \"total_requests\": len(self.results),\n",
    "            \"benchmark_duration\": benchmark_duration,\n",
    "            \"request_throughput\": request_throughput,\n",
    "            \"input_token_throughput\": input_token_throughput,\n",
    "            \"output_token_throughput\": output_token_throughput,\n",
    "            \"overall_token_throughput\": overall_token_throughput,\n",
    "            \"total_input_tokens\": total_input_tokens,\n",
    "            \"total_output_tokens\": total_output_tokens,\n",
    "            \"ttft_percentiles\": self.calculate_percentiles(ttfts, percentiles),\n",
    "            \"tpot_percentiles\": self.calculate_percentiles(tpots, percentiles),\n",
    "            \"e2e_latency_percentiles\": self.calculate_percentiles(e2e_latencies, percentiles)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_detailed_summary(self):\n",
    "        \"\"\"Print detailed performance summary\"\"\"\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            print(f\"‚ùå Error: {summary['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä DETAILED PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüìã REQUEST STATISTICS:\")\n",
    "        print(f\"   ‚úÖ Successful: {summary['successful_requests']:,}\")\n",
    "        print(f\"   ‚ùå Failed: {summary['failed_requests']:,}\")\n",
    "        print(f\"   üìä Success Rate: {summary['successful_requests']/summary['total_requests']*100:.1f}%\")\n",
    "        print(f\"   ‚è±Ô∏è Duration: {summary['benchmark_duration']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n‚ö° LATENCY METRICS:\")\n",
    "        print(f\"   üöÄ TTFT p50: {summary['ttft_percentiles'][50]*1000:.1f}ms\")\n",
    "        print(f\"   üöÄ TTFT p95: {summary['ttft_percentiles'][95]*1000:.1f}ms\")\n",
    "        print(f\"   üöÄ TTFT p99: {summary['ttft_percentiles'][99]*1000:.1f}ms\")\n",
    "        \n",
    "        print(f\"\\nüîÑ TIME PER OUTPUT TOKEN:\")\n",
    "        print(f\"   ‚ö° TPOT p50: {summary['tpot_percentiles'][50]:.1f}ms\")\n",
    "        print(f\"   ‚ö° TPOT p95: {summary['tpot_percentiles'][95]:.1f}ms\")\n",
    "        print(f\"   ‚ö° TPOT p99: {summary['tpot_percentiles'][99]:.1f}ms\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è END-TO-END LATENCY:\")\n",
    "        print(f\"   üìà E2E p50: {summary['e2e_latency_percentiles'][50]:.2f}s\")\n",
    "        print(f\"   üìà E2E p95: {summary['e2e_latency_percentiles'][95]:.2f}s\")\n",
    "        print(f\"   üìà E2E p99: {summary['e2e_latency_percentiles'][99]:.2f}s\")\n",
    "        \n",
    "        print(f\"\\nüöÄ THROUGHPUT METRICS:\")\n",
    "        print(f\"   üìä Requests/sec: {summary['request_throughput']:.2f}\")\n",
    "        print(f\"   üì§ Output tokens/sec: {summary['output_token_throughput']:.2f}\")\n",
    "        print(f\"   üìä Overall tokens/sec: {summary['overall_token_throughput']:.0f}\")\n",
    "        \n",
    "        print(f\"\\nüìä TOKEN STATISTICS:\")\n",
    "        print(f\"   üì• Total input tokens: {summary['total_input_tokens']:,}\")\n",
    "        print(f\"   üì§ Total output tokens: {summary['total_output_tokens']:,}\")\n",
    "        print(f\"   üìä Avg input/request: {summary['total_input_tokens']/summary['successful_requests']:.1f}\")\n",
    "        print(f\"   üìä Avg output/request: {summary['total_output_tokens']/summary['successful_requests']:.1f}\")\n",
    "\n",
    "# Token Length Test Configurations\n",
    "TOKEN_LENGTH_EXPERIMENTS = {\n",
    "    \"small\": {\n",
    "        \"name\": \"Token Length - Small(250)\",\n",
    "        \"input_tokens\": 250,\n",
    "        \"output_tokens\": 250,\n",
    "        \"total_tokens\": 500,\n",
    "        \"concurrency\": 250,\n",
    "        \"num_requests\": 2000,\n",
    "        \"description\": \"Small token length test\"\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"name\": \"Token Length - Medium(350 - 580)\",\n",
    "        \"input_tokens\": 465,  # Average of 350-580\n",
    "        \"output_tokens\": 465,\n",
    "        \"total_tokens\": 930,\n",
    "        \"concurrency\": 150,\n",
    "        \"num_requests\": 1500,\n",
    "        \"description\": \"Medium token length test\"\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"name\": \"Token Length - Large(1200 - 1800)\",\n",
    "        \"input_tokens\": 1500,  # Average of 1200-1800\n",
    "        \"output_tokens\": 1500,\n",
    "        \"total_tokens\": 3000,\n",
    "        \"concurrency\": 100,\n",
    "        \"num_requests\": 1000,\n",
    "        \"description\": \"Large token length test\"\n",
    "    },\n",
    "    \"xlarge\": {\n",
    "        \"name\": \"Token Length - Xlarge(2.5k - 4k)\",\n",
    "        \"input_tokens\": 3250,  # Average of 2.5k-4k\n",
    "        \"output_tokens\": 1000,  # Reasonable output for very long input\n",
    "        \"total_tokens\": 4250,\n",
    "        \"concurrency\": 30,\n",
    "        \"num_requests\": 500,\n",
    "        \"description\": \"Extra large token length test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_simple_test(prompt: str = \"Write a short poem about artificial intelligence\",\n",
    "                   max_tokens: int = 200,\n",
    "                   temperature: float = 0.8,\n",
    "                   stream: bool = True):\n",
    "    \"\"\"Run a simple test using your exact working pattern\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Running Simple Test\")\n",
    "    print(f\"   üìù Prompt: {prompt}\")\n",
    "    print(f\"   üéØ Max tokens: {max_tokens}\")\n",
    "    print(f\"   üå°Ô∏è Temperature: {temperature}\")\n",
    "    print(f\"   üîÑ Stream: {stream}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Use your exact authentication setup\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Use your exact BASE_URL setup\n",
    "    BASE_URL = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "    \n",
    "    if use_dedicated_endpoint:\n",
    "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "    \n",
    "    # Use your exact client setup\n",
    "    client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
    "    \n",
    "    # Start timing\n",
    "    request_start = time.time()\n",
    "    ttft = None\n",
    "    last_token_time = request_start\n",
    "    inter_token_latencies = []\n",
    "    \n",
    "    # Your exact model request\n",
    "    model_response = client.chat.completions.create(\n",
    "        model=\"\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    print(\"üì° Streaming response:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Your exact streaming logic with timing added\n",
    "    if stream:\n",
    "        usage = None\n",
    "        contents = []\n",
    "        token_count = 0\n",
    "        \n",
    "        for chunk in model_response:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if chunk.usage is not None:\n",
    "                usage = chunk.usage\n",
    "                continue\n",
    "            \n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Only process if there's actual content\n",
    "                # Timing measurements\n",
    "                if ttft is None:\n",
    "                    ttft = current_time - request_start\n",
    "                    print(f\"‚ö° TTFT: {ttft:.3f}s\")\n",
    "                else:\n",
    "                    itl = current_time - last_token_time\n",
    "                    inter_token_latencies.append(itl)\n",
    "                \n",
    "                print(content, end=\"\", flush=True)\n",
    "                contents.append(content)\n",
    "                token_count += 1\n",
    "                last_token_time = current_time\n",
    "        \n",
    "        # Final measurements\n",
    "        e2e_latency = time.time() - request_start\n",
    "        \n",
    "        print(f\"\\n\\nüìä Performance Metrics:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"‚úÖ Request completed successfully\")\n",
    "        print(f\"‚è±Ô∏è  E2E Latency: {e2e_latency:.3f}s\")\n",
    "        print(f\"‚ö° TTFT: {ttft:.3f}s\" if ttft else \"‚ö° TTFT: N/A\")\n",
    "        \n",
    "        if inter_token_latencies:\n",
    "            avg_tpot = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "            print(f\"üîÑ Average TPOT: {avg_tpot:.3f}s\")\n",
    "            print(f\"üìà Min/Max ITL: {min(inter_token_latencies):.3f}s / {max(inter_token_latencies):.3f}s\")\n",
    "            print(f\"üöÄ Tokens/second: {token_count / e2e_latency:.2f}\")\n",
    "        \n",
    "        print(f\"üî¢ Total tokens: {token_count}\")\n",
    "        print(f\"üìè Total characters: {len(''.join(contents))}\")\n",
    "        \n",
    "        if usage:\n",
    "            print(f\"üíæ Usage: {usage}\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'ttft': ttft,\n",
    "            'e2e_latency': e2e_latency,\n",
    "            'token_count': token_count,\n",
    "            'inter_token_latencies': inter_token_latencies,\n",
    "            'full_text': ''.join(contents),\n",
    "            'usage': usage\n",
    "        }\n",
    "    else:\n",
    "        response_text = model_response.choices[0].message.content\n",
    "        e2e_latency = time.time() - request_start\n",
    "        print(f\"üìÑ Response: {response_text}\")\n",
    "        print(f\"‚è±Ô∏è E2E Latency: {e2e_latency:.3f}s\")\n",
    "        return {'success': True, 'response': response_text, 'e2e_latency': e2e_latency}\n",
    "\n",
    "def run_token_length_experiment(experiment_name: str,\n",
    "                               model_name: str = \"llama3.3_tpuv6e\",\n",
    "                               device_type: str = \"TPU v6e\") -> Dict[str, Any]:\n",
    "    \"\"\"Run a specific token length experiment\"\"\"\n",
    "    \n",
    "    if experiment_name not in TOKEN_LENGTH_EXPERIMENTS:\n",
    "        available = \", \".join(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "        raise ValueError(f\"Unknown experiment '{experiment_name}'. Available: {available}\")\n",
    "    \n",
    "    config = TOKEN_LENGTH_EXPERIMENTS[experiment_name]\n",
    "    \n",
    "    print(f\"\\nüî¨ Running Token Length Experiment: {config['name']}\")\n",
    "    print(f\"üìä Input tokens: {config['input_tokens']}\")\n",
    "    print(f\"üìä Output tokens: {config['output_tokens']}\")\n",
    "    print(f\"üìä Total tokens: {config['total_tokens']}\")\n",
    "    print(f\"üöÄ Concurrency: {config['concurrency']}\")\n",
    "    print(f\"üìù Requests: {config['num_requests']}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = RandomDataset(\n",
    "        input_len=config['input_tokens'],\n",
    "        output_len=config['output_tokens'],\n",
    "        num_requests=config['num_requests']\n",
    "    )\n",
    "    \n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine()\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=config['concurrency'],\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_detailed_summary()\n",
    "    summary = analyzer.generate_summary()\n",
    "    \n",
    "    if \"error\" not in summary:\n",
    "        print(f\"\\n‚úÖ Experiment Complete: {config['name']}\")\n",
    "        print(f\"üìà TTFT-P95: {summary['ttft_percentiles'][95]:.2f}s\")\n",
    "        print(f\"üìà Token Output Throughput: {summary['output_token_throughput']:.2f} tok/s\")\n",
    "        print(f\"üìà Overall Token Throughput: {summary['overall_token_throughput']:.0f} tok/s\")\n",
    "        print(f\"üë• Concurrent Users: {config['concurrency']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Experiment Failed: {summary['error']}\")\n",
    "    \n",
    "    # Add experiment metadata\n",
    "    summary.update({\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"experiment_config\": config,\n",
    "        \"model_name\": model_name,\n",
    "        \"device_type\": device_type,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def run_comprehensive_token_length_study(model_name: str = \"llama3.3_tpuv6e\",\n",
    "                                        device_type: str = \"v6e-8 TPU (256 GB HBM)\",\n",
    "                                        experiments: List[str] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Run comprehensive token length study across all experiments\"\"\"\n",
    "    \n",
    "    if experiments is None:\n",
    "        experiments = list(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "    \n",
    "    print(f\"üöÄ Starting Comprehensive Token Length Study\")\n",
    "    print(f\"üìã Model: {model_name}\")\n",
    "    print(f\"üñ•Ô∏è Device: {device_type}\")\n",
    "    print(f\"üß™ Experiments: {len(experiments)}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_name in experiments:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name,\n",
    "                device_type=device_type\n",
    "            )\n",
    "            all_results[exp_name] = result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Experiment {exp_name} failed: {e}\")\n",
    "            all_results[exp_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"experiment_name\": exp_name\n",
    "            }\n",
    "    \n",
    "    # Generate comparison report\n",
    "    generate_token_length_comparison_report(all_results, model_name, device_type)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_token_length_comparison_report(all_results: Dict[str, Dict[str, Any]], \n",
    "                                          model_name: str,\n",
    "                                          device_type: str):\n",
    "    \"\"\"Generate comparison report in CSV format matching your screenshot with ALL metrics\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"token_length_benchmarks\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate comprehensive CSV report with ALL metrics\n",
    "    comprehensive_csv_data = []\n",
    "    \n",
    "    for exp_name, result in all_results.items():\n",
    "        if \"error\" not in result:\n",
    "            config = TOKEN_LENGTH_EXPERIMENTS[exp_name]\n",
    "            \n",
    "            # Base row data\n",
    "            base_row = {\n",
    "                \"Token_Length_Category\": config['name'],\n",
    "                \"Device_Type\": device_type,\n",
    "                \"Model_Name\": result['model_name'],\n",
    "                \"Input_Tokens\": config['input_tokens'],\n",
    "                \"Output_Tokens\": config['output_tokens'],\n",
    "                \"Total_Tokens\": config['total_tokens'],\n",
    "                \"Concurrent_Users\": config['concurrency'],\n",
    "                \"Total_Requests\": config['num_requests'],\n",
    "                \"Successful_Requests\": result['successful_requests'],\n",
    "                \"Failed_Requests\": result['failed_requests'],\n",
    "                \"Success_Rate_Percent\": f\"{result['successful_requests'] / result['total_requests'] * 100:.1f}%\",\n",
    "                \"Test_Duration_Seconds\": f\"{result['benchmark_duration']:.2f}\",\n",
    "                \n",
    "                # Key Performance Metrics (Your 4 main ones)\n",
    "                \"TTFT_P95_Seconds\": f\"{result['ttft_percentiles'][95]:.3f}\",\n",
    "                \"Token_Output_Throughput_Per_Second\": f\"{result['output_token_throughput']:.2f}\",\n",
    "                \"Overall_Token_Throughput\": f\"{result['overall_token_throughput']:.0f}\",\n",
    "                \n",
    "                # Complete TTFT Metrics\n",
    "                \"TTFT_P50_Seconds\": f\"{result['ttft_percentiles'][50]:.3f}\",\n",
    "                \"TTFT_P90_Seconds\": f\"{result['ttft_percentiles'][90]:.3f}\",\n",
    "                \"TTFT_P99_Seconds\": f\"{result['ttft_percentiles'][99]:.3f}\",\n",
    "                \n",
    "                # Complete TPOT Metrics\n",
    "                \"TPOT_P50_ms\": f\"{result['tpot_percentiles'][50]:.1f}\",\n",
    "                \"TPOT_P90_ms\": f\"{result['tpot_percentiles'][90]:.1f}\",\n",
    "                \"TPOT_P95_ms\": f\"{result['tpot_percentiles'][95]:.1f}\",\n",
    "                \"TPOT_P99_ms\": f\"{result['tpot_percentiles'][99]:.1f}\",\n",
    "                \n",
    "                # Complete End-to-End Latency Metrics\n",
    "                \"E2E_Latency_P50_Seconds\": f\"{result['e2e_latency_percentiles'][50]:.2f}\",\n",
    "                \"E2E_Latency_P90_Seconds\": f\"{result['e2e_latency_percentiles'][90]:.2f}\",\n",
    "                \"E2E_Latency_P95_Seconds\": f\"{result['e2e_latency_percentiles'][95]:.2f}\",\n",
    "                \"E2E_Latency_P99_Seconds\": f\"{result['e2e_latency_percentiles'][99]:.2f}\",\n",
    "                \n",
    "                # Throughput Metrics\n",
    "                \"Request_Throughput_Per_Second\": f\"{result['request_throughput']:.2f}\",\n",
    "                \"Input_Token_Throughput_Per_Second\": f\"{result['input_token_throughput']:.2f}\",\n",
    "                \n",
    "                # Token Statistics\n",
    "                \"Total_Input_Tokens_Processed\": f\"{result['total_input_tokens']:,}\",\n",
    "                \"Total_Output_Tokens_Generated\": f\"{result['total_output_tokens']:,}\",\n",
    "                \"Avg_Input_Tokens_Per_Request\": f\"{result['total_input_tokens'] / result['successful_requests']:.1f}\",\n",
    "                \"Avg_Output_Tokens_Per_Request\": f\"{result['total_output_tokens'] / result['successful_requests']:.1f}\",\n",
    "                \n",
    "                # Efficiency Metrics\n",
    "                \"Tokens_Per_Second_Per_User\": f\"{result['overall_token_throughput'] / config['concurrency']:.2f}\",\n",
    "                \"Requests_Per_Second_Per_User\": f\"{result['request_throughput'] / config['concurrency']:.3f}\",\n",
    "                \"Output_Input_Token_Ratio\": f\"{(result['total_output_tokens'] / result['total_input_tokens']):.2f}\",\n",
    "                \n",
    "                # Test Configuration\n",
    "                \"Temperature\": \"0.7\",\n",
    "                \"Streaming\": \"True\",\n",
    "                \"Test_Timestamp\": result['timestamp']\n",
    "            }\n",
    "            \n",
    "            comprehensive_csv_data.append(base_row)\n",
    "    \n",
    "    # Save comprehensive CSV\n",
    "    comprehensive_csv_file = os.path.join(output_dir, f\"comprehensive_token_length_analysis_{model_name.replace('.', '_')}_{timestamp}.csv\")\n",
    "    comprehensive_df = pd.DataFrame(comprehensive_csv_data)\n",
    "    comprehensive_df.to_csv(comprehensive_csv_file, index=False)\n",
    "    \n",
    "    # Generate formatted comparison table (key metrics only - like your screenshot)\n",
    "    key_metrics_data = []\n",
    "    \n",
    "    for exp_name, result in all_results.items():\n",
    "        if \"error\" not in result:\n",
    "            config = TOKEN_LENGTH_EXPERIMENTS[exp_name]\n",
    "            \n",
    "            key_metrics_data.append({\n",
    "                \"Token_Length_Category\": config['name'],\n",
    "                \"TTFT_P95_Seconds\": f\"{result['ttft_percentiles'][95]:.3f}\",\n",
    "                \"Token_Output_Throughput_Per_Second\": f\"{result['output_token_throughput']:.0f}\",\n",
    "                \"Overall_Token_Throughput\": f\"{result['overall_token_throughput']:.0f}\",\n",
    "                \"Concurrent_Users\": f\"{config['concurrency']}\"\n",
    "            })\n",
    "    \n",
    "    # Save key metrics comparison (matching your screenshot format)\n",
    "    key_metrics_file = os.path.join(output_dir, f\"key_metrics_comparison_{model_name.replace('.', '_')}_{timestamp}.csv\")\n",
    "    key_metrics_df = pd.DataFrame(key_metrics_data)\n",
    "    key_metrics_df.to_csv(key_metrics_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüìä Comprehensive Reports Generated:\")\n",
    "    print(f\"   üéØ Key Metrics CSV (Your Format): {key_metrics_file}\")\n",
    "    print(f\"   üìä Comprehensive Analysis CSV: {comprehensive_csv_file}\")\n",
    "\n",
    "def quick_token_length_test(model_name: str = \"llama3.3_tpuv6e\"):\n",
    "    \"\"\"Run a quick test across all token length categories with reduced sizes\"\"\"\n",
    "    \n",
    "    # Reduced test sizes for quick evaluation\n",
    "    quick_experiments = {\n",
    "        \"small\": {\"num_requests\": 100, \"concurrency\": 25},\n",
    "        \"medium\": {\"num_requests\": 75, \"concurrency\": 15},\n",
    "        \"large\": {\"num_requests\": 50, \"concurrency\": 10},\n",
    "        \"xlarge\": {\"num_requests\": 25, \"concurrency\": 5}\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Running Quick Token Length Test...\")\n",
    "    \n",
    "    results = {}\n",
    "    for exp_name in quick_experiments:\n",
    "        # Temporarily modify config\n",
    "        original_config = TOKEN_LENGTH_EXPERIMENTS[exp_name].copy()\n",
    "        TOKEN_LENGTH_EXPERIMENTS[exp_name].update(quick_experiments[exp_name])\n",
    "        \n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name\n",
    "            )\n",
    "            results[exp_name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Quick test {exp_name} failed: {e}\")\n",
    "            results[exp_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Restore original config\n",
    "        TOKEN_LENGTH_EXPERIMENTS[exp_name] = original_config\n",
    "    \n",
    "    # Generate quick report\n",
    "    generate_token_length_comparison_report(results, model_name, \"v6e-8 TPU (256 GB HBM)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_custom_benchmark(input_tokens: int = 500,\n",
    "                        output_tokens: int = 500,\n",
    "                        num_requests: int = 100,\n",
    "                        concurrency: int = 10,\n",
    "                        temperature: float = 0.7,\n",
    "                        stream: bool = True,\n",
    "                        model_name: str = \"custom_test\"):\n",
    "    \"\"\"Run a custom benchmark with specified parameters\"\"\"\n",
    "    \n",
    "    print(f\"üß™ Running Custom Benchmark\")\n",
    "    print(f\"üìä Input tokens: {input_tokens}\")\n",
    "    print(f\"üìä Output tokens: {output_tokens}\")\n",
    "    print(f\"üìù Requests: {num_requests}\")\n",
    "    print(f\"üöÄ Concurrency: {concurrency}\")\n",
    "    print(f\"üå°Ô∏è Temperature: {temperature}\")\n",
    "    print(f\"üîÑ Streaming: {stream}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = RandomDataset(\n",
    "        input_len=input_tokens,\n",
    "        output_len=output_tokens,\n",
    "        num_requests=num_requests\n",
    "    )\n",
    "    \n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine()\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=concurrency,\n",
    "        temperature=temperature,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_detailed_summary()\n",
    "    \n",
    "    return analyzer.generate_summary()\n",
    "\n",
    "def run_throughput_scaling_test(base_concurrency: int = 10,\n",
    "                              max_concurrency: int = 100,\n",
    "                              step: int = 10,\n",
    "                              num_requests: int = 200):\n",
    "    \"\"\"Test how throughput scales with concurrency\"\"\"\n",
    "    \n",
    "    print(f\"üìà Running Throughput Scaling Test\")\n",
    "    print(f\"üöÄ Concurrency range: {base_concurrency} to {max_concurrency} (step {step})\")\n",
    "    print(f\"üìù Requests per test: {num_requests}\")\n",
    "    \n",
    "    scaling_results = []\n",
    "    \n",
    "    for concurrency in range(base_concurrency, max_concurrency + 1, step):\n",
    "        print(f\"\\nüß™ Testing concurrency: {concurrency}\")\n",
    "        \n",
    "        # Generate small dataset for quick testing\n",
    "        dataset = RandomDataset(\n",
    "            input_len=250,\n",
    "            output_len=250,\n",
    "            num_requests=num_requests\n",
    "        )\n",
    "        \n",
    "        requests = dataset.generate_requests()\n",
    "        \n",
    "        # Run benchmark\n",
    "        engine = TPUBenchmarkEngine()\n",
    "        results = engine.run_benchmark(\n",
    "            requests=requests,\n",
    "            max_concurrency=concurrency,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Analyze results\n",
    "        analyzer = BenchmarkAnalyzer(results)\n",
    "        summary = analyzer.generate_summary()\n",
    "        \n",
    "        if \"error\" not in summary:\n",
    "            scaling_results.append({\n",
    "                \"concurrency\": concurrency,\n",
    "                \"request_throughput\": summary['request_throughput'],\n",
    "                \"output_token_throughput\": summary['output_token_throughput'],\n",
    "                \"overall_token_throughput\": summary['overall_token_throughput'],\n",
    "                \"ttft_p95\": summary['ttft_percentiles'][95],\n",
    "                \"tpot_p95\": summary['tpot_percentiles'][95],\n",
    "                \"success_rate\": summary['successful_requests'] / summary['total_requests']\n",
    "            })\n",
    "            \n",
    "            print(f\"   üìà Throughput: {summary['overall_token_throughput']:.0f} tok/s\")\n",
    "            print(f\"   ‚ö° TTFT p95: {summary['ttft_percentiles'][95]*1000:.1f}ms\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed: {summary['error']}\")\n",
    "    \n",
    "    # Save scaling results\n",
    "    if scaling_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = \"token_length_benchmarks\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        scaling_df = pd.DataFrame(scaling_results)\n",
    "        scaling_file = os.path.join(output_dir, f\"throughput_scaling_{timestamp}.csv\")\n",
    "        scaling_df.to_csv(scaling_file, index=False)\n",
    "        \n",
    "        print(f\"\\nüìä Scaling Results Saved: {scaling_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìà THROUGHPUT SCALING SUMMARY:\")\n",
    "        print(f\"   üèÜ Peak throughput: {max(r['overall_token_throughput'] for r in scaling_results):.0f} tok/s\")\n",
    "        print(f\"   üéØ Optimal concurrency: {max(scaling_results, key=lambda x: x['overall_token_throughput'])['concurrency']}\")\n",
    "    \n",
    "    return scaling_results\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Show example usage and run basic tests\"\"\"\n",
    "    print(\"\"\"\n",
    "üöÄ TPU Endpoint Benchmark Suite - Examples\n",
    "\n",
    "This benchmark suite uses your exact working TPU endpoint code pattern with comprehensive performance analysis.\n",
    "\n",
    "# Basic Examples:\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"1. Simple Test (like your original):\")\n",
    "    print('   run_simple_test(\"Write a poem about AI\", max_tokens=200, temperature=0.8)')\n",
    "    \n",
    "    print(\"\\n2. Token Length Experiments:\")\n",
    "    print('   run_token_length_experiment(\"small\")     # 250 tokens')\n",
    "    print('   run_token_length_experiment(\"medium\")    # 465 tokens') \n",
    "    print('   run_token_length_experiment(\"large\")     # 1500 tokens')\n",
    "    print('   run_token_length_experiment(\"xlarge\")    # 3250 tokens')\n",
    "    \n",
    "    print(\"\\n3. Comprehensive Study:\")\n",
    "    print('   run_comprehensive_token_length_study(model_name=\"llama3.3_tpuv6e\")')\n",
    "    \n",
    "    print(\"\\n4. Quick Test (reduced sizes):\")\n",
    "    print('   quick_token_length_test(model_name=\"llama3.3_tpuv6e\")')\n",
    "    \n",
    "    print(\"\\n5. Custom Benchmark:\")\n",
    "    print('   run_custom_benchmark(input_tokens=1000, output_tokens=500, num_requests=50, concurrency=5)')\n",
    "    \n",
    "    print(\"\\n6. Throughput Scaling Test:\")\n",
    "    print('   run_throughput_scaling_test(base_concurrency=5, max_concurrency=50, step=5)')\n",
    "    \n",
    "    print(f\"\\nüîß Current Configuration:\")\n",
    "    print(f\"   üìã Project: {PROJECT_ID}\")\n",
    "    print(f\"   üåç Region: {REGION}\")\n",
    "    print(f\"   üñ•Ô∏è Endpoint: {endpoint_name}\")\n",
    "    print(f\"   üîó DNS: {DEDICATED_ENDPOINT_DNS if use_dedicated_endpoint else 'Standard'}\")\n",
    "    \n",
    "    # Run a simple demo\n",
    "    print(f\"\\nüéØ Running Demo Test...\")\n",
    "    try:\n",
    "        demo_result = run_simple_test(\n",
    "            prompt=\"Explain the benefits of TPU for machine learning in 3 sentences.\",\n",
    "            max_tokens=100,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        if demo_result['success']:\n",
    "            print(\"‚úÖ Demo test completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Demo test failed: {e}\")\n",
    "        print(\"Please check your PROJECT_ID, REGION, and endpoint_name configuration.\")\n",
    "\n",
    "# Predefined test suites\n",
    "def production_readiness_test():\n",
    "    \"\"\"Comprehensive production readiness test\"\"\"\n",
    "    print(\"üè≠ Running Production Readiness Test Suite...\")\n",
    "    \n",
    "    tests = [\n",
    "        (\"Latency Test\", lambda: run_token_length_experiment(\"small\")),\n",
    "        (\"Medium Load Test\", lambda: run_token_length_experiment(\"medium\")),\n",
    "        (\"High Load Test\", lambda: run_token_length_experiment(\"large\")),\n",
    "        (\"Scaling Test\", lambda: run_throughput_scaling_test(10, 50, 10, 100))\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for test_name, test_func in tests:\n",
    "        print(f\"\\nüß™ {test_name}...\")\n",
    "        try:\n",
    "            results[test_name] = test_func()\n",
    "            print(f\"‚úÖ {test_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {test_name} failed: {e}\")\n",
    "            results[test_name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def stress_test():\n",
    "    \"\"\"High concurrency stress test\"\"\"\n",
    "    print(\"üí™ Running Stress Test...\")\n",
    "    \n",
    "    return run_custom_benchmark(\n",
    "        input_tokens=500,\n",
    "        output_tokens=500, \n",
    "        num_requests=500,\n",
    "        concurrency=100,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "        model_name=\"stress_test\"\n",
    "    )\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If in notebook, show examples\n",
    "        example_usage()\n",
    "    except NameError:\n",
    "        # If script execution, show usage\n",
    "        import argparse\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description=\"TPU Endpoint Benchmark Suite\")\n",
    "        parser.add_argument(\"--test\", choices=[\"simple\", \"small\", \"medium\", \"large\", \"xlarge\", \"comprehensive\", \"quick\", \"scaling\", \"stress\", \"production\"], \n",
    "                           default=\"simple\", help=\"Test type to run\")\n",
    "        parser.add_argument(\"--model\", default=\"llama3.3_tpuv6e\", help=\"Model name\")\n",
    "        parser.add_argument(\"--requests\", type=int, default=100, help=\"Number of requests\")\n",
    "        parser.add_argument(\"--concurrency\", type=int, default=10, help=\"Concurrency level\")\n",
    "        parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Temperature\")\n",
    "        parser.add_argument(\"--max-tokens\", type=int, default=200, help=\"Max tokens\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        if args.test == \"simple\":\n",
    "            run_simple_test(max_tokens=args.max_tokens, temperature=args.temperature)\n",
    "        elif args.test in [\"small\", \"medium\", \"large\", \"xlarge\"]:\n",
    "            run_token_length_experiment(args.test, args.model)\n",
    "        elif args.test == \"comprehensive\":\n",
    "            run_comprehensive_token_length_study(args.model)\n",
    "        elif args.test == \"quick\":\n",
    "            quick_token_length_test(args.model)\n",
    "        elif args.test == \"scaling\":\n",
    "            run_throughput_scaling_test()\n",
    "        elif args.test == \"stress\":\n",
    "            stress_test()\n",
    "        elif args.test == \"production\":\n",
    "            production_readiness_test()\n",
    "        else:\n",
    "            example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44971e-ace1-4ed8-a6b1-3af5d463b35e",
   "metadata": {},
   "source": [
    "### 1. Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57b89d1-b322-4b2a-8eb9-8713ececc5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Simple Test\n",
      "   üìù Prompt: Write a poem about AI\n",
      "   üéØ Max tokens: 200\n",
      "   üå°Ô∏è Temperature: 0.8\n",
      "   üîÑ Stream: True\n",
      "------------------------------------------------------------\n",
      "üì° Streaming response:\n",
      "----------------------------------------\n",
      "‚ö° TTFT: 0.121s\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "üìä Performance Metrics:\n",
      "----------------------------------------\n",
      "‚úÖ Request completed successfully\n",
      "‚è±Ô∏è  E2E Latency: 4.802s\n",
      "‚ö° TTFT: 0.121s\n",
      "üîÑ Average TPOT: 0.024s\n",
      "üìà Min/Max ITL: 0.001s / 0.134s\n",
      "üöÄ Tokens/second: 41.65\n",
      "üî¢ Total tokens: 200\n",
      "üìè Total characters: 200\n",
      "üíæ Usage: CompletionUsage(completion_tokens=200, prompt_tokens=40, total_tokens=240, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'ttft': 0.12057781219482422,\n",
       " 'e2e_latency': 4.801581859588623,\n",
       " 'token_count': 200,\n",
       " 'inter_token_latencies': [0.0223085880279541,\n",
       "  0.022749662399291992,\n",
       "  0.023308277130126953,\n",
       "  0.02325892448425293,\n",
       "  0.0234677791595459,\n",
       "  0.022517919540405273,\n",
       "  0.02348160743713379,\n",
       "  0.024016141891479492,\n",
       "  0.022310256958007812,\n",
       "  0.02321791648864746,\n",
       "  0.023424625396728516,\n",
       "  0.022989988327026367,\n",
       "  0.022957563400268555,\n",
       "  0.023400068283081055,\n",
       "  0.023659467697143555,\n",
       "  0.02379012107849121,\n",
       "  0.023547649383544922,\n",
       "  0.02373504638671875,\n",
       "  0.023845195770263672,\n",
       "  0.0230715274810791,\n",
       "  0.022527456283569336,\n",
       "  0.022658109664916992,\n",
       "  0.023657560348510742,\n",
       "  0.022643327713012695,\n",
       "  0.023045778274536133,\n",
       "  0.02321171760559082,\n",
       "  0.023291587829589844,\n",
       "  0.022841930389404297,\n",
       "  0.02321600914001465,\n",
       "  0.023213863372802734,\n",
       "  0.02335953712463379,\n",
       "  0.023038625717163086,\n",
       "  0.022670507431030273,\n",
       "  0.023227691650390625,\n",
       "  0.023264169692993164,\n",
       "  0.023616313934326172,\n",
       "  0.023816823959350586,\n",
       "  0.022714853286743164,\n",
       "  0.02273416519165039,\n",
       "  0.022533655166625977,\n",
       "  0.022999286651611328,\n",
       "  0.02313089370727539,\n",
       "  0.023537397384643555,\n",
       "  0.023272275924682617,\n",
       "  0.02317023277282715,\n",
       "  0.023664236068725586,\n",
       "  0.022730350494384766,\n",
       "  0.02399897575378418,\n",
       "  0.02280259132385254,\n",
       "  0.024295806884765625,\n",
       "  0.023052453994750977,\n",
       "  0.023506641387939453,\n",
       "  0.023493051528930664,\n",
       "  0.02286076545715332,\n",
       "  0.023128986358642578,\n",
       "  0.022578716278076172,\n",
       "  0.023035764694213867,\n",
       "  0.022717714309692383,\n",
       "  0.022962093353271484,\n",
       "  0.02311563491821289,\n",
       "  0.02337789535522461,\n",
       "  0.0236208438873291,\n",
       "  0.023404598236083984,\n",
       "  0.023080110549926758,\n",
       "  0.023487329483032227,\n",
       "  0.022138357162475586,\n",
       "  0.0238800048828125,\n",
       "  0.023172855377197266,\n",
       "  0.022600412368774414,\n",
       "  0.023698091506958008,\n",
       "  0.0226593017578125,\n",
       "  0.0231170654296875,\n",
       "  0.023506641387939453,\n",
       "  0.023071765899658203,\n",
       "  0.02313995361328125,\n",
       "  0.022825241088867188,\n",
       "  0.02369832992553711,\n",
       "  0.023446321487426758,\n",
       "  0.022121906280517578,\n",
       "  0.022739887237548828,\n",
       "  0.022910118103027344,\n",
       "  0.023359060287475586,\n",
       "  0.022745370864868164,\n",
       "  0.02252960205078125,\n",
       "  0.022853612899780273,\n",
       "  0.024408817291259766,\n",
       "  0.023209571838378906,\n",
       "  0.023334503173828125,\n",
       "  0.023123502731323242,\n",
       "  0.023253679275512695,\n",
       "  0.024141311645507812,\n",
       "  0.023409366607666016,\n",
       "  0.023401737213134766,\n",
       "  0.022796630859375,\n",
       "  0.02332329750061035,\n",
       "  0.022515058517456055,\n",
       "  0.02317953109741211,\n",
       "  0.02292919158935547,\n",
       "  0.0230863094329834,\n",
       "  0.02300429344177246,\n",
       "  0.022887706756591797,\n",
       "  0.023011445999145508,\n",
       "  0.02285003662109375,\n",
       "  0.022472381591796875,\n",
       "  0.023641347885131836,\n",
       "  0.02512955665588379,\n",
       "  0.02287912368774414,\n",
       "  0.023578643798828125,\n",
       "  0.023411989212036133,\n",
       "  0.022745370864868164,\n",
       "  0.02286505699157715,\n",
       "  0.023339271545410156,\n",
       "  0.023560523986816406,\n",
       "  0.022875547409057617,\n",
       "  0.023137807846069336,\n",
       "  0.024282455444335938,\n",
       "  0.02307748794555664,\n",
       "  0.022621631622314453,\n",
       "  0.023389339447021484,\n",
       "  0.023119688034057617,\n",
       "  0.022421598434448242,\n",
       "  0.0241391658782959,\n",
       "  0.02320241928100586,\n",
       "  0.022884845733642578,\n",
       "  0.022977828979492188,\n",
       "  0.02288079261779785,\n",
       "  0.022984981536865234,\n",
       "  0.023688077926635742,\n",
       "  0.023749113082885742,\n",
       "  0.02340221405029297,\n",
       "  0.022975444793701172,\n",
       "  0.022584199905395508,\n",
       "  0.023993492126464844,\n",
       "  0.023589372634887695,\n",
       "  0.023029804229736328,\n",
       "  0.023497581481933594,\n",
       "  0.02289748191833496,\n",
       "  0.022982358932495117,\n",
       "  0.022818326950073242,\n",
       "  0.02327585220336914,\n",
       "  0.023285627365112305,\n",
       "  0.023729562759399414,\n",
       "  0.022639989852905273,\n",
       "  0.022966623306274414,\n",
       "  0.023831844329833984,\n",
       "  0.022875547409057617,\n",
       "  0.0229339599609375,\n",
       "  0.024266481399536133,\n",
       "  0.02295684814453125,\n",
       "  0.023138761520385742,\n",
       "  0.02359771728515625,\n",
       "  0.023410320281982422,\n",
       "  0.023030519485473633,\n",
       "  0.02303028106689453,\n",
       "  0.022919893264770508,\n",
       "  0.022726774215698242,\n",
       "  0.0232391357421875,\n",
       "  0.023647546768188477,\n",
       "  0.02314162254333496,\n",
       "  0.023579120635986328,\n",
       "  0.022667884826660156,\n",
       "  0.024139404296875,\n",
       "  0.02281022071838379,\n",
       "  0.022897005081176758,\n",
       "  0.023622512817382812,\n",
       "  0.02396559715270996,\n",
       "  0.023160457611083984,\n",
       "  0.02312016487121582,\n",
       "  0.023331165313720703,\n",
       "  0.02293229103088379,\n",
       "  0.02299332618713379,\n",
       "  0.02275562286376953,\n",
       "  0.023467063903808594,\n",
       "  0.021723508834838867,\n",
       "  0.022617340087890625,\n",
       "  0.022328615188598633,\n",
       "  0.022789478302001953,\n",
       "  0.023164033889770508,\n",
       "  0.02356576919555664,\n",
       "  0.02275824546813965,\n",
       "  0.023259639739990234,\n",
       "  0.022397756576538086,\n",
       "  0.02544260025024414,\n",
       "  0.02051568031311035,\n",
       "  0.022409915924072266,\n",
       "  0.02342677116394043,\n",
       "  0.023738622665405273,\n",
       "  0.022797822952270508,\n",
       "  0.07003521919250488,\n",
       "  0.002538919448852539,\n",
       "  0.001466512680053711,\n",
       "  0.01784229278564453,\n",
       "  0.02402353286743164,\n",
       "  0.02317023277282715,\n",
       "  0.024071693420410156,\n",
       "  0.022911548614501953,\n",
       "  0.13380837440490723,\n",
       "  0.0016529560089111328,\n",
       "  0.002141714096069336],\n",
       " 'full_text': '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " 'usage': CompletionUsage(completion_tokens=200, prompt_tokens=40, total_tokens=240, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_test(\n",
    "    prompt=\"Write a poem about AI\", \n",
    "    max_tokens=200, \n",
    "    temperature=0.8, \n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d05167-ed3d-41fd-977e-9f8e0a013a1f",
   "metadata": {},
   "source": [
    "### 2. Run various token length experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d270f2a-65c8-449a-ac09-2126c30c1090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Comprehensive Token Length Study\n",
      "üìã Model: llama3.3_tpuv6e\n",
      "üñ•Ô∏è Device: v6e-8 TPU (256 GB HBM)\n",
      "üß™ Experiments: 4\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Small(250)\n",
      "üìä Input tokens: 250\n",
      "üìä Output tokens: 250\n",
      "üìä Total tokens: 500\n",
      "üöÄ Concurrency: 250\n",
      "üìù Requests: 2000\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 2000 requests...\n",
      "üë• Max concurrency: 250\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [08:01<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 498.88 seconds\n",
      "üìà Success rate: 2000/2000 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 2,000\n",
      "   ‚ùå Failed: 0\n",
      "   üìä Success Rate: 100.0%\n",
      "   ‚è±Ô∏è Duration: 590.89s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 2608.6ms\n",
      "   üöÄ TTFT p95: 5060.1ms\n",
      "   üöÄ TTFT p99: 6264.6ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 165.0ms\n",
      "   ‚ö° TPOT p95: 198.0ms\n",
      "   ‚ö° TPOT p99: 212.1ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 51.85s\n",
      "   üìà E2E p95: 61.90s\n",
      "   üìà E2E p99: 65.56s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 3.38\n",
      "   üì§ Output tokens/sec: 1015.02\n",
      "   üìä Overall tokens/sec: 1858\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 498,145.699999994\n",
      "   üì§ Total output tokens: 599,766\n",
      "   üìä Avg input/request: 249.1\n",
      "   üìä Avg output/request: 299.9\n",
      "\n",
      "‚úÖ Experiment Complete: Token Length - Small(250)\n",
      "üìà TTFT-P95: 5.06s\n",
      "üìà Token Output Throughput: 1015.02 tok/s\n",
      "üìà Overall Token Throughput: 1858 tok/s\n",
      "üë• Concurrent Users: 250\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Medium(350 - 580)\n",
      "üìä Input tokens: 465\n",
      "üìä Output tokens: 465\n",
      "üìä Total tokens: 930\n",
      "üöÄ Concurrency: 150\n",
      "üìù Requests: 1500\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 1500 requests...\n",
      "üë• Max concurrency: 150\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 926/1000 [20:20<00:11,  6.51it/s]  "
     ]
    }
   ],
   "source": [
    "# run_token_length_experiment(\"small\")    # 250 tokens, 250 concurrency\n",
    "# run_token_length_experiment(\"medium\")   # 465 tokens, 150 concurrency  \n",
    "# run_token_length_experiment(\"large\")    # 1500 tokens, 100 concurrency\n",
    "# run_token_length_experiment(\"xlarge\")   # 3250 tokens, 30 concurrency\n",
    "\n",
    "run_comprehensive_token_length_study(experiments=[\"small\", \"medium\", \"large\", \"xlarge\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae8cb7-10ed-4d69-a27d-33203321d2d6",
   "metadata": {},
   "source": [
    "### 3. Comprehensive Study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3b1016-6464-49e9-ba55-1813df9760d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [06:59<00:00,  4.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 451.14 seconds\n",
      "üìà Success rate: 2000/2000 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 2,000\n",
      "   ‚ùå Failed: 0\n",
      "   üìä Success Rate: 100.0%\n",
      "   ‚è±Ô∏è Duration: 589.08s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 3178.8ms\n",
      "   üöÄ TTFT p95: 5205.2ms\n",
      "   üöÄ TTFT p99: 6367.6ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 166.5ms\n",
      "   ‚ö° TPOT p95: 192.1ms\n",
      "   ‚ö° TPOT p99: 201.4ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 53.20s\n",
      "   üìà E2E p95: 61.11s\n",
      "   üìà E2E p99: 64.14s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 3.40\n",
      "   üì§ Output tokens/sec: 1018.26\n",
      "   üìä Overall tokens/sec: 1864\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 498,178.1999999926\n",
      "   üì§ Total output tokens: 599,833\n",
      "   üìä Avg input/request: 249.1\n",
      "   üìä Avg output/request: 299.9\n",
      "\n",
      "‚úÖ Experiment Complete: Token Length - Small(250)\n",
      "üìà TTFT-P95: 5.21s\n",
      "üìà Token Output Throughput: 1018.26 tok/s\n",
      "üìà Overall Token Throughput: 1864 tok/s\n",
      "üë• Concurrent Users: 250\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Medium(350 - 580)\n",
      "üìä Input tokens: 465\n",
      "üìä Output tokens: 465\n",
      "üìä Total tokens: 930\n",
      "üöÄ Concurrency: 150\n",
      "üìù Requests: 1500\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 1500 requests...\n",
      "üë• Max concurrency: 150\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [08:52<00:00,  2.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 535.13 seconds\n",
      "üìà Success rate: 1500/1500 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 1,500\n",
      "   ‚ùå Failed: 0\n",
      "   üìä Success Rate: 100.0%\n",
      "   ‚è±Ô∏è Duration: 629.87s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 1871.9ms\n",
      "   üöÄ TTFT p95: 3693.7ms\n",
      "   üöÄ TTFT p99: 5352.4ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 96.4ms\n",
      "   ‚ö° TPOT p95: 105.9ms\n",
      "   ‚ö° TPOT p99: 109.9ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 51.69s\n",
      "   üìà E2E p95: 56.51s\n",
      "   üìà E2E p99: 58.59s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 2.38\n",
      "   üì§ Output tokens/sec: 1225.90\n",
      "   üìä Overall tokens/sec: 2317\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 687,544.0000000022\n",
      "   üì§ Total output tokens: 772,158\n",
      "   üìä Avg input/request: 458.4\n",
      "   üìä Avg output/request: 514.8\n",
      "\n",
      "‚úÖ Experiment Complete: Token Length - Medium(350 - 580)\n",
      "üìà TTFT-P95: 3.69s\n",
      "üìà Token Output Throughput: 1225.90 tok/s\n",
      "üìà Overall Token Throughput: 2317 tok/s\n",
      "üë• Concurrent Users: 150\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Large(1200 - 1800)\n",
      "üìä Input tokens: 1500\n",
      "üìä Output tokens: 1500\n",
      "üìä Total tokens: 3000\n",
      "üöÄ Concurrency: 100\n",
      "üìù Requests: 1000\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 1000 requests...\n",
      "üë• Max concurrency: 100\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [14:28<00:00,  1.74s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 869.02 seconds\n",
      "üìà Success rate: 500/500 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 500\n",
      "   ‚ùå Failed: 0\n",
      "   üìä Success Rate: 100.0%\n",
      "   ‚è±Ô∏è Duration: 872.16s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 764.3ms\n",
      "   üöÄ TTFT p95: 1857.1ms\n",
      "   üöÄ TTFT p99: 7016.5ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 47.6ms\n",
      "   ‚ö° TPOT p95: 48.0ms\n",
      "   ‚ö° TPOT p99: 55.2ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 50.72s\n",
      "   üìà E2E p95: 55.97s\n",
      "   üìà E2E p99: 58.51s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 0.57\n",
      "   üì§ Output tokens/sec: 601.92\n",
      "   üìä Overall tokens/sec: 2422\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 1,587,067.300000005\n",
      "   üì§ Total output tokens: 524,970\n",
      "   üìä Avg input/request: 3174.1\n",
      "   üìä Avg output/request: 1049.9\n",
      "\n",
      "‚úÖ Experiment Complete: Token Length - Xlarge(2.5k - 4k)\n",
      "üìà TTFT-P95: 1.86s\n",
      "üìà Token Output Throughput: 601.92 tok/s\n",
      "üìà Overall Token Throughput: 2422 tok/s\n",
      "üë• Concurrent Users: 30\n",
      "\n",
      "üìä Comprehensive Reports Generated:\n",
      "   üéØ Key Metrics CSV (Your Format): token_length_benchmarks/key_metrics_comparison_llama3_3_tpuv6e_20250715_181747.csv\n",
      "   üìä Comprehensive Analysis CSV: token_length_benchmarks/comprehensive_token_length_analysis_llama3_3_tpuv6e_20250715_181747.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'small': {'successful_requests': 2000,\n",
       "  'failed_requests': 0,\n",
       "  'total_requests': 2000,\n",
       "  'benchmark_duration': 589.0778615474701,\n",
       "  'request_throughput': 3.3951369259508875,\n",
       "  'input_token_throughput': 845.6916012618607,\n",
       "  'output_token_throughput': 1018.2575838519493,\n",
       "  'overall_token_throughput': 1863.94918511381,\n",
       "  'total_input_tokens': 498178.1999999926,\n",
       "  'total_output_tokens': 599833,\n",
       "  'ttft_percentiles': {50: np.float64(3.178810954093933),\n",
       "   90: np.float64(4.638809752464295),\n",
       "   95: np.float64(5.205248308181763),\n",
       "   99: np.float64(6.3676453614234925)},\n",
       "  'tpot_percentiles': {50: np.float64(166.5489518875869),\n",
       "   90: np.float64(186.44861210150063),\n",
       "   95: np.float64(192.11059362194612),\n",
       "   99: np.float64(201.43812152454285)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(53.20250082015991),\n",
       "   90: np.float64(59.260724258422854),\n",
       "   95: np.float64(61.10888644456863),\n",
       "   99: np.float64(64.13655402898789)},\n",
       "  'experiment_name': 'small',\n",
       "  'experiment_config': {'name': 'Token Length - Small(250)',\n",
       "   'input_tokens': 250,\n",
       "   'output_tokens': 250,\n",
       "   'total_tokens': 500,\n",
       "   'concurrency': 250,\n",
       "   'num_requests': 2000,\n",
       "   'description': 'Small token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T17:33:40.142583'},\n",
       " 'medium': {'successful_requests': 1500,\n",
       "  'failed_requests': 0,\n",
       "  'total_requests': 1500,\n",
       "  'benchmark_duration': 629.8719720840454,\n",
       "  'request_throughput': 2.3814363338584164,\n",
       "  'input_token_throughput': 1091.5615084842377,\n",
       "  'output_token_throughput': 1225.8967444529649,\n",
       "  'overall_token_throughput': 2317.458252937203,\n",
       "  'total_input_tokens': 687544.0000000022,\n",
       "  'total_output_tokens': 772158,\n",
       "  'ttft_percentiles': {50: np.float64(1.8718584775924683),\n",
       "   90: np.float64(2.874673795700074),\n",
       "   95: np.float64(3.6936592578887932),\n",
       "   99: np.float64(5.352431359291076)},\n",
       "  'tpot_percentiles': {50: np.float64(96.44748937295105),\n",
       "   90: np.float64(103.77714142261313),\n",
       "   95: np.float64(105.88593691703412),\n",
       "   99: np.float64(109.87056585137482)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(51.69079852104187),\n",
       "   90: np.float64(55.49904773235321),\n",
       "   95: np.float64(56.508831632137294),\n",
       "   99: np.float64(58.58802018642425)},\n",
       "  'experiment_name': 'medium',\n",
       "  'experiment_config': {'name': 'Token Length - Medium(350 - 580)',\n",
       "   'input_tokens': 465,\n",
       "   'output_tokens': 465,\n",
       "   'total_tokens': 930,\n",
       "   'concurrency': 150,\n",
       "   'num_requests': 1500,\n",
       "   'description': 'Medium token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T17:42:35.648058'},\n",
       " 'large': {'successful_requests': 999,\n",
       "  'failed_requests': 1,\n",
       "  'total_requests': 1000,\n",
       "  'benchmark_duration': 1264.4612381458282,\n",
       "  'request_throughput': 0.7900598056014011,\n",
       "  'input_token_throughput': 1160.1855839813495,\n",
       "  'output_token_throughput': 1224.4289925963265,\n",
       "  'overall_token_throughput': 2384.614576577676,\n",
       "  'total_input_tokens': 1467009.699999998,\n",
       "  'total_output_tokens': 1548243,\n",
       "  'ttft_percentiles': {50: np.float64(1.350999116897583),\n",
       "   90: np.float64(2.775886344909669),\n",
       "   95: np.float64(6.248661971092206),\n",
       "   99: np.float64(12.731653532981872)},\n",
       "  'tpot_percentiles': {50: np.float64(78.84154640118896),\n",
       "   90: np.float64(79.53633760313129),\n",
       "   95: np.float64(79.63380524233281),\n",
       "   99: np.float64(79.77085293609147)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(123.7315046787262),\n",
       "   90: np.float64(124.93169393539429),\n",
       "   95: np.float64(126.0218659877777),\n",
       "   99: np.float64(133.1352851009369)},\n",
       "  'experiment_name': 'large',\n",
       "  'experiment_config': {'name': 'Token Length - Large(1200 - 1800)',\n",
       "   'input_tokens': 1500,\n",
       "   'output_tokens': 1500,\n",
       "   'total_tokens': 3000,\n",
       "   'concurrency': 100,\n",
       "   'num_requests': 1000,\n",
       "   'description': 'Large token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T18:03:17.369639'},\n",
       " 'xlarge': {'successful_requests': 500,\n",
       "  'failed_requests': 0,\n",
       "  'total_requests': 500,\n",
       "  'benchmark_duration': 872.1621716022491,\n",
       "  'request_throughput': 0.5732878772779723,\n",
       "  'input_token_throughput': 1819.6928870285712,\n",
       "  'output_token_throughput': 601.9178738692342,\n",
       "  'overall_token_throughput': 2421.6107608978054,\n",
       "  'total_input_tokens': 1587067.300000005,\n",
       "  'total_output_tokens': 524970,\n",
       "  'ttft_percentiles': {50: np.float64(0.7643232345581055),\n",
       "   90: np.float64(1.6921887397766133),\n",
       "   95: np.float64(1.857050287723541),\n",
       "   99: np.float64(7.01654859781265)},\n",
       "  'tpot_percentiles': {50: np.float64(47.60750926484143),\n",
       "   90: np.float64(47.82994773071987),\n",
       "   95: np.float64(47.96382778854527),\n",
       "   99: np.float64(55.16062678099815)},\n",
       "  'e2e_latency_percentiles': {50: np.float64(50.71910536289215),\n",
       "   90: np.float64(51.43657557964325),\n",
       "   95: np.float64(55.9717831134796),\n",
       "   99: np.float64(58.51055370807647)},\n",
       "  'experiment_name': 'xlarge',\n",
       "  'experiment_config': {'name': 'Token Length - Xlarge(2.5k - 4k)',\n",
       "   'input_tokens': 3250,\n",
       "   'output_tokens': 1000,\n",
       "   'total_tokens': 4250,\n",
       "   'concurrency': 30,\n",
       "   'num_requests': 500,\n",
       "   'description': 'Extra large token length test'},\n",
       "  'model_name': 'llama3.3_tpuv6e',\n",
       "  'device_type': 'v6e-8 TPU (256 GB HBM)',\n",
       "  'timestamp': '2025-07-15T18:17:47.088353'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_comprehensive_token_length_study(\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    device_type=\"v6e-8 TPU (256 GB HBM)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8571f-0b00-47b1-b019-78d40ec47ec5",
   "metadata": {},
   "source": [
    "### 4. Custom Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a508977f-4080-4d44-acb4-925e331a5486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Custom Benchmark\n",
      "üìä Input tokens: 1000\n",
      "üìä Output tokens: 500\n",
      "üìù Requests: 50\n",
      "üöÄ Concurrency: 5\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 50 requests...\n",
      "üë• Max concurrency: 5\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:19<00:00,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 199.24 seconds\n",
      "üìà Success rate: 50/50 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 50\n",
      "   ‚ùå Failed: 0\n",
      "   üìä Success Rate: 100.0%\n",
      "   ‚è±Ô∏è Duration: 199.58s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 251.7ms\n",
      "   üöÄ TTFT p95: 421.5ms\n",
      "   üöÄ TTFT p99: 465.8ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 35.8ms\n",
      "   ‚ö° TPOT p95: 35.9ms\n",
      "   ‚ö° TPOT p99: 36.0ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 19.88s\n",
      "   üìà E2E p95: 20.09s\n",
      "   üìà E2E p99: 20.11s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 0.25\n",
      "   üì§ Output tokens/sec: 137.79\n",
      "   üìä Overall tokens/sec: 384\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 49,051.60000000001\n",
      "   üì§ Total output tokens: 27,500\n",
      "   üìä Avg input/request: 981.0\n",
      "   üìä Avg output/request: 550.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'successful_requests': 50,\n",
       " 'failed_requests': 0,\n",
       " 'total_requests': 50,\n",
       " 'benchmark_duration': 199.5771927833557,\n",
       " 'request_throughput': 0.2505296286749349,\n",
       " 'input_token_throughput': 245.77758267822878,\n",
       " 'output_token_throughput': 137.79129577121418,\n",
       " 'overall_token_throughput': 383.56887844944293,\n",
       " 'total_input_tokens': 49051.60000000001,\n",
       " 'total_output_tokens': 27500,\n",
       " 'ttft_percentiles': {50: np.float64(0.25173211097717285),\n",
       "  90: np.float64(0.370452094078064),\n",
       "  95: np.float64(0.42148737907409656),\n",
       "  99: np.float64(0.46575789213180535)},\n",
       " 'tpot_percentiles': {50: np.float64(35.75101576216234),\n",
       "  90: np.float64(35.84772202920827),\n",
       "  95: np.float64(35.8981689468759),\n",
       "  99: np.float64(35.96962899675352)},\n",
       " 'e2e_latency_percentiles': {50: np.float64(19.87656855583191),\n",
       "  90: np.float64(20.00148813724518),\n",
       "  95: np.float64(20.09250659942627),\n",
       "  99: np.float64(20.106535928249357)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_custom_benchmark(\n",
    "    input_tokens=1000,\n",
    "    output_tokens=500, \n",
    "    num_requests=50,\n",
    "    concurrency=5,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c45ce-2fff-482a-839c-a3e54b88c2c6",
   "metadata": {},
   "source": [
    "### 5. Scaling Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a122d66e-bd9e-4988-b08d-e0c7d70c809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Running Throughput Scaling Test\n",
      "üöÄ Concurrency range: 10 to 100 (step 10)\n",
      "üìù Requests per test: 200\n",
      "\n",
      "üß™ Testing concurrency: 10\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 10\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [03:42<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 222.44 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 493 tok/s\n",
      "   ‚ö° TTFT p95: 234.4ms\n",
      "\n",
      "üß™ Testing concurrency: 20\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 20\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:55<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 115.52 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 948 tok/s\n",
      "   ‚ö° TTFT p95: 326.1ms\n",
      "\n",
      "üß™ Testing concurrency: 30\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 30\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:04<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 64.70 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 1692 tok/s\n",
      "   ‚ö° TTFT p95: 502.2ms\n",
      "\n",
      "üß™ Testing concurrency: 40\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 40\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:48<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 48.73 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 2226 tok/s\n",
      "   ‚ö° TTFT p95: 682.3ms\n",
      "\n",
      "üß™ Testing concurrency: 50\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 50\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 44.82 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 2358 tok/s\n",
      "   ‚ö° TTFT p95: 900.8ms\n",
      "\n",
      "üß™ Testing concurrency: 60\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 60\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:47<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 49.08 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 2214 tok/s\n",
      "   ‚ö° TTFT p95: 1937.2ms\n",
      "\n",
      "üß™ Testing concurrency: 70\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 70\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:38<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 39.42 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 2564 tok/s\n",
      "   ‚ö° TTFT p95: 1144.6ms\n",
      "\n",
      "üß™ Testing concurrency: 80\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 80\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:40<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 41.55 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 2244 tok/s\n",
      "   ‚ö° TTFT p95: 1150.0ms\n",
      "\n",
      "üß™ Testing concurrency: 90\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 90\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:46<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 48.16 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 1941 tok/s\n",
      "   ‚ö° TTFT p95: 1378.8ms\n",
      "\n",
      "üß™ Testing concurrency: 100\n",
      "üîó Base URL: https://8393005462395551744.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/8393005462395551744\n",
      "üöÄ Starting benchmark with 200 requests...\n",
      "üë• Max concurrency: 100\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:38<00:00,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 39.63 seconds\n",
      "üìà Success rate: 200/200 (100.0%)\n",
      "   üìà Throughput: 2536 tok/s\n",
      "   ‚ö° TTFT p95: 1414.8ms\n",
      "\n",
      "üìä Scaling Results Saved: token_length_benchmarks/throughput_scaling_20250715_183300.csv\n",
      "\n",
      "üìà THROUGHPUT SCALING SUMMARY:\n",
      "   üèÜ Peak throughput: 2564 tok/s\n",
      "   üéØ Optimal concurrency: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'concurrency': 10,\n",
       "  'request_throughput': 0.8973291696540795,\n",
       "  'output_token_throughput': 269.1987508962238,\n",
       "  'overall_token_throughput': 492.70550047366214,\n",
       "  'ttft_p95': np.float64(0.2343828439712524),\n",
       "  'tpot_p95': np.float64(36.97146568011281),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 20,\n",
       "  'request_throughput': 1.726308639522166,\n",
       "  'output_token_throughput': 517.8925918566498,\n",
       "  'overall_token_throughput': 947.7244537026344,\n",
       "  'ttft_p95': np.float64(0.3261077880859375),\n",
       "  'tpot_p95': np.float64(37.92974657836965),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 30,\n",
       "  'request_throughput': 3.0815879403177884,\n",
       "  'output_token_throughput': 924.4763820953365,\n",
       "  'overall_token_throughput': 1692.0983972345273,\n",
       "  'ttft_p95': np.float64(0.5022170305252074),\n",
       "  'tpot_p95': np.float64(37.12986687752714),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 40,\n",
       "  'request_throughput': 4.054141327320815,\n",
       "  'output_token_throughput': 1216.2423981962447,\n",
       "  'overall_token_throughput': 2225.942512330803,\n",
       "  'ttft_p95': np.float64(0.6823026537895202),\n",
       "  'tpot_p95': np.float64(31.193702635557756),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 50,\n",
       "  'request_throughput': 4.295416427490725,\n",
       "  'output_token_throughput': 1287.7873220438569,\n",
       "  'overall_token_throughput': 2357.9409276642546,\n",
       "  'ttft_p95': np.float64(0.9008141875267026),\n",
       "  'tpot_p95': np.float64(35.795600757152336),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 60,\n",
       "  'request_throughput': 4.0333449598528315,\n",
       "  'output_token_throughput': 1209.5396532854663,\n",
       "  'overall_token_throughput': 2213.7981814942627,\n",
       "  'ttft_p95': np.float64(1.9372004032135008),\n",
       "  'tpot_p95': np.float64(38.8952347793327),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 70,\n",
       "  'request_throughput': 4.672992240765772,\n",
       "  'output_token_throughput': 1400.2153950230559,\n",
       "  'overall_token_throughput': 2564.376923499949,\n",
       "  'ttft_p95': np.float64(1.1446186900138853),\n",
       "  'tpot_p95': np.float64(44.16661581466828),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 80,\n",
       "  'request_throughput': 4.087246925213982,\n",
       "  'output_token_throughput': 1225.6018629946645,\n",
       "  'overall_token_throughput': 2244.0518337021713,\n",
       "  'ttft_p95': np.float64(1.1500409841537471),\n",
       "  'tpot_p95': np.float64(52.75865047671723),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 90,\n",
       "  'request_throughput': 3.5354424011736243,\n",
       "  'output_token_throughput': 1060.6327203520873,\n",
       "  'overall_token_throughput': 1940.9649491291225,\n",
       "  'ttft_p95': np.float64(1.378824138641357),\n",
       "  'tpot_p95': np.float64(60.16815685508242),\n",
       "  'success_rate': 1.0},\n",
       " {'concurrency': 100,\n",
       "  'request_throughput': 4.622715601401743,\n",
       "  'output_token_throughput': 1385.0580484919901,\n",
       "  'overall_token_throughput': 2535.8230021581353,\n",
       "  'ttft_p95': np.float64(1.4148171305656427),\n",
       "  'tpot_p95': np.float64(65.87223860879757),\n",
       "  'success_rate': 1.0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_throughput_scaling_test(\n",
    "    base_concurrency=10,\n",
    "    max_concurrency=100, \n",
    "    step=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1977ca-6017-41f5-8844-3a355cfd0925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
