{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "ax7zWynUDcjk"
   },
   "outputs": [],
   "source": [
    "# @title Request for quota\n",
    "\n",
    "# @markdown For serving Llama 3.1 8B and Qwen3 32B models, we need 1 and 4 TPU v6es, respectively.\n",
    "\n",
    "# @markdown > | Model | Accelerator Type |\n",
    "# @markdown | ----------- | ----------- |\n",
    "# @markdown | Llama 3.1 8B |1 TPU v6e (ct6e-standard-1t)|\n",
    "# @markdown | Qwen3 32B|4 TPU v6e (ct6e-standard-4t)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "VrQvw5wN3gzl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-aiplatform (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-aiplatform (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-aiplatform (/opt/conda/envs/tensorflow/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mfatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n",
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-c0f22ec4-34a8-45ce-a7b2-61e5fc330d0f\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Google Cloud project\n",
    "\n",
    "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
    "\n",
    "# BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "# REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"europe-west4\" #\"us-south1\" #\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = LOCATION # \"us-south1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "YXFGIp1l-qtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# @title Access the models\n",
    "# @markdown ### Access Llama 3.1 and Qwen3 models on Vertex AI for serving\n",
    "# @markdown The models from the Hugging Face can be used for serving in Vertex AI.\n",
    "# @markdown 1. Open the [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) and [Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B) models from [Hugging Face](https://huggingface.co/).\n",
    "# @markdown 2. Review and accept the agreement.\n",
    "# @markdown 3. After accepting the agreement, models will be available for serving.\n",
    "# @markdown 4. You must provide a Hugging Face User Access Token (with read access) to access the Llama 3.1 model. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"Error: HF_TOKEN not found in .env file or not provided.\")\n",
    "    print(\"Please provide a read HF_TOKEN to Llama 3.1 model from Hugging Face in your .env file.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN loaded successfully.\")\n",
    "    # You can now use HF_TOKEN in your code, e.g., to authenticate with Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "7MgDO17YG7nz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.1-8B-Instruct will run on 1 tpu\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare\n",
    "\n",
    "# @markdown In this section you can choose a desired model and the region for TPU deployment.\n",
    "# @markdown Learn about [TPU v6e machine types](https://cloud.google.com/tpu/docs/v6e#configurations) for Vertex AI prediction.\n",
    "\n",
    "# @markdown Here are 2 example models you can run:\n",
    "\n",
    "MODEL_ID = \"Llama-3.1-8B-Instruct\"  # @param [\"Llama-3.1-8B-Instruct\", \"Qwen3-32B\"] {isTemplate: true}\n",
    "\n",
    "TPU_DEPLOYMENT_REGION = \"europe-west4\"  # @param {type:\"string\"}\n",
    "\n",
    "tpu_type = \"TPU_V6e\"\n",
    "\n",
    "\n",
    "if \"Llama-3\" in MODEL_ID:\n",
    "    model_path_prefix = \"meta-llama/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"meta\"\n",
    "    model_publisher_id = \"llama3\"\n",
    "    machine_type = \"ct6e-standard-1t\"\n",
    "    tpu_count = 1\n",
    "    tpu_topo = \"1x1\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpu\")\n",
    "elif \"Qwen3\" in MODEL_ID:\n",
    "    model_path_prefix = \"Qwen/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"qwen\"\n",
    "    model_publisher_id = \"qwen3\"\n",
    "    machine_type = \"ct6e-standard-4t\"\n",
    "    tpu_count = 4\n",
    "    tpu_topo = \"2x2\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpus\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported MODEL_ID: {MODEL_ID}\")\n",
    "\n",
    "\n",
    "vLLM_TPU_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250529_0917_tpu_experimental_RC00\"\n",
    "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "# common_util.check_quota(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=TPU_DEPLOYMENT_REGION,\n",
    "#     accelerator_type=tpu_type,\n",
    "#     accelerator_count=tpu_count,\n",
    "#     is_for_training=False,\n",
    "# )\n",
    "\n",
    "\n",
    "# Server parameters.\n",
    "tensor_parallel_size = tpu_count\n",
    "\n",
    "# Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "\n",
    "# Maximum number of running sequences in a continuous batch.\n",
    "max_running_seqs = 256  # @param\n",
    "# Maximum context length for a request.\n",
    "max_model_len = 2048  # @param\n",
    "\n",
    "# Endpoint configurations.\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n",
    "\n",
    "run_name = \"llama3\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown Note: The vLLM-TPU container used in this notebook is in experimental status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctdQJhrdaeeT"
   },
   "source": [
    "## Deploy prebuilt Llama 3.1 8B or Qwen3 32B models with vLLM on TPUs\n",
    "This section will download the prebuilt model chosen in the previous section and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Cwf_xWpEFkWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/87995179092/locations/europe-west4/endpoints/6807034706119360512/operations/1967437656879005696\n",
      "Endpoint created. Resource name: projects/87995179092/locations/europe-west4/endpoints/6807034706119360512\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/87995179092/locations/europe-west4/endpoints/6807034706119360512')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/87995179092/locations/europe-west4/models/615869448066170880/operations/7596937191092125696\n",
      "Model created. Resource name: projects/87995179092/locations/europe-west4/models/615869448066170880@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/87995179092/locations/europe-west4/models/615869448066170880@1')\n",
      "Deploying model to Endpoint : projects/87995179092/locations/europe-west4/endpoints/6807034706119360512\n",
      "Deploy Endpoint model backing LRO: projects/87995179092/locations/europe-west4/endpoints/6807034706119360512/operations/5671648350391238656\n",
      "Endpoint model deployed. Resource name: projects/87995179092/locations/europe-west4/endpoints/6807034706119360512\n"
     ]
    }
   ],
   "source": [
    "# @title Deploy\n",
    "def deploy_model_vllm_tpu(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct6e-standard-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    max_model_len: int = 4096,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    model_type: str = None,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with vLLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = int(tpu_topology.split(\"x\")[0])\n",
    "\n",
    "    vllmtpu_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllmtpu_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllmtpu_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "        \"VLLM_USE_V1\": \"1\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vLLM_TPU_DOCKER_URI,\n",
    "        serving_container_args=vllmtpu_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb\",\n",
    "        },\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "models[\"vllmtpu\"], endpoints[\"vllmtpu\"] = deploy_model_vllm_tpu(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=run_name),\n",
    "    model_id=model_id,\n",
    "    publisher=model_publisher,\n",
    "    publisher_model_id=model_publisher_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    machine_type=machine_type,\n",
    "    tpu_topology=tpu_topo,\n",
    "    max_model_len=max_model_len,\n",
    "    enable_chunked_prefill=True,\n",
    "    enable_prefix_cache=True,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "78nk6Cl2pqer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car that can run on the wall?\n",
      "Output:\n",
      " Very unlikely car I know, but it grabs me. Its not the very best car at all. Its Power generation is very weak like I said before dont expect alot, This thing has got a small Battery DC 12 VOLTS 250 AH with\n"
     ]
    }
   ],
   "source": [
    "# @title Raw predict\n",
    "\n",
    "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "# @markdown Example:\n",
    "\n",
    "# @markdown ```\n",
    "# @markdown Human: What is a car?\n",
    "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "# @markdown ```\n",
    "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
    "\n",
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "prompt = \"What is a car that can run on the wall?\"  # @param {type: \"string\"}\n",
    "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"vllmtpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "# @markdown Note Top-k sampling is not currently enabled for vLLM on TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test started at: 2025-06-30 14:32:12\n",
      "Output files will be saved as:\n",
      "- Summary CSV: vllm_performance_tests/vllm_test_20250630_143212.csv\n",
      "- Detailed CSV: vllm_performance_tests/vllm_detailed_20250630_143212.csv\n",
      "- Report MD: vllm_performance_tests/vllm_report_20250630_143212.md\n",
      "Testing endpoint with single request first...\n",
      "✅ Single request test successful!\n",
      "Response time: 6.11s\n",
      "Output length: 2007 chars\n",
      "Generating 50 test prompts...\n",
      "Generated 50 test prompts\n",
      "Sample prompt length: 63 words\n",
      "\n",
      "Starting load test:\n",
      "- Concurrent users: 250\n",
      "- Total requests: 50\n",
      "- Target output tokens: 317\n",
      "Completed 2/50 requests... Success rate: 100.0%\n",
      "Completed 4/50 requests... Success rate: 100.0%\n",
      "Completed 6/50 requests... Success rate: 100.0%\n",
      "Completed 8/50 requests... Success rate: 100.0%\n",
      "Completed 10/50 requests... Success rate: 100.0%\n",
      "Completed 12/50 requests... Success rate: 100.0%\n",
      "Completed 14/50 requests... Success rate: 100.0%\n",
      "Completed 16/50 requests... Success rate: 100.0%\n",
      "Completed 18/50 requests... Success rate: 100.0%\n",
      "Completed 20/50 requests... Success rate: 100.0%\n",
      "Completed 22/50 requests... Success rate: 100.0%\n",
      "Completed 24/50 requests... Success rate: 100.0%\n",
      "Completed 26/50 requests... Success rate: 100.0%\n",
      "Completed 28/50 requests... Success rate: 100.0%\n",
      "Completed 30/50 requests... Success rate: 100.0%\n",
      "Completed 32/50 requests... Success rate: 100.0%\n",
      "Completed 34/50 requests... Success rate: 100.0%\n",
      "Completed 36/50 requests... Success rate: 100.0%\n",
      "Completed 38/50 requests... Success rate: 100.0%\n",
      "Completed 40/50 requests... Success rate: 100.0%\n",
      "Completed 42/50 requests... Success rate: 100.0%\n",
      "Completed 44/50 requests... Success rate: 100.0%\n",
      "Completed 46/50 requests... Success rate: 100.0%\n",
      "Completed 48/50 requests... Success rate: 100.0%\n",
      "Completed 50/50 requests... Success rate: 100.0%\n",
      "\n",
      "============================================================\n",
      "LOAD TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Summary:\n",
      "- Total requests: 50\n",
      "- Successful requests: 50\n",
      "- Failed requests: 0\n",
      "- Success rate: 100.0%\n",
      "- Total test time: 13.9 seconds\n",
      "\n",
      "Latency Metrics:\n",
      "- TTFT (p50): 0.276s\n",
      "- TTFT (p95): 0.277s\n",
      "- Inter-token Latency (p95): 0.050s\n",
      "- End-to-End (p95): 13.8s\n",
      "\n",
      "Throughput Metrics:\n",
      "- Token Output Throughput: 1112.21 tok/sec\n",
      "- Overall Token Throughput: 1407.46 tok/sec\n",
      "- Requests per second: 3.61 req/sec\n",
      "\n",
      "Token Statistics:\n",
      "- Average input tokens: 81.9\n",
      "- Average output tokens: 308.5\n",
      "\n",
      "Saving results...\n",
      "\n",
      "============================================================\n",
      "FILES SAVED\n",
      "============================================================\n",
      "📄 Summary: vllm_performance_tests/vllm_test_20250630_143212.csv\n",
      "📊 Details: vllm_performance_tests/vllm_detailed_20250630_143212.csv\n",
      "📝 Report: vllm_performance_tests/vllm_report_20250630_143212.md\n",
      "\n",
      "📊 Partial results saved. Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration - REDUCED for testing\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 250,      # Start small to test endpoint stability\n",
    "    'total_requests': 50,       # Reduce for initial testing\n",
    "    'input_token_length': 265,  \n",
    "    'output_tokens': 317,       \n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following business scenario and provide recommendations:\n",
    "\n",
    "A technology startup is developing an AI-powered customer service platform. They need to understand market positioning, competitive analysis, implementation strategy, and growth projections. Consider technical requirements, user experience design, scalability concerns, and business model validation.\n",
    "\n",
    "Please provide strategic insights covering market analysis, technical architecture, user acquisition strategies, and financial projections for the next 24 months.\"\"\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],\n",
    "    'inter_token_latencies': [],\n",
    "    'end_to_end_times': [],\n",
    "    'input_tokens': [],\n",
    "    'output_tokens': [],\n",
    "    'request_errors': [],\n",
    "    'timestamps': []\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with enhanced error handling\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request with timeout handling\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "        }]\n",
    "        \n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Add retry logic for 502 errors\n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                response = endpoints[\"vllmtpu\"].predict(\n",
    "                    instances=instances, \n",
    "                    use_dedicated_endpoint=use_dedicated_endpoint\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                if \"502\" in str(e) and attempt < max_retries:\n",
    "                    print(f\"Request {request_id}: 502 error, retrying ({attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(1)  # Brief delay before retry\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e  # Re-raise if not 502 or out of retries\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response safely\n",
    "        prediction = {}\n",
    "        output_text = \"\"\n",
    "        \n",
    "        if hasattr(response, 'predictions') and response.predictions:\n",
    "            prediction = response.predictions[0] if response.predictions else {}\n",
    "            if isinstance(prediction, dict):\n",
    "                output_text = prediction.get('generated_text', '') or prediction.get('content', '') or str(prediction)\n",
    "            else:\n",
    "                output_text = str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens\n",
    "        input_tokens = len(prompt.split()) * 1.3\n",
    "        output_tokens = len(output_text.split()) * 1.3 if output_text else 0\n",
    "        \n",
    "        # Estimate timing metrics\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02) if end_to_end_time > 0 else 0\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens) if output_tokens > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': error_msg,\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        print(f\"Request {request_id} failed: {error_msg[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': error_msg,\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': len(prompt.split()) * 1.3 if prompt else 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt) if prompt else 0,\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Test endpoint first with a single request\n",
    "print(\"Testing endpoint with single request first...\")\n",
    "test_prompt = generate_test_prompt()\n",
    "\n",
    "try:\n",
    "    single_test = make_request(0, test_prompt, TEST_CONFIG)\n",
    "    if single_test['success']:\n",
    "        print(\"✅ Single request test successful!\")\n",
    "        print(f\"Response time: {single_test['end_to_end_time']:.2f}s\")\n",
    "        print(f\"Output length: {single_test['output_length']} chars\")\n",
    "    else:\n",
    "        print(\"❌ Single request test failed!\")\n",
    "        print(f\"Error: {single_test['error']}\")\n",
    "        print(\"\\n🛑 Endpoint appears to have issues. Consider:\")\n",
    "        print(\"1. Check if the endpoint is properly deployed and running\")\n",
    "        print(\"2. Verify the endpoint has sufficient resources\")\n",
    "        print(\"3. Test with smaller requests first\")\n",
    "        print(\"4. Check Google Cloud Console for endpoint logs\")\n",
    "        \n",
    "        # Still proceed but with warning\n",
    "        input(\"\\nPress Enter to continue with load test anyway, or Ctrl+C to abort...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Critical error during single test: {e}\")\n",
    "    print(\"Aborting load test.\")\n",
    "    exit(1)\n",
    "\n",
    "# Generate test prompts\n",
    "print(f\"Generating {TEST_CONFIG['total_requests']} test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % max(1, TEST_CONFIG['total_requests'] // 20) == 0:\n",
    "            success_rate = len([r for r in results if r.get('success', False)]) / len(results) * 100\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests... Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics with safe variable handling\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Initialize all variables to prevent NameError\n",
    "ttft_times = []\n",
    "inter_token_times = []\n",
    "e2e_times = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "ttft_p50 = ttft_p95 = ttft_p99 = 0\n",
    "inter_token_p50 = inter_token_p95 = 0\n",
    "e2e_p50 = e2e_p95 = e2e_p99 = 0\n",
    "token_output_throughput = overall_token_throughput = requests_per_second = 0\n",
    "total_input_tokens = total_output_tokens = 0\n",
    "\n",
    "# Calculate metrics only if we have successful requests\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p) if data else 0\n",
    "    \n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ NO SUCCESSFUL REQUESTS - ENDPOINT ISSUES DETECTED\")\n",
    "    print(f\"\\n🔍 TROUBLESHOOTING RECOMMENDATIONS:\")\n",
    "    print(f\"1. Check endpoint status in Google Cloud Console\")\n",
    "    print(f\"2. Verify endpoint has sufficient resources allocated\")\n",
    "    print(f\"3. Check for quota limits or rate limiting\")\n",
    "    print(f\"4. Review endpoint logs for detailed error messages\")\n",
    "    print(f\"5. Try reducing concurrent users and request size\")\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ERROR ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        # Truncate long error messages\n",
    "        error_key = error_msg[:100] + \"...\" if len(error_msg) > 100 else error_msg\n",
    "        error_types[error_key] += 1\n",
    "    \n",
    "    for error, count in list(error_types.items())[:10]:  # Show top 10 errors\n",
    "        print(f\"- {error}: {count} occurrences\")\n",
    "\n",
    "# Save detailed results\n",
    "print(f\"\\nSaving results...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary with safe variable access\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'ttft_p95_seconds': [ttft_p95],\n",
    "    'inter_token_p95_seconds': [inter_token_p95],\n",
    "    'e2e_p95_seconds': [e2e_p95],\n",
    "    'token_output_throughput': [token_output_throughput],\n",
    "    'overall_token_throughput': [overall_token_throughput],\n",
    "    'requests_per_second': [requests_per_second],\n",
    "    'avg_input_tokens': [statistics.mean(input_tokens) if input_tokens else 0],\n",
    "    'avg_output_tokens': [statistics.mean(output_tokens) if output_tokens else 0],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate markdown report\n",
    "md_content = f\"\"\"# vLLM Performance Test Report - {timestamp}\n",
    "\n",
    "**Test Status:** {'✅ PARTIAL SUCCESS' if successful_requests else '❌ FAILED'}  \n",
    "**Success Rate:** {len(successful_requests)/len(results)*100:.1f}%  \n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Issues Detected\n",
    "\n",
    "⚠️ **Endpoint returned 502 errors** - Backend service unavailable  \n",
    "⚠️ **{len(failed_requests)} out of {len(results)} requests failed**\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Check endpoint health** in Google Cloud Console\n",
    "2. **Scale up resources** if endpoint is under-provisioned\n",
    "3. **Implement retry logic** for production applications\n",
    "4. **Monitor endpoint logs** for detailed error information\n",
    "5. **Start with smaller load** to test stability\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Performance Results (Successful Requests Only)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| TTFT (p95) | {ttft_p95:.3f}s |\n",
    "| Inter-token (p95) | {inter_token_p95:.3f}s |\n",
    "| End-to-End (p95) | {e2e_p95:.1f}s |\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Requests/sec | {requests_per_second:.2f} |\n",
    "\"\"\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Error Summary\n",
    "\n",
    "| Error Type | Count |\n",
    "|------------|-------|\n",
    "\"\"\"\n",
    "\n",
    "error_types = defaultdict(int)\n",
    "for req in failed_requests:\n",
    "    error_msg = req.get('error', 'Unknown error')\n",
    "    error_key = error_msg[:50] + \"...\" if len(error_msg) > 50 else error_msg\n",
    "    error_types[error_key] += 1\n",
    "\n",
    "for error, count in list(error_types.items())[:5]:\n",
    "    md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "# Save markdown\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary: {csv_filename}\")\n",
    "print(f\"📊 Details: {detailed_csv_filename}\")\n",
    "print(f\"📝 Report: {md_filename}\")\n",
    "\n",
    "if len(successful_requests) == 0:\n",
    "    print(f\"\\n🚨 CRITICAL: All requests failed. Check your endpoint!\")\n",
    "else:\n",
    "    print(f\"\\n📊 Partial results saved. Success rate: {len(successful_requests)/len(results)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Report with test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test started at: 2025-06-30 12:56:19\n",
      "Output files will be saved as:\n",
      "- Summary CSV: vllm_performance_tests/vllm_test_20250630_125619.csv\n",
      "- Detailed CSV: vllm_performance_tests/vllm_detailed_20250630_125619.csv\n",
      "- Report MD: vllm_performance_tests/vllm_report_20250630_125619.md\n",
      "Generating test prompts...\n",
      "Generated 10000 test prompts\n",
      "Sample prompt length: 289 words\n",
      "Sample prompt preview: Analyze the following complex scenario and provide a detailed response covering multiple aspects:\n",
      "\n",
      "A multinational technology company is considering implementing a comprehensive artificial intelligenc...\n",
      "\n",
      "Starting load test:\n",
      "- Concurrent users: 25\n",
      "- Total requests: 10000\n",
      "- Target output tokens: 317\n",
      "Completed 100/10000 requests...\n",
      "Completed 200/10000 requests...\n",
      "Completed 300/10000 requests...\n",
      "Completed 400/10000 requests...\n",
      "Completed 500/10000 requests...\n",
      "Completed 600/10000 requests...\n",
      "Completed 700/10000 requests...\n",
      "Completed 800/10000 requests...\n",
      "Completed 900/10000 requests...\n",
      "Completed 1000/10000 requests...\n",
      "Completed 1100/10000 requests...\n",
      "Completed 1200/10000 requests...\n",
      "Completed 1300/10000 requests...\n",
      "Completed 1400/10000 requests...\n",
      "Completed 1500/10000 requests...\n",
      "Completed 1600/10000 requests...\n",
      "Completed 1700/10000 requests...\n",
      "Completed 1800/10000 requests...\n",
      "Completed 1900/10000 requests...\n",
      "Completed 2000/10000 requests...\n",
      "Completed 2100/10000 requests...\n",
      "Completed 2200/10000 requests...\n",
      "Completed 2300/10000 requests...\n",
      "Completed 2400/10000 requests...\n",
      "Completed 2500/10000 requests...\n",
      "Completed 2600/10000 requests...\n",
      "Completed 2700/10000 requests...\n",
      "Completed 2800/10000 requests...\n",
      "Completed 2900/10000 requests...\n",
      "Completed 3000/10000 requests...\n",
      "Completed 3100/10000 requests...\n",
      "Completed 3200/10000 requests...\n",
      "Completed 3300/10000 requests...\n",
      "Completed 3400/10000 requests...\n",
      "Completed 3500/10000 requests...\n",
      "Completed 3600/10000 requests...\n",
      "Completed 3700/10000 requests...\n",
      "Completed 3800/10000 requests...\n",
      "Completed 3900/10000 requests...\n",
      "Completed 4000/10000 requests...\n",
      "Completed 4100/10000 requests...\n",
      "Completed 4200/10000 requests...\n",
      "Completed 4300/10000 requests...\n",
      "Completed 4400/10000 requests...\n",
      "Completed 4500/10000 requests...\n",
      "Completed 4600/10000 requests...\n",
      "Completed 4700/10000 requests...\n",
      "Completed 4800/10000 requests...\n",
      "Completed 4900/10000 requests...\n",
      "Completed 5000/10000 requests...\n",
      "Completed 5100/10000 requests...\n",
      "Completed 5200/10000 requests...\n",
      "Completed 5300/10000 requests...\n",
      "Completed 7100/10000 requests...\n",
      "Completed 7200/10000 requests...\n",
      "Completed 7300/10000 requests...\n",
      "Completed 7400/10000 requests...\n",
      "Completed 7500/10000 requests...\n",
      "Completed 7600/10000 requests...\n",
      "Completed 7700/10000 requests...\n",
      "Completed 7800/10000 requests...\n",
      "Completed 7900/10000 requests...\n",
      "Completed 8000/10000 requests...\n",
      "Completed 8100/10000 requests...\n",
      "Completed 8200/10000 requests...\n",
      "Completed 8300/10000 requests...\n",
      "Completed 8400/10000 requests...\n",
      "Completed 8500/10000 requests...\n",
      "Completed 8600/10000 requests...\n",
      "Completed 8700/10000 requests...\n",
      "Completed 8800/10000 requests...\n",
      "Completed 8900/10000 requests...\n",
      "Completed 9000/10000 requests...\n",
      "Completed 9100/10000 requests...\n",
      "Completed 9200/10000 requests...\n",
      "Completed 9300/10000 requests...\n",
      "Completed 9400/10000 requests...\n",
      "Completed 9500/10000 requests...\n",
      "Completed 9600/10000 requests...\n",
      "Completed 9700/10000 requests...\n",
      "Completed 9800/10000 requests...\n",
      "Completed 9900/10000 requests...\n",
      "Completed 10000/10000 requests...\n",
      "\n",
      "============================================================\n",
      "LOAD TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Summary:\n",
      "- Total requests: 10000\n",
      "- Successful requests: 0\n",
      "- Failed requests: 10000\n",
      "- Success rate: 0.0%\n",
      "- Total test time: 3779.1 seconds\n",
      "\n",
      "Saving detailed results to vllm_performance_tests/vllm_detailed_20250630_125619.csv...\n",
      "Saving summary to vllm_performance_tests/vllm_test_20250630_125619.csv...\n",
      "Generating Markdown report: vllm_performance_tests/vllm_report_20250630_125619.md...\n",
      "\n",
      "============================================================\n",
      "FILES SAVED SUCCESSFULLY\n",
      "============================================================\n",
      "📄 Summary CSV: vllm_performance_tests/vllm_test_20250630_125619.csv\n",
      "📊 Detailed CSV: vllm_performance_tests/vllm_detailed_20250630_125619.csv\n",
      "📝 Markdown Report: vllm_performance_tests/vllm_report_20250630_125619.md\n",
      "\n",
      "Test completed at: 2025-06-30 13:59:18\n",
      "\n",
      "❌ TEST FAILED: No successful requests completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration matching your target metrics\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 25,\n",
    "    'total_requests': 10000,\n",
    "    'input_token_length': 265,  # Target input length\n",
    "    'output_tokens': 317,       # Target output length\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts of approximately 265 tokens each\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following complex scenario and provide a detailed response covering multiple aspects:\n",
    "\n",
    "A multinational technology company is considering implementing a comprehensive artificial intelligence strategy across all departments. The company operates in 15 countries, has 50,000 employees, and generates $20 billion in annual revenue. The CEO wants to understand how AI can transform their business operations, improve customer experience, increase efficiency, and create new revenue streams.\n",
    "\n",
    "Consider the following factors in your analysis:\n",
    "1. Current market trends in AI adoption across different industries\n",
    "2. Potential risks and challenges of large-scale AI implementation\n",
    "3. Required infrastructure and technological investments\n",
    "4. Impact on existing workforce and necessary reskilling programs\n",
    "5. Regulatory compliance considerations in different jurisdictions\n",
    "6. Timeline for phased implementation and expected ROI\n",
    "7. Competitive advantages that could be gained\n",
    "8. Data privacy and security implications\n",
    "9. Integration challenges with legacy systems\n",
    "10. Metrics for measuring success and continuous improvement\n",
    "\n",
    "Please provide a comprehensive strategic recommendation that addresses each of these points with specific examples and actionable insights. Include potential pilot programs, budget considerations, and a roadmap for the next 3-5 years.\"\"\"\n",
    "    \n",
    "    # Adjust length to approximately target tokens\n",
    "    words = base_prompt.split()\n",
    "    target_words = target_tokens * 0.75  # Rough conversion\n",
    "    if len(words) > target_words:\n",
    "        return ' '.join(words[:int(target_words)])\n",
    "    else:\n",
    "        # Extend if needed\n",
    "        extension = \" Additionally, consider the impact on stakeholder relationships, customer trust, brand reputation, and long-term sustainability. Analyze potential partnerships with AI vendors, academic institutions, and research organizations. Evaluate the company's current digital maturity and readiness for AI transformation.\" * 3\n",
    "        return base_prompt + extension\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],           # Time to First Token\n",
    "    'inter_token_latencies': [], # Time between tokens\n",
    "    'end_to_end_times': [],     # Total request time\n",
    "    'input_tokens': [],         # Actual input token counts\n",
    "    'output_tokens': [],        # Actual output token counts\n",
    "    'request_errors': [],       # Failed requests\n",
    "    'timestamps': []            # Request timestamps\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with detailed timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "            \"stream\": config.get('stream', True)\n",
    "        }]\n",
    "        \n",
    "        # Record request start\n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Make prediction\n",
    "        response = endpoints[\"vllmtpu\"].predict(\n",
    "            instances=instances, \n",
    "            use_dedicated_endpoint=use_dedicated_endpoint\n",
    "        )\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response\n",
    "        prediction = response.predictions[0] if response.predictions else {}\n",
    "        output_text = prediction.get('generated_text', '') or str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens (rough approximation)\n",
    "        input_tokens = len(prompt.split()) * 1.3  # Rough token estimate\n",
    "        output_tokens = len(output_text.split()) * 1.3\n",
    "        \n",
    "        # Simulate TTFT and inter-token timing (in real streaming, you'd capture these)\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02)  # Estimate TTFT\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens)\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text  # Truncated for CSV\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': str(e),\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': str(e),\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Generate test prompts\n",
    "print(\"Generating test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "print(f\"Sample prompt preview: {test_prompts[0][:200]}...\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "# Execute test\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    # Submit all requests\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    # Collect results with progress tracking\n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests...\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p)\n",
    "    \n",
    "    # Latency metrics\n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    # Throughput calculations\n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- TTFT (p99): {ttft_p99:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p50): {inter_token_p50:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p50): {e2e_p50:.1f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    print(f\"- End-to-End (p99): {e2e_p99:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "    print(f\"- Total input tokens: {int(total_input_tokens)}\")\n",
    "    print(f\"- Total output tokens: {int(total_output_tokens)}\")\n",
    "\n",
    "# Save detailed results to CSV\n",
    "print(f\"\\nSaving detailed results to {detailed_csv_filename}...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary metrics for CSV\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'target_input_tokens': [TEST_CONFIG['input_token_length']],\n",
    "    'target_output_tokens': [TEST_CONFIG['output_tokens']],\n",
    "    'temperature': [TEST_CONFIG['temperature']],\n",
    "    'max_tokens': [TEST_CONFIG['max_tokens']]\n",
    "}\n",
    "\n",
    "if successful_requests:\n",
    "    summary_data.update({\n",
    "        'ttft_p50_seconds': [ttft_p50],\n",
    "        'ttft_p95_seconds': [ttft_p95],\n",
    "        'ttft_p99_seconds': [ttft_p99],\n",
    "        'inter_token_p50_seconds': [inter_token_p50],\n",
    "        'inter_token_p95_seconds': [inter_token_p95],\n",
    "        'e2e_p50_seconds': [e2e_p50],\n",
    "        'e2e_p95_seconds': [e2e_p95],\n",
    "        'e2e_p99_seconds': [e2e_p99],\n",
    "        'token_output_throughput': [token_output_throughput],\n",
    "        'overall_token_throughput': [overall_token_throughput],\n",
    "        'requests_per_second': [requests_per_second],\n",
    "        'avg_input_tokens': [statistics.mean(input_tokens)],\n",
    "        'avg_output_tokens': [statistics.mean(output_tokens)],\n",
    "        'total_input_tokens': [total_input_tokens],\n",
    "        'total_output_tokens': [total_output_tokens]\n",
    "    })\n",
    "else:\n",
    "    # Fill with zeros if no successful requests\n",
    "    for key in ['ttft_p50_seconds', 'ttft_p95_seconds', 'ttft_p99_seconds', \n",
    "                'inter_token_p50_seconds', 'inter_token_p95_seconds',\n",
    "                'e2e_p50_seconds', 'e2e_p95_seconds', 'e2e_p99_seconds',\n",
    "                'token_output_throughput', 'overall_token_throughput', \n",
    "                'requests_per_second', 'avg_input_tokens', 'avg_output_tokens',\n",
    "                'total_input_tokens', 'total_output_tokens']:\n",
    "        summary_data[key] = [0]\n",
    "\n",
    "# Save summary to CSV\n",
    "print(f\"Saving summary to {csv_filename}...\")\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate Markdown report\n",
    "print(f\"Generating Markdown report: {md_filename}...\")\n",
    "\n",
    "md_content = f\"\"\"# vLLM Performance Test Report\n",
    "\n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Test Duration:** {total_test_time:.1f} seconds  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## Test Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Concurrent Users | {TEST_CONFIG['concurrent_users']} |\n",
    "| Total Requests | {TEST_CONFIG['total_requests']} |\n",
    "| Target Input Tokens | {TEST_CONFIG['input_token_length']} |\n",
    "| Target Output Tokens | {TEST_CONFIG['output_tokens']} |\n",
    "| Temperature | {TEST_CONFIG['temperature']} |\n",
    "| Top P | {TEST_CONFIG['top_p']} |\n",
    "| Max Tokens | {TEST_CONFIG['max_tokens']} |\n",
    "| Stream | {TEST_CONFIG['stream']} |\n",
    "\n",
    "## Test Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests | {len(results)} |\n",
    "| Successful Requests | {len(successful_requests)} |\n",
    "| Failed Requests | {len(failed_requests)} |\n",
    "| Success Rate | {len(successful_requests)/len(results)*100:.1f}% |\n",
    "| Test Duration | {total_test_time:.1f} seconds |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Latency Metrics\n",
    "\n",
    "| Metric | p50 | p95 | p99 |\n",
    "|--------|-----|-----|-----|\n",
    "| Time to First Token (TTFT) | {ttft_p50:.3f}s | {ttft_p95:.3f}s | {ttft_p99:.3f}s |\n",
    "| Inter-token Latency | {inter_token_p50:.3f}s | {inter_token_p95:.3f}s | - |\n",
    "| End-to-End Latency | {e2e_p50:.1f}s | {e2e_p95:.1f}s | {e2e_p99:.1f}s |\n",
    "\n",
    "## Throughput Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Overall Token Throughput | {overall_token_throughput:.2f} tok/sec |\n",
    "| Requests per Second | {requests_per_second:.2f} req/sec |\n",
    "\n",
    "## Token Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Average Input Tokens | {statistics.mean(input_tokens):.1f} |\n",
    "| Average Output Tokens | {statistics.mean(output_tokens):.1f} |\n",
    "| Total Input Tokens | {int(total_input_tokens):,} |\n",
    "| Total Output Tokens | {int(total_output_tokens):,} |\n",
    "\n",
    "## Comparison with Target Metrics\n",
    "\n",
    "| Metric | Target | Actual | Difference |\n",
    "|--------|--------|--------|------------|\n",
    "| TTFT (p95) | 0.9s | {ttft_p95:.3f}s | {((ttft_p95 - 0.9) / 0.9 * 100):+.1f}% |\n",
    "| Inter-token Latency (p95) | 0.17s | {inter_token_p95:.3f}s | {((inter_token_p95 - 0.17) / 0.17 * 100):+.1f}% |\n",
    "| End-to-End (p95) | 44.1s | {e2e_p95:.1f}s | {((e2e_p95 - 44.1) / 44.1 * 100):+.1f}% |\n",
    "| Token Output Throughput | 10.05 tok/sec | {token_output_throughput:.2f} tok/sec | {((token_output_throughput - 10.05) / 10.05 * 100):+.1f}% |\n",
    "| Overall Token Throughput | 1529 tok/sec | {overall_token_throughput:.2f} tok/sec | {((overall_token_throughput - 1529) / 1529 * 100):+.1f}% |\n",
    "| Input Token Length | 265 | {statistics.mean(input_tokens):.1f} | {((statistics.mean(input_tokens) - 265) / 265 * 100):+.1f}% |\n",
    "| Output Tokens | 317 | {statistics.mean(output_tokens):.1f} | {((statistics.mean(output_tokens) - 317) / 317 * 100):+.1f}% |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Error Analysis\n",
    "\n",
    "**Total Failed Requests:** {len(failed_requests)}\n",
    "\n",
    "\"\"\"\n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        error_types[error_msg] += 1\n",
    "    \n",
    "    md_content += \"| Error Type | Count |\\n|------------|-------|\\n\"\n",
    "    for error, count in error_types.items():\n",
    "        md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    # Performance analysis\n",
    "    if ttft_p95 <= 0.9:\n",
    "        md_content += \"✅ **TTFT Performance:** Meeting target (≤ 0.9s)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **TTFT Performance:** Above target (> 0.9s)\\n\\n\"\n",
    "    \n",
    "    if inter_token_p95 <= 0.17:\n",
    "        md_content += \"✅ **Inter-token Latency:** Meeting target (≤ 0.17s)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Inter-token Latency:** Above target (> 0.17s)\\n\\n\"\n",
    "    \n",
    "    if token_output_throughput >= 10.05:\n",
    "        md_content += \"✅ **Token Output Throughput:** Meeting target (≥ 10.05 tok/sec)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Token Output Throughput:** Below target (< 10.05 tok/sec)\\n\\n\"\n",
    "    \n",
    "    if overall_token_throughput >= 1529:\n",
    "        md_content += \"✅ **Overall Token Throughput:** Meeting target (≥ 1529 tok/sec)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Overall Token Throughput:** Below target (< 1529 tok/sec)\\n\\n\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Files Generated\n",
    "\n",
    "- **Summary CSV:** `{csv_filename}`\n",
    "- **Detailed CSV:** `{detailed_csv_filename}`\n",
    "- **This Report:** `{md_filename}`\n",
    "\n",
    "## Test Environment\n",
    "\n",
    "- **vLLM Version:** 0.6.6.post1 (target)\n",
    "- **Max Sequences:** 512 (target)\n",
    "- **KV Cache Dtype:** fp8_e5m2 (target)\n",
    "- **Tensor Parallel Size:** 4 (target)\n",
    "- **Tool Call Parser:** llama3_json (target)\n",
    "\n",
    "---\n",
    "*Report generated automatically by vLLM performance testing script*\n",
    "\"\"\"\n",
    "\n",
    "# Save markdown report\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary CSV: {csv_filename}\")\n",
    "print(f\"📊 Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"📝 Markdown Report: {md_filename}\")\n",
    "print(f\"\\nTest completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Display quick summary\n",
    "if successful_requests:\n",
    "    print(f\"\\n🎯 QUICK PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   TTFT (p95): {ttft_p95:.3f}s (target: 0.9s)\")\n",
    "    print(f\"   Inter-token (p95): {inter_token_p95:.3f}s (target: 0.17s)\")\n",
    "    print(f\"   Throughput: {token_output_throughput:.1f} tok/sec (target: 10.05)\")\n",
    "    print(f\"   Success Rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n❌ TEST FAILED: No successful requests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test started at: 2025-06-30 14:00:31\n",
      "Output files will be saved as:\n",
      "- Summary CSV: vllm_performance_tests/vllm_test_20250630_140031.csv\n",
      "- Detailed CSV: vllm_performance_tests/vllm_detailed_20250630_140031.csv\n",
      "- Report MD: vllm_performance_tests/vllm_report_20250630_140031.md\n",
      "Testing endpoint with single request first...\n",
      "✅ Single request test successful!\n",
      "Response time: 6.05s\n",
      "Output length: 1897 chars\n",
      "Generating 50 test prompts...\n",
      "Generated 50 test prompts\n",
      "Sample prompt length: 63 words\n",
      "\n",
      "Starting load test:\n",
      "- Concurrent users: 5\n",
      "- Total requests: 50\n",
      "- Target output tokens: 317\n",
      "Completed 2/50 requests... Success rate: 100.0%\n",
      "Completed 4/50 requests... Success rate: 100.0%\n",
      "Completed 6/50 requests... Success rate: 100.0%\n",
      "Completed 8/50 requests... Success rate: 100.0%\n",
      "Completed 10/50 requests... Success rate: 100.0%\n",
      "Completed 12/50 requests... Success rate: 100.0%\n",
      "Completed 14/50 requests... Success rate: 100.0%\n",
      "Completed 16/50 requests... Success rate: 100.0%\n",
      "Completed 18/50 requests... Success rate: 100.0%\n",
      "Completed 20/50 requests... Success rate: 100.0%\n",
      "Completed 22/50 requests... Success rate: 100.0%\n",
      "Completed 24/50 requests... Success rate: 100.0%\n",
      "Completed 26/50 requests... Success rate: 100.0%\n",
      "Completed 28/50 requests... Success rate: 100.0%\n",
      "Completed 30/50 requests... Success rate: 100.0%\n",
      "Completed 32/50 requests... Success rate: 100.0%\n",
      "Completed 34/50 requests... Success rate: 100.0%\n",
      "Completed 36/50 requests... Success rate: 100.0%\n",
      "Completed 38/50 requests... Success rate: 100.0%\n",
      "Completed 40/50 requests... Success rate: 100.0%\n",
      "Completed 42/50 requests... Success rate: 100.0%\n",
      "Completed 44/50 requests... Success rate: 100.0%\n",
      "Completed 46/50 requests... Success rate: 100.0%\n",
      "Completed 48/50 requests... Success rate: 100.0%\n",
      "Completed 50/50 requests... Success rate: 100.0%\n",
      "\n",
      "============================================================\n",
      "LOAD TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Summary:\n",
      "- Total requests: 50\n",
      "- Successful requests: 50\n",
      "- Failed requests: 0\n",
      "- Success rate: 100.0%\n",
      "- Total test time: 66.8 seconds\n",
      "\n",
      "Latency Metrics:\n",
      "- TTFT (p50): 0.133s\n",
      "- TTFT (p95): 0.134s\n",
      "- Inter-token Latency (p95): 0.024s\n",
      "- End-to-End (p95): 6.7s\n",
      "\n",
      "Throughput Metrics:\n",
      "- Token Output Throughput: 229.37 tok/sec\n",
      "- Overall Token Throughput: 290.67 tok/sec\n",
      "- Requests per second: 0.75 req/sec\n",
      "\n",
      "Token Statistics:\n",
      "- Average input tokens: 81.9\n",
      "- Average output tokens: 306.5\n",
      "\n",
      "Saving results...\n",
      "\n",
      "============================================================\n",
      "FILES SAVED\n",
      "============================================================\n",
      "📄 Summary: vllm_performance_tests/vllm_test_20250630_140031.csv\n",
      "📊 Details: vllm_performance_tests/vllm_detailed_20250630_140031.csv\n",
      "📝 Report: vllm_performance_tests/vllm_report_20250630_140031.md\n",
      "\n",
      "📊 Partial results saved. Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration - REDUCED for testing\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 5,      # Start small to test endpoint stability\n",
    "    'total_requests': 50,       # Reduce for initial testing\n",
    "    'input_token_length': 265,  \n",
    "    'output_tokens': 317,       \n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following business scenario and provide recommendations:\n",
    "\n",
    "A technology startup is developing an AI-powered customer service platform. They need to understand market positioning, competitive analysis, implementation strategy, and growth projections. Consider technical requirements, user experience design, scalability concerns, and business model validation.\n",
    "\n",
    "Please provide strategic insights covering market analysis, technical architecture, user acquisition strategies, and financial projections for the next 24 months.\"\"\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],\n",
    "    'inter_token_latencies': [],\n",
    "    'end_to_end_times': [],\n",
    "    'input_tokens': [],\n",
    "    'output_tokens': [],\n",
    "    'request_errors': [],\n",
    "    'timestamps': []\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with enhanced error handling\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request with timeout handling\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "        }]\n",
    "        \n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Add retry logic for 502 errors\n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                response = endpoints[\"vllmtpu\"].predict(\n",
    "                    instances=instances, \n",
    "                    use_dedicated_endpoint=use_dedicated_endpoint\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                if \"502\" in str(e) and attempt < max_retries:\n",
    "                    print(f\"Request {request_id}: 502 error, retrying ({attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(1)  # Brief delay before retry\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e  # Re-raise if not 502 or out of retries\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response safely\n",
    "        prediction = {}\n",
    "        output_text = \"\"\n",
    "        \n",
    "        if hasattr(response, 'predictions') and response.predictions:\n",
    "            prediction = response.predictions[0] if response.predictions else {}\n",
    "            if isinstance(prediction, dict):\n",
    "                output_text = prediction.get('generated_text', '') or prediction.get('content', '') or str(prediction)\n",
    "            else:\n",
    "                output_text = str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens\n",
    "        input_tokens = len(prompt.split()) * 1.3\n",
    "        output_tokens = len(output_text.split()) * 1.3 if output_text else 0\n",
    "        \n",
    "        # Estimate timing metrics\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02) if end_to_end_time > 0 else 0\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens) if output_tokens > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': error_msg,\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        print(f\"Request {request_id} failed: {error_msg[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': error_msg,\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': len(prompt.split()) * 1.3 if prompt else 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt) if prompt else 0,\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Test endpoint first with a single request\n",
    "print(\"Testing endpoint with single request first...\")\n",
    "test_prompt = generate_test_prompt()\n",
    "\n",
    "try:\n",
    "    single_test = make_request(0, test_prompt, TEST_CONFIG)\n",
    "    if single_test['success']:\n",
    "        print(\"✅ Single request test successful!\")\n",
    "        print(f\"Response time: {single_test['end_to_end_time']:.2f}s\")\n",
    "        print(f\"Output length: {single_test['output_length']} chars\")\n",
    "    else:\n",
    "        print(\"❌ Single request test failed!\")\n",
    "        print(f\"Error: {single_test['error']}\")\n",
    "        print(\"\\n🛑 Endpoint appears to have issues. Consider:\")\n",
    "        print(\"1. Check if the endpoint is properly deployed and running\")\n",
    "        print(\"2. Verify the endpoint has sufficient resources\")\n",
    "        print(\"3. Test with smaller requests first\")\n",
    "        print(\"4. Check Google Cloud Console for endpoint logs\")\n",
    "        \n",
    "        # Still proceed but with warning\n",
    "        input(\"\\nPress Enter to continue with load test anyway, or Ctrl+C to abort...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Critical error during single test: {e}\")\n",
    "    print(\"Aborting load test.\")\n",
    "    exit(1)\n",
    "\n",
    "# Generate test prompts\n",
    "print(f\"Generating {TEST_CONFIG['total_requests']} test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % max(1, TEST_CONFIG['total_requests'] // 20) == 0:\n",
    "            success_rate = len([r for r in results if r.get('success', False)]) / len(results) * 100\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests... Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics with safe variable handling\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Initialize all variables to prevent NameError\n",
    "ttft_times = []\n",
    "inter_token_times = []\n",
    "e2e_times = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "ttft_p50 = ttft_p95 = ttft_p99 = 0\n",
    "inter_token_p50 = inter_token_p95 = 0\n",
    "e2e_p50 = e2e_p95 = e2e_p99 = 0\n",
    "token_output_throughput = overall_token_throughput = requests_per_second = 0\n",
    "total_input_tokens = total_output_tokens = 0\n",
    "\n",
    "# Calculate metrics only if we have successful requests\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p) if data else 0\n",
    "    \n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ NO SUCCESSFUL REQUESTS - ENDPOINT ISSUES DETECTED\")\n",
    "    print(f\"\\n🔍 TROUBLESHOOTING RECOMMENDATIONS:\")\n",
    "    print(f\"1. Check endpoint status in Google Cloud Console\")\n",
    "    print(f\"2. Verify endpoint has sufficient resources allocated\")\n",
    "    print(f\"3. Check for quota limits or rate limiting\")\n",
    "    print(f\"4. Review endpoint logs for detailed error messages\")\n",
    "    print(f\"5. Try reducing concurrent users and request size\")\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ERROR ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        # Truncate long error messages\n",
    "        error_key = error_msg[:100] + \"...\" if len(error_msg) > 100 else error_msg\n",
    "        error_types[error_key] += 1\n",
    "    \n",
    "    for error, count in list(error_types.items())[:10]:  # Show top 10 errors\n",
    "        print(f\"- {error}: {count} occurrences\")\n",
    "\n",
    "# Save detailed results\n",
    "print(f\"\\nSaving results...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary with safe variable access\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'ttft_p95_seconds': [ttft_p95],\n",
    "    'inter_token_p95_seconds': [inter_token_p95],\n",
    "    'e2e_p95_seconds': [e2e_p95],\n",
    "    'token_output_throughput': [token_output_throughput],\n",
    "    'overall_token_throughput': [overall_token_throughput],\n",
    "    'requests_per_second': [requests_per_second],\n",
    "    'avg_input_tokens': [statistics.mean(input_tokens) if input_tokens else 0],\n",
    "    'avg_output_tokens': [statistics.mean(output_tokens) if output_tokens else 0],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate markdown report\n",
    "md_content = f\"\"\"# vLLM Performance Test Report - {timestamp}\n",
    "\n",
    "**Test Status:** {'✅ PARTIAL SUCCESS' if successful_requests else '❌ FAILED'}  \n",
    "**Success Rate:** {len(successful_requests)/len(results)*100:.1f}%  \n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Issues Detected\n",
    "\n",
    "⚠️ **Endpoint returned 502 errors** - Backend service unavailable  \n",
    "⚠️ **{len(failed_requests)} out of {len(results)} requests failed**\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Check endpoint health** in Google Cloud Console\n",
    "2. **Scale up resources** if endpoint is under-provisioned\n",
    "3. **Implement retry logic** for production applications\n",
    "4. **Monitor endpoint logs** for detailed error information\n",
    "5. **Start with smaller load** to test stability\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Performance Results (Successful Requests Only)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| TTFT (p95) | {ttft_p95:.3f}s |\n",
    "| Inter-token (p95) | {inter_token_p95:.3f}s |\n",
    "| End-to-End (p95) | {e2e_p95:.1f}s |\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Requests/sec | {requests_per_second:.2f} |\n",
    "\"\"\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Error Summary\n",
    "\n",
    "| Error Type | Count |\n",
    "|------------|-------|\n",
    "\"\"\"\n",
    "\n",
    "error_types = defaultdict(int)\n",
    "for req in failed_requests:\n",
    "    error_msg = req.get('error', 'Unknown error')\n",
    "    error_key = error_msg[:50] + \"...\" if len(error_msg) > 50 else error_msg\n",
    "    error_types[error_key] += 1\n",
    "\n",
    "for error, count in list(error_types.items())[:5]:\n",
    "    md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "# Save markdown\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary: {csv_filename}\")\n",
    "print(f\"📊 Details: {detailed_csv_filename}\")\n",
    "print(f\"📝 Report: {md_filename}\")\n",
    "\n",
    "if len(successful_requests) == 0:\n",
    "    print(f\"\\n🚨 CRITICAL: All requests failed. Check your endpoint!\")\n",
    "else:\n",
    "    print(f\"\\n📊 Partial results saved. Success rate: {len(successful_requests)/len(results)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxaKC69ypQds"
   },
   "source": [
    "## Clean up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "HYNmBz8PdcvJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # @title Delete the models and endpoints\n",
    "# # @markdown  Delete the experiment models and endpoints to recycle the resources\n",
    "# # @markdown  and avoid unnecessary continuous charges that may incur.\n",
    "\n",
    "# # Undeploy model and delete endpoint.\n",
    "# for endpoint in endpoints.values():\n",
    "#     endpoint.delete(force=True)\n",
    "\n",
    "# # Delete models.\n",
    "# for model in models.values():\n",
    "#     model.delete()\n",
    "\n",
    "# delete_bucket = False  # @param {type:\"boolean\"}\n",
    "# if delete_bucket:\n",
    "#     ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
