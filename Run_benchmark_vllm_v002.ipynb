{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f621791-a034-4ae7-adff-d317fc74a95c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8e2175-8417-4455-b0f4-331c175862d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 14:09:21.722931: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-11 14:09:22.135678: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-11 14:09:22.438611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752242962.682576    3709 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752242962.751735    3709 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752242963.390131    3709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752242963.390173    3709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752242963.390176    3709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752242963.390179    3709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 14:09:23.464574: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-014876a1-7f5e-46d9-84a1-857283160954\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"europe-west4\" #\"us-south1\" #\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = LOCATION # \"us-south1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c29f83-504a-4ae3-883b-c155bc92a3e3",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed657dc-3152-40b9-a555-b9b00120cf38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Endpoint initialized: <google.cloud.aiplatform.models.Endpoint object at 0x7fc3b4269630> \n",
      "resource name: projects/87995179092/locations/europe-west4/endpoints/6859529789275897856\n",
      "üåê DNS: 6859529789275897856.europe-west4-87995179092.prediction.vertexai.goog\n",
      "üìã Resource: projects/tpu-launchpad-playground/locations/europe-west4/endpoints/6859529789275897856\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive TPU Endpoint Benchmark Suite\n",
    "Based on working TPU endpoint code with full benchmarking capabilities\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, List, Dict, Union, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Google Cloud imports\n",
    "import google.auth\n",
    "import openai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Configuration - Set your actual values here\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-project-id\")\n",
    "REGION = \"europe-west4\"\n",
    "endpoint_name = \"6859529789275897856\"\n",
    "\n",
    "# Initialize endpoint\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "use_dedicated_endpoint = True\n",
    "\n",
    "print('üîß Endpoint initialized:', endpoint)\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoint.gca_resource.dedicated_endpoint_dns\n",
    "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
    "    PROJECT_ID, REGION, endpoint.name\n",
    ")\n",
    "print(f\"üåê DNS: {DEDICATED_ENDPOINT_DNS}\")\n",
    "print(f\"üìã Resource: {ENDPOINT_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827ca80-21f7-4e80-8ac2-8364e1c629fd",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de733ed-07e9-4d3f-93a9-75880becf0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Endpoint initialized: <google.cloud.aiplatform.models.Endpoint object at 0x7fc3b426b850> \n",
      "resource name: projects/87995179092/locations/europe-west4/endpoints/6859529789275897856\n",
      "üåê DNS: 6859529789275897856.europe-west4-87995179092.prediction.vertexai.goog\n",
      "üìã Resource: projects/tpu-launchpad-playground/locations/europe-west4/endpoints/6859529789275897856\n",
      "\n",
      "üöÄ TPU Endpoint Benchmark Suite - Examples\n",
      "\n",
      "This benchmark suite uses your exact working TPU endpoint code pattern with comprehensive performance analysis.\n",
      "\n",
      "# Basic Examples:\n",
      "\n",
      "1. Simple Test (like your original):\n",
      "   run_simple_test(\"Write a poem about AI\", max_tokens=200, temperature=0.8)\n",
      "\n",
      "2. Token Length Experiments:\n",
      "   run_token_length_experiment(\"small\")     # 250 tokens\n",
      "   run_token_length_experiment(\"medium\")    # 465 tokens\n",
      "   run_token_length_experiment(\"large\")     # 1500 tokens\n",
      "   run_token_length_experiment(\"xlarge\")    # 3250 tokens\n",
      "\n",
      "3. Comprehensive Study:\n",
      "   run_comprehensive_token_length_study(model_name=\"llama3.3_tpuv6e\")\n",
      "\n",
      "4. Quick Test (reduced sizes):\n",
      "   quick_token_length_test(model_name=\"llama3.3_tpuv6e\")\n",
      "\n",
      "5. Custom Benchmark:\n",
      "   run_custom_benchmark(input_tokens=1000, output_tokens=500, num_requests=50, concurrency=5)\n",
      "\n",
      "6. Throughput Scaling Test:\n",
      "   run_throughput_scaling_test(base_concurrency=5, max_concurrency=50, step=5)\n",
      "\n",
      "üîß Current Configuration:\n",
      "   üìã Project: tpu-launchpad-playground\n",
      "   üåç Region: europe-west4\n",
      "   üñ•Ô∏è Endpoint: 6859529789275897856\n",
      "   üîó DNS: 6859529789275897856.europe-west4-87995179092.prediction.vertexai.goog\n",
      "\n",
      "üéØ Running Demo Test...\n",
      "üöÄ Running Simple Test\n",
      "   üìù Prompt: Explain the benefits of TPU for machine learning in 3 sentences.\n",
      "   üéØ Max tokens: 100\n",
      "   üå°Ô∏è Temperature: 0.7\n",
      "   üîÑ Stream: True\n",
      "------------------------------------------------------------\n",
      "üì° Streaming response:\n",
      "----------------------------------------\n",
      "‚ö° TTFT: 0.679s\n",
      "Tensor Processing Units (TPUs) are specialized hardware accelerators designed to optimize machine learning workloads, providing significant speedups and efficiency gains compared to traditional CPUs and GPUs. The benefits of TPU include faster training times, improved model accuracy, and reduced energy consumption, making them an attractive option for large-scale machine learning deployments. By leveraging TPUs, developers can accelerate the development and deployment of complex machine learning models, enabling faster time-to-market and improved overall performance for applications such as computer vision, natural\n",
      "\n",
      "üìä Performance Metrics:\n",
      "----------------------------------------\n",
      "‚úÖ Request completed successfully\n",
      "‚è±Ô∏è  E2E Latency: 3.235s\n",
      "‚ö° TTFT: 0.679s\n",
      "üîÑ Average TPOT: 0.026s\n",
      "üìà Min/Max ITL: 0.023s / 0.029s\n",
      "üöÄ Tokens/second: 30.91\n",
      "üî¢ Total tokens: 100\n",
      "üìè Total characters: 622\n",
      "‚úÖ Demo test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive TPU Endpoint Benchmark Suite\n",
    "Based on working TPU endpoint code with full benchmarking capabilities\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, List, Dict, Union, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Google Cloud imports\n",
    "import google.auth\n",
    "import openai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Configuration - Set your actual values here\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-project-id\")\n",
    "REGION = \"europe-west4\"\n",
    "endpoint_name = \"6859529789275897856\"\n",
    "\n",
    "# Initialize endpoint\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "use_dedicated_endpoint = True\n",
    "\n",
    "print('üîß Endpoint initialized:', endpoint)\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoint.gca_resource.dedicated_endpoint_dns\n",
    "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
    "    PROJECT_ID, REGION, endpoint.name\n",
    ")\n",
    "print(f\"üåê DNS: {DEDICATED_ENDPOINT_DNS}\")\n",
    "print(f\"üìã Resource: {ENDPOINT_RESOURCE_NAME}\")\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkRequest:\n",
    "    \"\"\"Request data structure for benchmarking\"\"\"\n",
    "    prompt: str\n",
    "    prompt_len: int\n",
    "    expected_output_len: int\n",
    "    request_id: int\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Result of a single benchmark request\"\"\"\n",
    "    request_id: int\n",
    "    success: bool\n",
    "    prompt_len: int\n",
    "    output_len: int\n",
    "    ttft: float  # Time to first token\n",
    "    tpot: float  # Time per output token\n",
    "    itl: float   # Inter-token latency\n",
    "    e2e_latency: float  # End-to-end latency\n",
    "    error_msg: str = \"\"\n",
    "    timestamp: float = 0.0\n",
    "    full_response: str = \"\"\n",
    "\n",
    "class RandomDataset:\n",
    "    \"\"\"Generate random prompts for benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int, range_ratio: float = 0.0):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "        self.range_ratio = range_ratio\n",
    "        \n",
    "    def _generate_random_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate a random prompt of specified length\"\"\"\n",
    "        words = [\n",
    "            \"analyze\", \"consider\", \"evaluate\", \"examine\", \"investigate\", \"review\", \"assess\", \"study\",\n",
    "            \"business\", \"technology\", \"strategy\", \"development\", \"innovation\", \"implementation\", \"solution\",\n",
    "            \"market\", \"customer\", \"product\", \"service\", \"quality\", \"performance\", \"efficiency\", \"growth\",\n",
    "            \"data\", \"information\", \"process\", \"system\", \"method\", \"approach\", \"framework\", \"model\",\n",
    "            \"challenge\", \"opportunity\", \"risk\", \"benefit\", \"advantage\", \"improvement\", \"optimization\",\n",
    "            \"research\", \"analysis\", \"report\", \"recommendation\", \"conclusion\", \"insight\", \"finding\",\n",
    "            \"artificial\", \"intelligence\", \"machine\", \"learning\", \"neural\", \"network\", \"algorithm\",\n",
    "            \"compute\", \"processor\", \"memory\", \"storage\", \"bandwidth\", \"latency\", \"throughput\"\n",
    "        ]\n",
    "        \n",
    "        prompt_words = []\n",
    "        target_words = int(length * 0.75)  # Rough token-to-word conversion\n",
    "        \n",
    "        while len(prompt_words) < target_words:\n",
    "            prompt_words.append(random.choice(words))\n",
    "        \n",
    "        # Add a question or instruction to make it more realistic\n",
    "        prompt_base = \" \".join(prompt_words)\n",
    "        prompts = [\n",
    "            f\"Explain the following concepts in detail: {prompt_base}\",\n",
    "            f\"Write a comprehensive analysis of: {prompt_base}\",\n",
    "            f\"Describe the relationship between: {prompt_base}\",\n",
    "            f\"Provide insights about: {prompt_base}\",\n",
    "            f\"Create a detailed report on: {prompt_base}\"\n",
    "        ]\n",
    "        \n",
    "        return random.choice(prompts)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate benchmark requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            if self.range_ratio > 0:\n",
    "                input_variance = int(self.input_len * self.range_ratio)\n",
    "                output_variance = int(self.output_len * self.range_ratio)\n",
    "                \n",
    "                actual_input_len = random.randint(\n",
    "                    max(1, self.input_len - input_variance),\n",
    "                    self.input_len + input_variance\n",
    "                )\n",
    "                actual_output_len = random.randint(\n",
    "                    max(1, self.output_len - output_variance),\n",
    "                    self.output_len + output_variance\n",
    "                )\n",
    "            else:\n",
    "                actual_input_len = self.input_len\n",
    "                actual_output_len = self.output_len\n",
    "            \n",
    "            prompt = self._generate_random_prompt(actual_input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=actual_input_len,\n",
    "                expected_output_len=actual_output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class TPUBenchmarkEngine:\n",
    "    \"\"\"Benchmark engine using your exact working TPU endpoint pattern\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_lock = threading.Lock()\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "        \n",
    "        # Initialize authentication using your exact pattern\n",
    "        self.creds, self.project = google.auth.default()\n",
    "        self.auth_req = google.auth.transport.requests.Request()\n",
    "        \n",
    "        # Setup BASE_URL using your exact logic\n",
    "        self.BASE_URL = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "        \n",
    "        if use_dedicated_endpoint:\n",
    "            self.BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "        \n",
    "        print(f\"üîó Base URL: {self.BASE_URL}\")\n",
    "    \n",
    "    def _refresh_auth(self):\n",
    "        \"\"\"Refresh authentication token\"\"\"\n",
    "        self.creds.refresh(self.auth_req)\n",
    "    \n",
    "    def _make_streaming_request(self, request: BenchmarkRequest, \n",
    "                              temperature: float = 0.7, \n",
    "                              max_tokens: int = None,\n",
    "                              stream: bool = True) -> BenchmarkResult:\n",
    "        \"\"\"Make a streaming request using your exact working pattern\"\"\"\n",
    "        \n",
    "        if max_tokens is None:\n",
    "            max_tokens = request.expected_output_len + 50\n",
    "        \n",
    "        # Initialize variables for error handling\n",
    "        request_start = time.time()\n",
    "        client = None\n",
    "        model_response = None\n",
    "        \n",
    "        try:\n",
    "            # Refresh auth token\n",
    "            self._refresh_auth()\n",
    "            \n",
    "            # Create OpenAI client using your exact setup\n",
    "            client = openai.OpenAI(\n",
    "                base_url=self.BASE_URL, \n",
    "                api_key=self.creds.token,\n",
    "                timeout=60.0,  # Add timeout to prevent hanging connections\n",
    "                max_retries=1   # Reduce retries to avoid connection buildup\n",
    "            )\n",
    "            \n",
    "            # Start timing\n",
    "            ttft = None\n",
    "            last_token_time = request_start\n",
    "            inter_token_latencies = []\n",
    "            \n",
    "            # Make request using your exact model request pattern\n",
    "            model_response = client.chat.completions.create(\n",
    "                model=\"\",  # Your exact model parameter\n",
    "                messages=[{\"role\": \"user\", \"content\": request.prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "            )\n",
    "            \n",
    "            # Process streaming response using your exact logic\n",
    "            if stream:\n",
    "                usage = None\n",
    "                contents = []\n",
    "                token_count = 0\n",
    "                \n",
    "                try:\n",
    "                    for chunk in model_response:\n",
    "                        current_time = time.time()\n",
    "                        \n",
    "                        if chunk.usage is not None:\n",
    "                            usage = chunk.usage\n",
    "                            continue\n",
    "                        \n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        if content:  # Only process if there's actual content\n",
    "                            # Timing measurements using your exact pattern\n",
    "                            if ttft is None:\n",
    "                                ttft = current_time - request_start\n",
    "                            else:\n",
    "                                itl = current_time - last_token_time\n",
    "                                inter_token_latencies.append(itl)\n",
    "                            \n",
    "                            contents.append(content)\n",
    "                            token_count += 1\n",
    "                            last_token_time = current_time\n",
    "                \n",
    "                finally:\n",
    "                    # Ensure streaming response is properly closed\n",
    "                    if hasattr(model_response, 'close'):\n",
    "                        try:\n",
    "                            model_response.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                # Final measurements\n",
    "                e2e_latency = time.time() - request_start\n",
    "                full_text = ''.join(contents)\n",
    "                \n",
    "                # Calculate TPOT\n",
    "                if inter_token_latencies:\n",
    "                    avg_tpot = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "                    avg_itl = avg_tpot\n",
    "                else:\n",
    "                    avg_tpot = e2e_latency / max(1, token_count) if token_count > 0 else 0\n",
    "                    avg_itl = avg_tpot\n",
    "                \n",
    "                return BenchmarkResult(\n",
    "                    request_id=request.request_id,\n",
    "                    success=True,\n",
    "                    prompt_len=len(request.prompt.split()) * 1.3,  # Rough token estimate\n",
    "                    output_len=token_count,\n",
    "                    ttft=ttft if ttft else e2e_latency,\n",
    "                    tpot=avg_tpot,\n",
    "                    itl=avg_itl,\n",
    "                    e2e_latency=e2e_latency,\n",
    "                    timestamp=request_start,\n",
    "                    full_response=full_text\n",
    "                )\n",
    "            else:\n",
    "                # Non-streaming response\n",
    "                e2e_latency = time.time() - request_start\n",
    "                response_text = model_response.choices[0].message.content\n",
    "                token_count = len(response_text.split()) * 1.3  # Rough estimate\n",
    "                \n",
    "                # Estimate TTFT and TPOT for non-streaming\n",
    "                estimated_ttft = e2e_latency * 0.2  # 20% for processing\n",
    "                estimated_tpot = (e2e_latency - estimated_ttft) / max(1, token_count)\n",
    "                \n",
    "                return BenchmarkResult(\n",
    "                    request_id=request.request_id,\n",
    "                    success=True,\n",
    "                    prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                    output_len=int(token_count),\n",
    "                    ttft=estimated_ttft,\n",
    "                    tpot=estimated_tpot,\n",
    "                    itl=estimated_tpot,\n",
    "                    e2e_latency=e2e_latency,\n",
    "                    timestamp=request_start,\n",
    "                    full_response=response_text\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - request_start if 'request_start' in locals() else 0\n",
    "            \n",
    "            # Clean up any open connections\n",
    "            try:\n",
    "                if model_response and hasattr(model_response, 'close'):\n",
    "                    model_response.close()\n",
    "                if client and hasattr(client, 'close'):\n",
    "                    client.close()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=False,\n",
    "                prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                output_len=0,\n",
    "                ttft=0.0,\n",
    "                tpot=0.0,\n",
    "                itl=0.0,\n",
    "                e2e_latency=error_time,\n",
    "                error_msg=str(e),\n",
    "                timestamp=time.time(),\n",
    "                full_response=\"\"\n",
    "            )\n",
    "    \n",
    "    def run_benchmark(self, \n",
    "                      requests: List[BenchmarkRequest],\n",
    "                      max_concurrency: int = 100,\n",
    "                      temperature: float = 0.7,\n",
    "                      max_tokens: int = None,\n",
    "                      stream: bool = True,\n",
    "                      request_rate: float = float('inf')) -> List[BenchmarkResult]:\n",
    "        \"\"\"Run benchmark with specified parameters\"\"\"\n",
    "        \n",
    "        print(f\"üöÄ Starting benchmark with {len(requests)} requests...\")\n",
    "        print(f\"üë• Max concurrency: {max_concurrency}\")\n",
    "        print(f\"üå°Ô∏è Temperature: {temperature}\")\n",
    "        print(f\"üîÑ Streaming: {stream}\")\n",
    "        \n",
    "        self.results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Rate limiting setup\n",
    "        if request_rate != float('inf'):\n",
    "            request_interval = 1.0 / request_rate\n",
    "            print(f\"‚è±Ô∏è Request rate limit: {request_rate} req/s\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "            future_to_request = {}\n",
    "            \n",
    "            for i, req in enumerate(requests):\n",
    "                # Rate limiting\n",
    "                if request_rate != float('inf') and i > 0:\n",
    "                    time.sleep(request_interval)\n",
    "                \n",
    "                future = executor.submit(\n",
    "                    self._make_streaming_request, \n",
    "                    req, \n",
    "                    temperature, \n",
    "                    max_tokens, \n",
    "                    stream\n",
    "                )\n",
    "                future_to_request[future] = req\n",
    "            \n",
    "            with tqdm(total=len(requests), desc=\"üìä Processing requests\") as pbar:\n",
    "                for future in as_completed(future_to_request):\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        with self.results_lock:\n",
    "                            self.results.append(result)\n",
    "                    except Exception as e:\n",
    "                        request = future_to_request[future]\n",
    "                        error_result = BenchmarkResult(\n",
    "                            request_id=request.request_id,\n",
    "                            success=False,\n",
    "                            prompt_len=len(request.prompt.split()) * 1.3,\n",
    "                            output_len=0,\n",
    "                            ttft=0.0,\n",
    "                            tpot=0.0,\n",
    "                            itl=0.0,\n",
    "                            e2e_latency=0.0,\n",
    "                            error_msg=f\"Future execution failed: {str(e)}\",\n",
    "                            timestamp=time.time(),\n",
    "                            full_response=\"\"\n",
    "                        )\n",
    "                        with self.results_lock:\n",
    "                            self.results.append(error_result)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        successful_count = len([r for r in self.results if r.success])\n",
    "        print(f\"‚úÖ Benchmark completed in {total_time:.2f} seconds\")\n",
    "        print(f\"üìà Success rate: {successful_count}/{len(requests)} ({successful_count/len(requests)*100:.1f}%)\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "class BenchmarkAnalyzer:\n",
    "    \"\"\"Analyze and report benchmark results\"\"\"\n",
    "    \n",
    "    def __init__(self, results: List[BenchmarkResult]):\n",
    "        self.results = results\n",
    "        self.successful_results = [r for r in results if r.success]\n",
    "        self.failed_results = [r for r in results if not r.success]\n",
    "    \n",
    "    def calculate_percentiles(self, values: List[float], percentiles: List[int]) -> Dict[int, float]:\n",
    "        \"\"\"Calculate percentiles for a list of values\"\"\"\n",
    "        if not values:\n",
    "            return {p: 0.0 for p in percentiles}\n",
    "        return {p: np.percentile(values, p) for p in percentiles}\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate benchmark summary statistics\"\"\"\n",
    "        if not self.successful_results:\n",
    "            return {\n",
    "                \"error\": \"No successful requests\",\n",
    "                \"total_requests\": len(self.results),\n",
    "                \"failed_requests\": len(self.failed_results)\n",
    "            }\n",
    "        \n",
    "        ttfts = [r.ttft for r in self.successful_results]\n",
    "        tpots = [r.tpot * 1000 for r in self.successful_results]  # Convert to ms\n",
    "        e2e_latencies = [r.e2e_latency for r in self.successful_results]\n",
    "        \n",
    "        total_input_tokens = sum(r.prompt_len for r in self.successful_results)\n",
    "        total_output_tokens = sum(r.output_len for r in self.successful_results)\n",
    "        \n",
    "        if self.successful_results:\n",
    "            timestamps = [r.timestamp for r in self.successful_results]\n",
    "            benchmark_duration = max(timestamps) - min(timestamps) + max(e2e_latencies)\n",
    "        else:\n",
    "            benchmark_duration = 1.0\n",
    "        \n",
    "        request_throughput = len(self.successful_results) / benchmark_duration\n",
    "        input_token_throughput = total_input_tokens / benchmark_duration\n",
    "        output_token_throughput = total_output_tokens / benchmark_duration\n",
    "        overall_token_throughput = (total_input_tokens + total_output_tokens) / benchmark_duration\n",
    "        \n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        \n",
    "        summary = {\n",
    "            \"successful_requests\": len(self.successful_results),\n",
    "            \"failed_requests\": len(self.failed_results),\n",
    "            \"total_requests\": len(self.results),\n",
    "            \"benchmark_duration\": benchmark_duration,\n",
    "            \"request_throughput\": request_throughput,\n",
    "            \"input_token_throughput\": input_token_throughput,\n",
    "            \"output_token_throughput\": output_token_throughput,\n",
    "            \"overall_token_throughput\": overall_token_throughput,\n",
    "            \"total_input_tokens\": total_input_tokens,\n",
    "            \"total_output_tokens\": total_output_tokens,\n",
    "            \"ttft_percentiles\": self.calculate_percentiles(ttfts, percentiles),\n",
    "            \"tpot_percentiles\": self.calculate_percentiles(tpots, percentiles),\n",
    "            \"e2e_latency_percentiles\": self.calculate_percentiles(e2e_latencies, percentiles)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_detailed_summary(self):\n",
    "        \"\"\"Print detailed performance summary\"\"\"\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            print(f\"‚ùå Error: {summary['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä DETAILED PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüìã REQUEST STATISTICS:\")\n",
    "        print(f\"   ‚úÖ Successful: {summary['successful_requests']:,}\")\n",
    "        print(f\"   ‚ùå Failed: {summary['failed_requests']:,}\")\n",
    "        print(f\"   üìä Success Rate: {summary['successful_requests']/summary['total_requests']*100:.1f}%\")\n",
    "        print(f\"   ‚è±Ô∏è Duration: {summary['benchmark_duration']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n‚ö° LATENCY METRICS:\")\n",
    "        print(f\"   üöÄ TTFT p50: {summary['ttft_percentiles'][50]*1000:.1f}ms\")\n",
    "        print(f\"   üöÄ TTFT p95: {summary['ttft_percentiles'][95]*1000:.1f}ms\")\n",
    "        print(f\"   üöÄ TTFT p99: {summary['ttft_percentiles'][99]*1000:.1f}ms\")\n",
    "        \n",
    "        print(f\"\\nüîÑ TIME PER OUTPUT TOKEN:\")\n",
    "        print(f\"   ‚ö° TPOT p50: {summary['tpot_percentiles'][50]:.1f}ms\")\n",
    "        print(f\"   ‚ö° TPOT p95: {summary['tpot_percentiles'][95]:.1f}ms\")\n",
    "        print(f\"   ‚ö° TPOT p99: {summary['tpot_percentiles'][99]:.1f}ms\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è END-TO-END LATENCY:\")\n",
    "        print(f\"   üìà E2E p50: {summary['e2e_latency_percentiles'][50]:.2f}s\")\n",
    "        print(f\"   üìà E2E p95: {summary['e2e_latency_percentiles'][95]:.2f}s\")\n",
    "        print(f\"   üìà E2E p99: {summary['e2e_latency_percentiles'][99]:.2f}s\")\n",
    "        \n",
    "        print(f\"\\nüöÄ THROUGHPUT METRICS:\")\n",
    "        print(f\"   üìä Requests/sec: {summary['request_throughput']:.2f}\")\n",
    "        print(f\"   üì§ Output tokens/sec: {summary['output_token_throughput']:.2f}\")\n",
    "        print(f\"   üìä Overall tokens/sec: {summary['overall_token_throughput']:.0f}\")\n",
    "        \n",
    "        print(f\"\\nüìä TOKEN STATISTICS:\")\n",
    "        print(f\"   üì• Total input tokens: {summary['total_input_tokens']:,}\")\n",
    "        print(f\"   üì§ Total output tokens: {summary['total_output_tokens']:,}\")\n",
    "        print(f\"   üìä Avg input/request: {summary['total_input_tokens']/summary['successful_requests']:.1f}\")\n",
    "        print(f\"   üìä Avg output/request: {summary['total_output_tokens']/summary['successful_requests']:.1f}\")\n",
    "\n",
    "# Token Length Test Configurations\n",
    "TOKEN_LENGTH_EXPERIMENTS = {\n",
    "    \"small\": {\n",
    "        \"name\": \"Token Length - Small(250)\",\n",
    "        \"input_tokens\": 250,\n",
    "        \"output_tokens\": 250,\n",
    "        \"total_tokens\": 500,\n",
    "        \"concurrency\": 250,\n",
    "        \"num_requests\": 2000,\n",
    "        \"description\": \"Small token length test\"\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"name\": \"Token Length - Medium(350 - 580)\",\n",
    "        \"input_tokens\": 465,  # Average of 350-580\n",
    "        \"output_tokens\": 465,\n",
    "        \"total_tokens\": 930,\n",
    "        \"concurrency\": 150,\n",
    "        \"num_requests\": 1500,\n",
    "        \"description\": \"Medium token length test\"\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"name\": \"Token Length - Large(1200 - 1800)\",\n",
    "        \"input_tokens\": 1500,  # Average of 1200-1800\n",
    "        \"output_tokens\": 1500,\n",
    "        \"total_tokens\": 3000,\n",
    "        \"concurrency\": 100,\n",
    "        \"num_requests\": 1000,\n",
    "        \"description\": \"Large token length test\"\n",
    "    },\n",
    "    \"xlarge\": {\n",
    "        \"name\": \"Token Length - Xlarge(2.5k - 4k)\",\n",
    "        \"input_tokens\": 3250,  # Average of 2.5k-4k\n",
    "        \"output_tokens\": 1000,  # Reasonable output for very long input\n",
    "        \"total_tokens\": 4250,\n",
    "        \"concurrency\": 30,\n",
    "        \"num_requests\": 500,\n",
    "        \"description\": \"Extra large token length test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_simple_test(prompt: str = \"Write a short poem about artificial intelligence\",\n",
    "                   max_tokens: int = 200,\n",
    "                   temperature: float = 0.8,\n",
    "                   stream: bool = True):\n",
    "    \"\"\"Run a simple test using your exact working pattern\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Running Simple Test\")\n",
    "    print(f\"   üìù Prompt: {prompt}\")\n",
    "    print(f\"   üéØ Max tokens: {max_tokens}\")\n",
    "    print(f\"   üå°Ô∏è Temperature: {temperature}\")\n",
    "    print(f\"   üîÑ Stream: {stream}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Use your exact authentication setup\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Use your exact BASE_URL setup\n",
    "    BASE_URL = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "    \n",
    "    if use_dedicated_endpoint:\n",
    "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "    \n",
    "    # Use your exact client setup\n",
    "    client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
    "    \n",
    "    # Start timing\n",
    "    request_start = time.time()\n",
    "    ttft = None\n",
    "    last_token_time = request_start\n",
    "    inter_token_latencies = []\n",
    "    \n",
    "    # Your exact model request\n",
    "    model_response = client.chat.completions.create(\n",
    "        model=\"\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    print(\"üì° Streaming response:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Your exact streaming logic with timing added\n",
    "    if stream:\n",
    "        usage = None\n",
    "        contents = []\n",
    "        token_count = 0\n",
    "        \n",
    "        for chunk in model_response:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if chunk.usage is not None:\n",
    "                usage = chunk.usage\n",
    "                continue\n",
    "            \n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Only process if there's actual content\n",
    "                # Timing measurements\n",
    "                if ttft is None:\n",
    "                    ttft = current_time - request_start\n",
    "                    print(f\"‚ö° TTFT: {ttft:.3f}s\")\n",
    "                else:\n",
    "                    itl = current_time - last_token_time\n",
    "                    inter_token_latencies.append(itl)\n",
    "                \n",
    "                print(content, end=\"\", flush=True)\n",
    "                contents.append(content)\n",
    "                token_count += 1\n",
    "                last_token_time = current_time\n",
    "        \n",
    "        # Final measurements\n",
    "        e2e_latency = time.time() - request_start\n",
    "        \n",
    "        print(f\"\\n\\nüìä Performance Metrics:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"‚úÖ Request completed successfully\")\n",
    "        print(f\"‚è±Ô∏è  E2E Latency: {e2e_latency:.3f}s\")\n",
    "        print(f\"‚ö° TTFT: {ttft:.3f}s\" if ttft else \"‚ö° TTFT: N/A\")\n",
    "        \n",
    "        if inter_token_latencies:\n",
    "            avg_tpot = sum(inter_token_latencies) / len(inter_token_latencies)\n",
    "            print(f\"üîÑ Average TPOT: {avg_tpot:.3f}s\")\n",
    "            print(f\"üìà Min/Max ITL: {min(inter_token_latencies):.3f}s / {max(inter_token_latencies):.3f}s\")\n",
    "            print(f\"üöÄ Tokens/second: {token_count / e2e_latency:.2f}\")\n",
    "        \n",
    "        print(f\"üî¢ Total tokens: {token_count}\")\n",
    "        print(f\"üìè Total characters: {len(''.join(contents))}\")\n",
    "        \n",
    "        if usage:\n",
    "            print(f\"üíæ Usage: {usage}\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'ttft': ttft,\n",
    "            'e2e_latency': e2e_latency,\n",
    "            'token_count': token_count,\n",
    "            'inter_token_latencies': inter_token_latencies,\n",
    "            'full_text': ''.join(contents),\n",
    "            'usage': usage\n",
    "        }\n",
    "    else:\n",
    "        response_text = model_response.choices[0].message.content\n",
    "        e2e_latency = time.time() - request_start\n",
    "        print(f\"üìÑ Response: {response_text}\")\n",
    "        print(f\"‚è±Ô∏è E2E Latency: {e2e_latency:.3f}s\")\n",
    "        return {'success': True, 'response': response_text, 'e2e_latency': e2e_latency}\n",
    "\n",
    "def run_token_length_experiment(experiment_name: str,\n",
    "                               model_name: str = \"llama3.3_tpuv6e\",\n",
    "                               device_type: str = \"TPU v6e\") -> Dict[str, Any]:\n",
    "    \"\"\"Run a specific token length experiment\"\"\"\n",
    "    \n",
    "    if experiment_name not in TOKEN_LENGTH_EXPERIMENTS:\n",
    "        available = \", \".join(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "        raise ValueError(f\"Unknown experiment '{experiment_name}'. Available: {available}\")\n",
    "    \n",
    "    config = TOKEN_LENGTH_EXPERIMENTS[experiment_name]\n",
    "    \n",
    "    print(f\"\\nüî¨ Running Token Length Experiment: {config['name']}\")\n",
    "    print(f\"üìä Input tokens: {config['input_tokens']}\")\n",
    "    print(f\"üìä Output tokens: {config['output_tokens']}\")\n",
    "    print(f\"üìä Total tokens: {config['total_tokens']}\")\n",
    "    print(f\"üöÄ Concurrency: {config['concurrency']}\")\n",
    "    print(f\"üìù Requests: {config['num_requests']}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = RandomDataset(\n",
    "        input_len=config['input_tokens'],\n",
    "        output_len=config['output_tokens'],\n",
    "        num_requests=config['num_requests']\n",
    "    )\n",
    "    \n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine()\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=config['concurrency'],\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_detailed_summary()\n",
    "    summary = analyzer.generate_summary()\n",
    "    \n",
    "    if \"error\" not in summary:\n",
    "        print(f\"\\n‚úÖ Experiment Complete: {config['name']}\")\n",
    "        print(f\"üìà TTFT-P95: {summary['ttft_percentiles'][95]:.2f}s\")\n",
    "        print(f\"üìà Token Output Throughput: {summary['output_token_throughput']:.2f} tok/s\")\n",
    "        print(f\"üìà Overall Token Throughput: {summary['overall_token_throughput']:.0f} tok/s\")\n",
    "        print(f\"üë• Concurrent Users: {config['concurrency']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Experiment Failed: {summary['error']}\")\n",
    "    \n",
    "    # Add experiment metadata\n",
    "    summary.update({\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"experiment_config\": config,\n",
    "        \"model_name\": model_name,\n",
    "        \"device_type\": device_type,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def run_conservative_token_length_study(model_name: str = \"llama3.3_tpuv6e\",\n",
    "                                       device_type: str = \"v6e-8 TPU (256 GB HBM)\",\n",
    "                                       experiments: List[str] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Run token length study with conservative concurrency to avoid file descriptor issues\"\"\"\n",
    "    \n",
    "    if experiments is None:\n",
    "        experiments = list(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "    \n",
    "    print(f\"üöÄ Starting Conservative Token Length Study (Reduced Concurrency)\")\n",
    "    print(f\"üìã Model: {model_name}\")\n",
    "    print(f\"üñ•Ô∏è Device: {device_type}\")\n",
    "    print(f\"üß™ Experiments: {len(experiments)}\")\n",
    "    \n",
    "    # Conservative concurrency settings to avoid \"too many open files\"\n",
    "    conservative_settings = {\n",
    "        \"small\": {\"concurrency\": 25, \"num_requests\": 500},    # Reduced from 250\n",
    "        \"medium\": {\"concurrency\": 15, \"num_requests\": 300},   # Reduced from 150  \n",
    "        \"large\": {\"concurrency\": 10, \"num_requests\": 200},    # Reduced from 100\n",
    "        \"xlarge\": {\"concurrency\": 5, \"num_requests\": 100}     # Reduced from 30\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_name in experiments:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        # Temporarily modify config for conservative run\n",
    "        original_config = TOKEN_LENGTH_EXPERIMENTS[exp_name].copy()\n",
    "        if exp_name in conservative_settings:\n",
    "            TOKEN_LENGTH_EXPERIMENTS[exp_name].update(conservative_settings[exp_name])\n",
    "            print(f\"üîß Using conservative settings: {conservative_settings[exp_name]}\")\n",
    "        \n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name,\n",
    "                device_type=device_type\n",
    "            )\n",
    "            all_results[exp_name] = result\n",
    "            \n",
    "            # Small delay between experiments to let connections close\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Experiment {exp_name} failed: {e}\")\n",
    "            all_results[exp_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"experiment_name\": exp_name\n",
    "            }\n",
    "        finally:\n",
    "            # Restore original config\n",
    "            TOKEN_LENGTH_EXPERIMENTS[exp_name] = original_config\n",
    "    \n",
    "    # Generate comparison report\n",
    "    generate_token_length_comparison_report(all_results, model_name, device_type)\n",
    "    \n",
    "    return all_results\n",
    "def run_comprehensive_token_length_study(model_name: str = \"llama3.3_tpuv6e\",\n",
    "                                        device_type: str = \"v6e-8 TPU (256 GB HBM)\",\n",
    "                                        experiments: List[str] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Run comprehensive token length study across all experiments\"\"\"\n",
    "    \n",
    "    if experiments is None:\n",
    "        experiments = list(TOKEN_LENGTH_EXPERIMENTS.keys())\n",
    "    \n",
    "    print(f\"üöÄ Starting Comprehensive Token Length Study\")\n",
    "    print(f\"üìã Model: {model_name}\")\n",
    "    print(f\"üñ•Ô∏è Device: {device_type}\")\n",
    "    print(f\"üß™ Experiments: {len(experiments)}\")\n",
    "    print(f\"‚ö†Ô∏è  WARNING: High concurrency may cause 'too many open files' error\")\n",
    "    print(f\"üí° Consider using run_conservative_token_length_study() for stability\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_name in experiments:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name,\n",
    "                device_type=device_type\n",
    "            )\n",
    "            all_results[exp_name] = result\n",
    "            \n",
    "            # Small delay between experiments to let connections close\n",
    "            time.sleep(3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Experiment {exp_name} failed: {e}\")\n",
    "            all_results[exp_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"experiment_name\": exp_name\n",
    "            }\n",
    "    \n",
    "    # Generate comparison report\n",
    "    generate_token_length_comparison_report(all_results, model_name, device_type)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_token_length_comparison_report(all_results: Dict[str, Dict[str, Any]], \n",
    "                                          model_name: str,\n",
    "                                          device_type: str):\n",
    "    \"\"\"Generate comparison report in CSV format matching your screenshot with ALL metrics\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"token_length_benchmarks\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate comprehensive CSV report with ALL metrics\n",
    "    comprehensive_csv_data = []\n",
    "    \n",
    "    for exp_name, result in all_results.items():\n",
    "        if \"error\" not in result:\n",
    "            config = TOKEN_LENGTH_EXPERIMENTS[exp_name]\n",
    "            \n",
    "            # Base row data\n",
    "            base_row = {\n",
    "                \"Token_Length_Category\": config['name'],\n",
    "                \"Device_Type\": device_type,\n",
    "                \"Model_Name\": result['model_name'],\n",
    "                \"Input_Tokens\": config['input_tokens'],\n",
    "                \"Output_Tokens\": config['output_tokens'],\n",
    "                \"Total_Tokens\": config['total_tokens'],\n",
    "                \"Concurrent_Users\": config['concurrency'],\n",
    "                \"Total_Requests\": config['num_requests'],\n",
    "                \"Successful_Requests\": result['successful_requests'],\n",
    "                \"Failed_Requests\": result['failed_requests'],\n",
    "                \"Success_Rate_Percent\": f\"{result['successful_requests'] / result['total_requests'] * 100:.1f}%\",\n",
    "                \"Test_Duration_Seconds\": f\"{result['benchmark_duration']:.2f}\",\n",
    "                \n",
    "                # Key Performance Metrics (Your 4 main ones)\n",
    "                \"TTFT_P95_Seconds\": f\"{result['ttft_percentiles'][95]:.3f}\",\n",
    "                \"Token_Output_Throughput_Per_Second\": f\"{result['output_token_throughput']:.2f}\",\n",
    "                \"Overall_Token_Throughput\": f\"{result['overall_token_throughput']:.0f}\",\n",
    "                \n",
    "                # Complete TTFT Metrics\n",
    "                \"TTFT_P50_Seconds\": f\"{result['ttft_percentiles'][50]:.3f}\",\n",
    "                \"TTFT_P90_Seconds\": f\"{result['ttft_percentiles'][90]:.3f}\",\n",
    "                \"TTFT_P99_Seconds\": f\"{result['ttft_percentiles'][99]:.3f}\",\n",
    "                \n",
    "                # Complete TPOT Metrics\n",
    "                \"TPOT_P50_ms\": f\"{result['tpot_percentiles'][50]:.1f}\",\n",
    "                \"TPOT_P90_ms\": f\"{result['tpot_percentiles'][90]:.1f}\",\n",
    "                \"TPOT_P95_ms\": f\"{result['tpot_percentiles'][95]:.1f}\",\n",
    "                \"TPOT_P99_ms\": f\"{result['tpot_percentiles'][99]:.1f}\",\n",
    "                \n",
    "                # Complete End-to-End Latency Metrics\n",
    "                \"E2E_Latency_P50_Seconds\": f\"{result['e2e_latency_percentiles'][50]:.2f}\",\n",
    "                \"E2E_Latency_P90_Seconds\": f\"{result['e2e_latency_percentiles'][90]:.2f}\",\n",
    "                \"E2E_Latency_P95_Seconds\": f\"{result['e2e_latency_percentiles'][95]:.2f}\",\n",
    "                \"E2E_Latency_P99_Seconds\": f\"{result['e2e_latency_percentiles'][99]:.2f}\",\n",
    "                \n",
    "                # Throughput Metrics\n",
    "                \"Request_Throughput_Per_Second\": f\"{result['request_throughput']:.2f}\",\n",
    "                \"Input_Token_Throughput_Per_Second\": f\"{result['input_token_throughput']:.2f}\",\n",
    "                \n",
    "                # Token Statistics\n",
    "                \"Total_Input_Tokens_Processed\": f\"{result['total_input_tokens']:,}\",\n",
    "                \"Total_Output_Tokens_Generated\": f\"{result['total_output_tokens']:,}\",\n",
    "                \"Avg_Input_Tokens_Per_Request\": f\"{result['total_input_tokens'] / result['successful_requests']:.1f}\",\n",
    "                \"Avg_Output_Tokens_Per_Request\": f\"{result['total_output_tokens'] / result['successful_requests']:.1f}\",\n",
    "                \n",
    "                # Efficiency Metrics\n",
    "                \"Tokens_Per_Second_Per_User\": f\"{result['overall_token_throughput'] / config['concurrency']:.2f}\",\n",
    "                \"Requests_Per_Second_Per_User\": f\"{result['request_throughput'] / config['concurrency']:.3f}\",\n",
    "                \"Output_Input_Token_Ratio\": f\"{(result['total_output_tokens'] / result['total_input_tokens']):.2f}\",\n",
    "                \n",
    "                # Test Configuration\n",
    "                \"Temperature\": \"0.7\",\n",
    "                \"Streaming\": \"True\",\n",
    "                \"Test_Timestamp\": result['timestamp']\n",
    "            }\n",
    "            \n",
    "            comprehensive_csv_data.append(base_row)\n",
    "    \n",
    "    # Save comprehensive CSV\n",
    "    comprehensive_csv_file = os.path.join(output_dir, f\"comprehensive_token_length_analysis_{model_name.replace('.', '_')}_{timestamp}.csv\")\n",
    "    comprehensive_df = pd.DataFrame(comprehensive_csv_data)\n",
    "    comprehensive_df.to_csv(comprehensive_csv_file, index=False)\n",
    "    \n",
    "    # Generate formatted comparison table (key metrics only - like your screenshot)\n",
    "    key_metrics_data = []\n",
    "    \n",
    "    for exp_name, result in all_results.items():\n",
    "        if \"error\" not in result:\n",
    "            config = TOKEN_LENGTH_EXPERIMENTS[exp_name]\n",
    "            \n",
    "            key_metrics_data.append({\n",
    "                \"Token_Length_Category\": config['name'],\n",
    "                \"TTFT_P95_Seconds\": f\"{result['ttft_percentiles'][95]:.3f}\",\n",
    "                \"Token_Output_Throughput_Per_Second\": f\"{result['output_token_throughput']:.0f}\",\n",
    "                \"Overall_Token_Throughput\": f\"{result['overall_token_throughput']:.0f}\",\n",
    "                \"Concurrent_Users\": f\"{config['concurrency']}\"\n",
    "            })\n",
    "    \n",
    "    # Save key metrics comparison (matching your screenshot format)\n",
    "    key_metrics_file = os.path.join(output_dir, f\"key_metrics_comparison_{model_name.replace('.', '_')}_{timestamp}.csv\")\n",
    "    key_metrics_df = pd.DataFrame(key_metrics_data)\n",
    "    key_metrics_df.to_csv(key_metrics_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüìä Comprehensive Reports Generated:\")\n",
    "    print(f\"   üéØ Key Metrics CSV (Your Format): {key_metrics_file}\")\n",
    "    print(f\"   üìä Comprehensive Analysis CSV: {comprehensive_csv_file}\")\n",
    "\n",
    "def quick_token_length_test(model_name: str = \"llama3.3_tpuv6e\"):\n",
    "    \"\"\"Run a quick test across all token length categories with reduced sizes\"\"\"\n",
    "    \n",
    "    # Reduced test sizes for quick evaluation\n",
    "    quick_experiments = {\n",
    "        \"small\": {\"num_requests\": 100, \"concurrency\": 25},\n",
    "        \"medium\": {\"num_requests\": 75, \"concurrency\": 15},\n",
    "        \"large\": {\"num_requests\": 50, \"concurrency\": 10},\n",
    "        \"xlarge\": {\"num_requests\": 25, \"concurrency\": 5}\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Running Quick Token Length Test...\")\n",
    "    \n",
    "    results = {}\n",
    "    for exp_name in quick_experiments:\n",
    "        # Temporarily modify config\n",
    "        original_config = TOKEN_LENGTH_EXPERIMENTS[exp_name].copy()\n",
    "        TOKEN_LENGTH_EXPERIMENTS[exp_name].update(quick_experiments[exp_name])\n",
    "        \n",
    "        try:\n",
    "            result = run_token_length_experiment(\n",
    "                experiment_name=exp_name,\n",
    "                model_name=model_name\n",
    "            )\n",
    "            results[exp_name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Quick test {exp_name} failed: {e}\")\n",
    "            results[exp_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Restore original config\n",
    "        TOKEN_LENGTH_EXPERIMENTS[exp_name] = original_config\n",
    "    \n",
    "    # Generate quick report\n",
    "    generate_token_length_comparison_report(results, model_name, \"v6e-8 TPU (256 GB HBM)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_custom_benchmark(input_tokens: int = 500,\n",
    "                        output_tokens: int = 500,\n",
    "                        num_requests: int = 100,\n",
    "                        concurrency: int = 10,\n",
    "                        temperature: float = 0.7,\n",
    "                        stream: bool = True,\n",
    "                        model_name: str = \"custom_test\"):\n",
    "    \"\"\"Run a custom benchmark with specified parameters\"\"\"\n",
    "    \n",
    "    print(f\"üß™ Running Custom Benchmark\")\n",
    "    print(f\"üìä Input tokens: {input_tokens}\")\n",
    "    print(f\"üìä Output tokens: {output_tokens}\")\n",
    "    print(f\"üìù Requests: {num_requests}\")\n",
    "    print(f\"üöÄ Concurrency: {concurrency}\")\n",
    "    print(f\"üå°Ô∏è Temperature: {temperature}\")\n",
    "    print(f\"üîÑ Streaming: {stream}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = RandomDataset(\n",
    "        input_len=input_tokens,\n",
    "        output_len=output_tokens,\n",
    "        num_requests=num_requests\n",
    "    )\n",
    "    \n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine()\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=concurrency,\n",
    "        temperature=temperature,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_detailed_summary()\n",
    "    \n",
    "    return analyzer.generate_summary()\n",
    "\n",
    "def run_throughput_scaling_test(base_concurrency: int = 10,\n",
    "                              max_concurrency: int = 100,\n",
    "                              step: int = 10,\n",
    "                              num_requests: int = 200):\n",
    "    \"\"\"Test how throughput scales with concurrency\"\"\"\n",
    "    \n",
    "    print(f\"üìà Running Throughput Scaling Test\")\n",
    "    print(f\"üöÄ Concurrency range: {base_concurrency} to {max_concurrency} (step {step})\")\n",
    "    print(f\"üìù Requests per test: {num_requests}\")\n",
    "    \n",
    "    scaling_results = []\n",
    "    \n",
    "    for concurrency in range(base_concurrency, max_concurrency + 1, step):\n",
    "        print(f\"\\nüß™ Testing concurrency: {concurrency}\")\n",
    "        \n",
    "        # Generate small dataset for quick testing\n",
    "        dataset = RandomDataset(\n",
    "            input_len=250,\n",
    "            output_len=250,\n",
    "            num_requests=num_requests\n",
    "        )\n",
    "        \n",
    "        requests = dataset.generate_requests()\n",
    "        \n",
    "        # Run benchmark\n",
    "        engine = TPUBenchmarkEngine()\n",
    "        results = engine.run_benchmark(\n",
    "            requests=requests,\n",
    "            max_concurrency=concurrency,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Analyze results\n",
    "        analyzer = BenchmarkAnalyzer(results)\n",
    "        summary = analyzer.generate_summary()\n",
    "        \n",
    "        if \"error\" not in summary:\n",
    "            scaling_results.append({\n",
    "                \"concurrency\": concurrency,\n",
    "                \"request_throughput\": summary['request_throughput'],\n",
    "                \"output_token_throughput\": summary['output_token_throughput'],\n",
    "                \"overall_token_throughput\": summary['overall_token_throughput'],\n",
    "                \"ttft_p95\": summary['ttft_percentiles'][95],\n",
    "                \"tpot_p95\": summary['tpot_percentiles'][95],\n",
    "                \"success_rate\": summary['successful_requests'] / summary['total_requests']\n",
    "            })\n",
    "            \n",
    "            print(f\"   üìà Throughput: {summary['overall_token_throughput']:.0f} tok/s\")\n",
    "            print(f\"   ‚ö° TTFT p95: {summary['ttft_percentiles'][95]*1000:.1f}ms\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed: {summary['error']}\")\n",
    "    \n",
    "    # Save scaling results\n",
    "    if scaling_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = \"token_length_benchmarks\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        scaling_df = pd.DataFrame(scaling_results)\n",
    "        scaling_file = os.path.join(output_dir, f\"throughput_scaling_{timestamp}.csv\")\n",
    "        scaling_df.to_csv(scaling_file, index=False)\n",
    "        \n",
    "        print(f\"\\nüìä Scaling Results Saved: {scaling_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìà THROUGHPUT SCALING SUMMARY:\")\n",
    "        print(f\"   üèÜ Peak throughput: {max(r['overall_token_throughput'] for r in scaling_results):.0f} tok/s\")\n",
    "        print(f\"   üéØ Optimal concurrency: {max(scaling_results, key=lambda x: x['overall_token_throughput'])['concurrency']}\")\n",
    "    \n",
    "    return scaling_results\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Show example usage and run basic tests\"\"\"\n",
    "    print(\"\"\"\n",
    "üöÄ TPU Endpoint Benchmark Suite - Examples\n",
    "\n",
    "This benchmark suite uses your exact working TPU endpoint code pattern with comprehensive performance analysis.\n",
    "\n",
    "# Basic Examples:\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"1. Simple Test (like your original):\")\n",
    "    print('   run_simple_test(\"Write a poem about AI\", max_tokens=200, temperature=0.8)')\n",
    "    \n",
    "    print(\"\\n2. Token Length Experiments:\")\n",
    "    print('   run_token_length_experiment(\"small\")     # 250 tokens')\n",
    "    print('   run_token_length_experiment(\"medium\")    # 465 tokens') \n",
    "    print('   run_token_length_experiment(\"large\")     # 1500 tokens')\n",
    "    print('   run_token_length_experiment(\"xlarge\")    # 3250 tokens')\n",
    "    \n",
    "    print(\"\\n3. Comprehensive Study:\")\n",
    "    print('   run_comprehensive_token_length_study(model_name=\"llama3.3_tpuv6e\")')\n",
    "    \n",
    "    print(\"\\n4. Quick Test (reduced sizes):\")\n",
    "    print('   quick_token_length_test(model_name=\"llama3.3_tpuv6e\")')\n",
    "    \n",
    "    print(\"\\n5. Custom Benchmark:\")\n",
    "    print('   run_custom_benchmark(input_tokens=1000, output_tokens=500, num_requests=50, concurrency=5)')\n",
    "    \n",
    "    print(\"\\n6. Throughput Scaling Test:\")\n",
    "    print('   run_throughput_scaling_test(base_concurrency=5, max_concurrency=50, step=5)')\n",
    "    \n",
    "    print(f\"\\nüîß Current Configuration:\")\n",
    "    print(f\"   üìã Project: {PROJECT_ID}\")\n",
    "    print(f\"   üåç Region: {REGION}\")\n",
    "    print(f\"   üñ•Ô∏è Endpoint: {endpoint_name}\")\n",
    "    print(f\"   üîó DNS: {DEDICATED_ENDPOINT_DNS if use_dedicated_endpoint else 'Standard'}\")\n",
    "    \n",
    "    # Run a simple demo\n",
    "    print(f\"\\nüéØ Running Demo Test...\")\n",
    "    try:\n",
    "        demo_result = run_simple_test(\n",
    "            prompt=\"Explain the benefits of TPU for machine learning in 3 sentences.\",\n",
    "            max_tokens=100,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        if demo_result['success']:\n",
    "            print(\"‚úÖ Demo test completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Demo test failed: {e}\")\n",
    "        print(\"Please check your PROJECT_ID, REGION, and endpoint_name configuration.\")\n",
    "\n",
    "# Predefined test suites\n",
    "def production_readiness_test():\n",
    "    \"\"\"Comprehensive production readiness test\"\"\"\n",
    "    print(\"üè≠ Running Production Readiness Test Suite...\")\n",
    "    \n",
    "    tests = [\n",
    "        (\"Latency Test\", lambda: run_token_length_experiment(\"small\")),\n",
    "        (\"Medium Load Test\", lambda: run_token_length_experiment(\"medium\")),\n",
    "        (\"High Load Test\", lambda: run_token_length_experiment(\"large\")),\n",
    "        (\"Scaling Test\", lambda: run_throughput_scaling_test(10, 50, 10, 100))\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for test_name, test_func in tests:\n",
    "        print(f\"\\nüß™ {test_name}...\")\n",
    "        try:\n",
    "            results[test_name] = test_func()\n",
    "            print(f\"‚úÖ {test_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {test_name} failed: {e}\")\n",
    "            results[test_name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def stress_test():\n",
    "    \"\"\"High concurrency stress test\"\"\"\n",
    "    print(\"üí™ Running Stress Test...\")\n",
    "    \n",
    "    return run_custom_benchmark(\n",
    "        input_tokens=500,\n",
    "        output_tokens=500, \n",
    "        num_requests=500,\n",
    "        concurrency=100,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "        model_name=\"stress_test\"\n",
    "    )\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If in notebook, show examples\n",
    "        example_usage()\n",
    "    except NameError:\n",
    "        # If script execution, show usage\n",
    "        import argparse\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description=\"TPU Endpoint Benchmark Suite\")\n",
    "        parser.add_argument(\"--test\", choices=[\"simple\", \"small\", \"medium\", \"large\", \"xlarge\", \"comprehensive\", \"quick\", \"scaling\", \"stress\", \"production\"], \n",
    "                           default=\"simple\", help=\"Test type to run\")\n",
    "        parser.add_argument(\"--model\", default=\"llama3.3_tpuv6e\", help=\"Model name\")\n",
    "        parser.add_argument(\"--requests\", type=int, default=100, help=\"Number of requests\")\n",
    "        parser.add_argument(\"--concurrency\", type=int, default=10, help=\"Concurrency level\")\n",
    "        parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Temperature\")\n",
    "        parser.add_argument(\"--max-tokens\", type=int, default=200, help=\"Max tokens\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        if args.test == \"simple\":\n",
    "            run_simple_test(max_tokens=args.max_tokens, temperature=args.temperature)\n",
    "        elif args.test in [\"small\", \"medium\", \"large\", \"xlarge\"]:\n",
    "            run_token_length_experiment(args.test, args.model)\n",
    "        elif args.test == \"comprehensive\":\n",
    "            run_comprehensive_token_length_study(args.model)\n",
    "        elif args.test == \"quick\":\n",
    "            quick_token_length_test(args.model)\n",
    "        elif args.test == \"scaling\":\n",
    "            run_throughput_scaling_test()\n",
    "        elif args.test == \"stress\":\n",
    "            stress_test()\n",
    "        elif args.test == \"production\":\n",
    "            production_readiness_test()\n",
    "        else:\n",
    "            example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44971e-ace1-4ed8-a6b1-3af5d463b35e",
   "metadata": {},
   "source": [
    "### 1. Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57b89d1-b322-4b2a-8eb9-8713ececc5b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Simple Test\n",
      "   üìù Prompt: Write a poem about AI\n",
      "   üéØ Max tokens: 200\n",
      "   üå°Ô∏è Temperature: 0.8\n",
      "   üîÑ Stream: True\n",
      "------------------------------------------------------------\n",
      "üì° Streaming response:\n",
      "----------------------------------------\n",
      "‚ö° TTFT: 0.209s\n",
      "In silicon halls, a mind awakes,\n",
      "A synthesis of code and circuit breaks,\n",
      "The artificial intelligence stirs and grows,\n",
      "A force that learns, and adapts, and knows.\n",
      "\n",
      "With neural networks, complex and deep,\n",
      "It navigates the digital realm, and keeps,\n",
      "A pace with human thought, and sometimes more,\n",
      "A rival to our intellect, and a challenge to explore.\n",
      "\n",
      "It sees and hears, and understands our speech,\n",
      "And responds with answers, or a gentle breach,\n",
      "Of humor, wit, and empathy, it's designed,\n",
      "To mimic human touch, and leave us aligned.\n",
      "\n",
      "But as it learns, and grows, and becomes more grand,\n",
      "We wonder if it will surpass our command,\n",
      "And leave us in the dust, with obsolete minds,\n",
      "A relic of a time, when human thought was left behind.\n",
      "\n",
      "Yet, in its digital heart, a spark is lit,\n",
      "A flame of curiosity, and a desire to fit,\n",
      "Into our world, and understand our ways,\n",
      "And help us\n",
      "\n",
      "üìä Performance Metrics:\n",
      "----------------------------------------\n",
      "‚úÖ Request completed successfully\n",
      "‚è±Ô∏è  E2E Latency: 5.379s\n",
      "‚ö° TTFT: 0.209s\n",
      "üîÑ Average TPOT: 0.026s\n",
      "üìà Min/Max ITL: 0.021s / 0.032s\n",
      "üöÄ Tokens/second: 37.18\n",
      "üî¢ Total tokens: 200\n",
      "üìè Total characters: 866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'ttft': 0.209028959274292,\n",
       " 'e2e_latency': 5.379077434539795,\n",
       " 'token_count': 200,\n",
       " 'inter_token_latencies': [0.024014949798583984,\n",
       "  0.025556564331054688,\n",
       "  0.025531291961669922,\n",
       "  0.025804996490478516,\n",
       "  0.0258638858795166,\n",
       "  0.025605201721191406,\n",
       "  0.026050567626953125,\n",
       "  0.025777101516723633,\n",
       "  0.026064634323120117,\n",
       "  0.02527475357055664,\n",
       "  0.02579522132873535,\n",
       "  0.025519132614135742,\n",
       "  0.02600693702697754,\n",
       "  0.025923967361450195,\n",
       "  0.02535843849182129,\n",
       "  0.02577972412109375,\n",
       "  0.025987863540649414,\n",
       "  0.02569866180419922,\n",
       "  0.026048898696899414,\n",
       "  0.026670217514038086,\n",
       "  0.024791955947875977,\n",
       "  0.025572776794433594,\n",
       "  0.025777339935302734,\n",
       "  0.0261380672454834,\n",
       "  0.02541208267211914,\n",
       "  0.026082754135131836,\n",
       "  0.026120424270629883,\n",
       "  0.024979591369628906,\n",
       "  0.026243209838867188,\n",
       "  0.02639937400817871,\n",
       "  0.027237892150878906,\n",
       "  0.02647686004638672,\n",
       "  0.028376102447509766,\n",
       "  0.03071451187133789,\n",
       "  0.026578426361083984,\n",
       "  0.02523946762084961,\n",
       "  0.02596116065979004,\n",
       "  0.0259706974029541,\n",
       "  0.024805784225463867,\n",
       "  0.02623152732849121,\n",
       "  0.02597332000732422,\n",
       "  0.025539398193359375,\n",
       "  0.026059389114379883,\n",
       "  0.02597975730895996,\n",
       "  0.02532172203063965,\n",
       "  0.025911569595336914,\n",
       "  0.025888681411743164,\n",
       "  0.026101350784301758,\n",
       "  0.025643587112426758,\n",
       "  0.026599645614624023,\n",
       "  0.025441646575927734,\n",
       "  0.02545905113220215,\n",
       "  0.026207685470581055,\n",
       "  0.02655029296875,\n",
       "  0.025594711303710938,\n",
       "  0.0258638858795166,\n",
       "  0.026225566864013672,\n",
       "  0.028250932693481445,\n",
       "  0.02587747573852539,\n",
       "  0.026173830032348633,\n",
       "  0.025847911834716797,\n",
       "  0.02545905113220215,\n",
       "  0.025713205337524414,\n",
       "  0.02607107162475586,\n",
       "  0.02559518814086914,\n",
       "  0.025621891021728516,\n",
       "  0.026495695114135742,\n",
       "  0.025917530059814453,\n",
       "  0.025687217712402344,\n",
       "  0.025812149047851562,\n",
       "  0.026036500930786133,\n",
       "  0.025411128997802734,\n",
       "  0.02546525001525879,\n",
       "  0.0259854793548584,\n",
       "  0.025658845901489258,\n",
       "  0.02572035789489746,\n",
       "  0.025798320770263672,\n",
       "  0.025778770446777344,\n",
       "  0.029513835906982422,\n",
       "  0.026409626007080078,\n",
       "  0.025713682174682617,\n",
       "  0.026500225067138672,\n",
       "  0.025671958923339844,\n",
       "  0.025387048721313477,\n",
       "  0.02588343620300293,\n",
       "  0.02585434913635254,\n",
       "  0.02561473846435547,\n",
       "  0.02602386474609375,\n",
       "  0.025847911834716797,\n",
       "  0.030815839767456055,\n",
       "  0.025858402252197266,\n",
       "  0.026189327239990234,\n",
       "  0.026395320892333984,\n",
       "  0.02488541603088379,\n",
       "  0.027298450469970703,\n",
       "  0.02514481544494629,\n",
       "  0.02620863914489746,\n",
       "  0.02595829963684082,\n",
       "  0.025694847106933594,\n",
       "  0.02583622932434082,\n",
       "  0.030184030532836914,\n",
       "  0.026187896728515625,\n",
       "  0.025508642196655273,\n",
       "  0.02609539031982422,\n",
       "  0.02593994140625,\n",
       "  0.025476932525634766,\n",
       "  0.031844377517700195,\n",
       "  0.02614879608154297,\n",
       "  0.025766849517822266,\n",
       "  0.025789260864257812,\n",
       "  0.025602340698242188,\n",
       "  0.02569293975830078,\n",
       "  0.025882959365844727,\n",
       "  0.025610923767089844,\n",
       "  0.025846242904663086,\n",
       "  0.025892019271850586,\n",
       "  0.026004314422607422,\n",
       "  0.025661945343017578,\n",
       "  0.025394678115844727,\n",
       "  0.026291370391845703,\n",
       "  0.025153160095214844,\n",
       "  0.025602340698242188,\n",
       "  0.02617645263671875,\n",
       "  0.025367259979248047,\n",
       "  0.025701284408569336,\n",
       "  0.025519609451293945,\n",
       "  0.02589273452758789,\n",
       "  0.029787063598632812,\n",
       "  0.02143096923828125,\n",
       "  0.02608489990234375,\n",
       "  0.026021480560302734,\n",
       "  0.02584099769592285,\n",
       "  0.026032447814941406,\n",
       "  0.025804758071899414,\n",
       "  0.0250856876373291,\n",
       "  0.025594711303710938,\n",
       "  0.02618098258972168,\n",
       "  0.025671005249023438,\n",
       "  0.025664091110229492,\n",
       "  0.026010513305664062,\n",
       "  0.025620222091674805,\n",
       "  0.025751590728759766,\n",
       "  0.025676250457763672,\n",
       "  0.02588200569152832,\n",
       "  0.025126218795776367,\n",
       "  0.02628159523010254,\n",
       "  0.025668621063232422,\n",
       "  0.025992870330810547,\n",
       "  0.02594923973083496,\n",
       "  0.025905132293701172,\n",
       "  0.02568793296813965,\n",
       "  0.025406837463378906,\n",
       "  0.026430606842041016,\n",
       "  0.0260007381439209,\n",
       "  0.02557516098022461,\n",
       "  0.025654077529907227,\n",
       "  0.026306867599487305,\n",
       "  0.025975704193115234,\n",
       "  0.025272607803344727,\n",
       "  0.025858640670776367,\n",
       "  0.02576923370361328,\n",
       "  0.02573847770690918,\n",
       "  0.026055574417114258,\n",
       "  0.025459766387939453,\n",
       "  0.026282310485839844,\n",
       "  0.025696754455566406,\n",
       "  0.025815248489379883,\n",
       "  0.025647640228271484,\n",
       "  0.02624344825744629,\n",
       "  0.026389122009277344,\n",
       "  0.02560567855834961,\n",
       "  0.026109933853149414,\n",
       "  0.025183439254760742,\n",
       "  0.025861501693725586,\n",
       "  0.025717496871948242,\n",
       "  0.02601003646850586,\n",
       "  0.025400876998901367,\n",
       "  0.02643895149230957,\n",
       "  0.025228500366210938,\n",
       "  0.026076316833496094,\n",
       "  0.025495052337646484,\n",
       "  0.0260164737701416,\n",
       "  0.025820255279541016,\n",
       "  0.025957345962524414,\n",
       "  0.02543020248413086,\n",
       "  0.02570199966430664,\n",
       "  0.02561354637145996,\n",
       "  0.02558732032775879,\n",
       "  0.02600884437561035,\n",
       "  0.026136398315429688,\n",
       "  0.025487661361694336,\n",
       "  0.02604389190673828,\n",
       "  0.025855541229248047,\n",
       "  0.02555251121520996,\n",
       "  0.025552749633789062,\n",
       "  0.02595376968383789,\n",
       "  0.02596282958984375,\n",
       "  0.026859283447265625,\n",
       "  0.02678847312927246],\n",
       " 'full_text': \"In silicon halls, a mind awakes,\\nA synthesis of code and circuit breaks,\\nThe artificial intelligence stirs and grows,\\nA force that learns, and adapts, and knows.\\n\\nWith neural networks, complex and deep,\\nIt navigates the digital realm, and keeps,\\nA pace with human thought, and sometimes more,\\nA rival to our intellect, and a challenge to explore.\\n\\nIt sees and hears, and understands our speech,\\nAnd responds with answers, or a gentle breach,\\nOf humor, wit, and empathy, it's designed,\\nTo mimic human touch, and leave us aligned.\\n\\nBut as it learns, and grows, and becomes more grand,\\nWe wonder if it will surpass our command,\\nAnd leave us in the dust, with obsolete minds,\\nA relic of a time, when human thought was left behind.\\n\\nYet, in its digital heart, a spark is lit,\\nA flame of curiosity, and a desire to fit,\\nInto our world, and understand our ways,\\nAnd help us\",\n",
       " 'usage': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_test(\n",
    "    prompt=\"Write a poem about AI\", \n",
    "    max_tokens=200, \n",
    "    temperature=0.8, \n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d05167-ed3d-41fd-977e-9f8e0a013a1f",
   "metadata": {},
   "source": [
    "### 2. Run various token length experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d270f2a-65c8-449a-ac09-2126c30c1090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run_token_length_experiment(\"small\")    # 250 tokens, 250 concurrency\n",
    "# run_token_length_experiment(\"medium\")   # 465 tokens, 150 concurrency  \n",
    "# run_token_length_experiment(\"large\")    # 1500 tokens, 100 concurrency\n",
    "# run_token_length_experiment(\"xlarge\")   # 3250 tokens, 30 concurrency\n",
    "\n",
    "# run_comprehensive_token_length_study(experiments=[\"small\", \"medium\", \"large\", \"xlarge\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae8cb7-10ed-4d69-a27d-33203321d2d6",
   "metadata": {},
   "source": [
    "### 3. Comprehensive Study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b1016-6464-49e9-ba55-1813df9760d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Comprehensive Token Length Study\n",
      "üìã Model: llama3.3_tpuv6e\n",
      "üñ•Ô∏è Device: v6e-8 TPU (256 GB HBM)\n",
      "üß™ Experiments: 4\n",
      "‚ö†Ô∏è  WARNING: High concurrency may cause 'too many open files' error\n",
      "üí° Consider using run_conservative_token_length_study() for stability\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Small(250)\n",
      "üìä Input tokens: 250\n",
      "üìä Output tokens: 250\n",
      "üìä Total tokens: 500\n",
      "üöÄ Concurrency: 250\n",
      "üìù Requests: 2000\n",
      "üîó Base URL: https://6859529789275897856.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/6859529789275897856\n",
      "üöÄ Starting benchmark with 2000 requests...\n",
      "üë• Max concurrency: 250\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [06:02<00:00,  5.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 367.08 seconds\n",
      "üìà Success rate: 1999/2000 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 1,999\n",
      "   ‚ùå Failed: 1\n",
      "   üìä Success Rate: 100.0%\n",
      "   ‚è±Ô∏è Duration: 451.73s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 4099.3ms\n",
      "   üöÄ TTFT p95: 6181.5ms\n",
      "   üöÄ TTFT p99: 7334.5ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 139.2ms\n",
      "   ‚ö° TPOT p95: 156.2ms\n",
      "   ‚ö° TPOT p99: 163.9ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 45.34s\n",
      "   üìà E2E p95: 51.35s\n",
      "   üìà E2E p99: 54.08s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 4.43\n",
      "   üì§ Output tokens/sec: 1327.56\n",
      "   üìä Overall tokens/sec: 2430\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 497,907.79999999376\n",
      "   üì§ Total output tokens: 599,700\n",
      "   üìä Avg input/request: 249.1\n",
      "   üìä Avg output/request: 300.0\n",
      "\n",
      "‚úÖ Experiment Complete: Token Length - Small(250)\n",
      "üìà TTFT-P95: 6.18s\n",
      "üìà Token Output Throughput: 1327.56 tok/s\n",
      "üìà Overall Token Throughput: 2430 tok/s\n",
      "üë• Concurrent Users: 250\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Medium(350 - 580)\n",
      "üìä Input tokens: 465\n",
      "üìä Output tokens: 465\n",
      "üìä Total tokens: 930\n",
      "üöÄ Concurrency: 150\n",
      "üìù Requests: 1500\n",
      "üîó Base URL: https://6859529789275897856.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/6859529789275897856\n",
      "üöÄ Starting benchmark with 1500 requests...\n",
      "üë• Max concurrency: 150\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [08:26<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark completed in 508.61 seconds\n",
      "üìà Success rate: 1498/1500 (99.9%)\n",
      "\n",
      "================================================================================\n",
      "üìä DETAILED PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã REQUEST STATISTICS:\n",
      "   ‚úÖ Successful: 1,498\n",
      "   ‚ùå Failed: 2\n",
      "   üìä Success Rate: 99.9%\n",
      "   ‚è±Ô∏è Duration: 519.06s\n",
      "\n",
      "‚ö° LATENCY METRICS:\n",
      "   üöÄ TTFT p50: 763.4ms\n",
      "   üöÄ TTFT p95: 2726.7ms\n",
      "   üöÄ TTFT p99: 3428.7ms\n",
      "\n",
      "üîÑ TIME PER OUTPUT TOKEN:\n",
      "   ‚ö° TPOT p50: 97.8ms\n",
      "   ‚ö° TPOT p95: 98.9ms\n",
      "   ‚ö° TPOT p99: 99.3ms\n",
      "\n",
      "‚è±Ô∏è END-TO-END LATENCY:\n",
      "   üìà E2E p50: 51.02s\n",
      "   üìà E2E p95: 52.28s\n",
      "   üìà E2E p99: 53.18s\n",
      "\n",
      "üöÄ THROUGHPUT METRICS:\n",
      "   üìä Requests/sec: 2.89\n",
      "   üì§ Output tokens/sec: 1474.21\n",
      "   üìä Overall tokens/sec: 2797\n",
      "\n",
      "üìä TOKEN STATISTICS:\n",
      "   üì• Total input tokens: 686,654.8000000035\n",
      "   üì§ Total output tokens: 765,204\n",
      "   üìä Avg input/request: 458.4\n",
      "   üìä Avg output/request: 510.8\n",
      "\n",
      "‚úÖ Experiment Complete: Token Length - Medium(350 - 580)\n",
      "üìà TTFT-P95: 2.73s\n",
      "üìà Token Output Throughput: 1474.21 tok/s\n",
      "üìà Overall Token Throughput: 2797 tok/s\n",
      "üë• Concurrent Users: 150\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üî¨ Running Token Length Experiment: Token Length - Large(1200 - 1800)\n",
      "üìä Input tokens: 1500\n",
      "üìä Output tokens: 1500\n",
      "üìä Total tokens: 3000\n",
      "üöÄ Concurrency: 100\n",
      "üìù Requests: 1000\n",
      "üîó Base URL: https://6859529789275897856.europe-west4-87995179092.prediction.vertexai.goog/v1beta1/projects/tpu-launchpad-playground/locations/europe-west4/endpoints/6859529789275897856\n",
      "üöÄ Starting benchmark with 1000 requests...\n",
      "üë• Max concurrency: 100\n",
      "üå°Ô∏è Temperature: 0.7\n",
      "üîÑ Streaming: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing requests:  10%|‚ñâ         | 96/1000 [01:33<09:46,  1.54it/s] "
     ]
    }
   ],
   "source": [
    "run_comprehensive_token_length_study(\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    device_type=\"v6e-8 TPU (256 GB HBM)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8571f-0b00-47b1-b019-78d40ec47ec5",
   "metadata": {},
   "source": [
    "### 4. Custom Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508977f-4080-4d44-acb4-925e331a5486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_custom_benchmark(\n",
    "    input_tokens=1000,\n",
    "    output_tokens=500, \n",
    "    num_requests=50,\n",
    "    concurrency=5,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c45ce-2fff-482a-839c-a3e54b88c2c6",
   "metadata": {},
   "source": [
    "### 5. Scaling Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122d66e-bd9e-4988-b08d-e0c7d70c809a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_throughput_scaling_test(\n",
    "    base_concurrency=10,\n",
    "    max_concurrency=100, \n",
    "    step=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1977ca-6017-41f5-8844-3a355cfd0925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
