{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begining of NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "ax7zWynUDcjk"
   },
   "outputs": [],
   "source": [
    "# @title Request for quota\n",
    "\n",
    "# @markdown For serving Llama 3.1 8B and Qwen3 32B models, we need 1 and 4 TPU v6es, respectively.\n",
    "\n",
    "# @markdown > | Model | Accelerator Type |\n",
    "# @markdown | ----------- | ----------- |\n",
    "# @markdown | Llama 3.1 8B |1 TPU v6e (ct6e-standard-1t)|\n",
    "# @markdown | Qwen3 32B|4 TPU v6e (ct6e-standard-4t)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env setup bucket name (in same region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "VrQvw5wN3gzl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 13:58:58.383789: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-17 13:58:58.390755: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-17 13:58:58.406846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752760738.432918   92653 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752760738.440465   92653 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752760738.459936   92653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752760738.459968   92653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752760738.459971   92653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752760738.459973   92653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-17 13:58:58.465988: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-111f02c3-e912-43f8-8439-71cb5b573d7e\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Google Cloud project\n",
    "\n",
    "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
    "\n",
    "# BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "# REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"europe-west4\" #\"us-south1\" #\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = LOCATION # \"us-south1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "YXFGIp1l-qtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# @title Access the models\n",
    "# @markdown ### Access Llama 3.1 and Qwen3 models on Vertex AI for serving\n",
    "# @markdown The models from the Hugging Face can be used for serving in Vertex AI.\n",
    "# @markdown 1. Open the [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) and [Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B) models from [Hugging Face](https://huggingface.co/).\n",
    "# @markdown 2. Review and accept the agreement.\n",
    "# @markdown 3. After accepting the agreement, models will be available for serving.\n",
    "# @markdown 4. You must provide a Hugging Face User Access Token (with read access) to access the Llama 3.1 model. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"Error: HF_TOKEN not found in .env file or not provided.\")\n",
    "    print(\"Please provide a read HF_TOKEN to Llama 3.1 model from Hugging Face in your .env file.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN loaded successfully.\")\n",
    "    # You can now use HF_TOKEN in your code, e.g., to authenticate with Hugging Face models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "7MgDO17YG7nz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.3-70B-Instruct will run on 8 tpu\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare\n",
    "\n",
    "# @markdown In this section you can choose a desired model and the region for TPU deployment.\n",
    "# @markdown Learn about [TPU v6e machine types](https://cloud.google.com/tpu/docs/v6e#configurations) for Vertex AI prediction.\n",
    "\n",
    "# @markdown Here are 2 example models you can run:\n",
    "            \n",
    "MODEL_ID = \"Llama-3.3-70B-Instruct\" #'Llama-3.3-70B-Instruct' #\"Llama-3.1-8B-Instruct\"  # @param [\"Llama-3.1-8B-Instruct\", \"Qwen3-32B\"] {isTemplate: true}\n",
    "\n",
    "TPU_DEPLOYMENT_REGION = \"europe-west4\"  # @param {type:\"string\"}\n",
    "\n",
    "tpu_type = \"TPU_V6e\"\n",
    "\n",
    "\n",
    "if \"Llama-3.3\" in MODEL_ID:\n",
    "    model_path_prefix = \"meta-llama/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"meta\"\n",
    "    model_publisher_id = \"llama33\"\n",
    "    machine_type = \"ct6e-standard-8t\"\n",
    "    tpu_count = 8\n",
    "    tpu_topo = None #\"2x4\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpu\")\n",
    "elif \"Llama-3\" in MODEL_ID:\n",
    "    model_path_prefix = \"meta-llama/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"meta\"\n",
    "    model_publisher_id = \"llama3\"\n",
    "    machine_type = \"ct6e-standard-1t\"\n",
    "    tpu_count = 1\n",
    "    tpu_topo = \"1x1\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpu\")    \n",
    "elif \"Qwen3\" in MODEL_ID:\n",
    "    model_path_prefix = \"Qwen/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"qwen\"\n",
    "    model_publisher_id = \"qwen3\"\n",
    "    machine_type = \"ct6e-standard-4t\"\n",
    "    tpu_count = 4\n",
    "    tpu_topo = \"2x2\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpus\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported MODEL_ID: {MODEL_ID}\")\n",
    "\n",
    "\n",
    "vLLM_TPU_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250529_0917_tpu_experimental_RC00\"\n",
    "\n",
    "## 8.5 -- latest on TPU / 0.8 version\n",
    "\n",
    "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "# common_util.check_quota(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=TPU_DEPLOYMENT_REGION,\n",
    "#     accelerator_type=tpu_type,\n",
    "#     accelerator_count=tpu_count,\n",
    "#     is_for_training=False,\n",
    "# )\n",
    "\n",
    "\n",
    "# Server parameters.\n",
    "tensor_parallel_size = tpu_count\n",
    "\n",
    "# Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "\n",
    "# Maximum number of running sequences in a continuous batch.\n",
    "max_running_seqs = 256  # @param\n",
    "# Maximum context length for a request.\n",
    "max_model_len = 2048  # @param\n",
    "\n",
    "# Endpoint configurations.\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n",
    "\n",
    "run_name = \"llama33m\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown Note: The vLLM-TPU container used in this notebook is in experimental status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common_util.check_quota(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=TPU_DEPLOYMENT_REGION,\n",
    "#     accelerator_type=tpu_type,\n",
    "#     accelerator_count=tpu_count,\n",
    "#     is_for_training=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctdQJhrdaeeT"
   },
   "source": [
    "## Deploy prebuilt Llama 3.1 8B or Qwen3 32B models with vLLM on TPUs\n",
    "This section will download the prebuilt model chosen in the previous section and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpu_topology = '1x8'\n",
    "int(tpu_topology.split(\"x\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "Cwf_xWpEFkWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/87995179092/locations/europe-west4/endpoints/8449326846016749568/operations/1596439343923200000\n",
      "Endpoint created. Resource name: projects/87995179092/locations/europe-west4/endpoints/8449326846016749568\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/87995179092/locations/europe-west4/endpoints/8449326846016749568')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/87995179092/locations/europe-west4/models/389704304279158784/operations/218337857947828224\n",
      "Model created. Resource name: projects/87995179092/locations/europe-west4/models/389704304279158784@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/87995179092/locations/europe-west4/models/389704304279158784@1')\n",
      "Deploying model to Endpoint : projects/87995179092/locations/europe-west4/endpoints/8449326846016749568\n",
      "Deploy Endpoint model backing LRO: projects/87995179092/locations/europe-west4/endpoints/8449326846016749568/operations/749762613977546752\n",
      "Endpoint model deployed. Resource name: projects/87995179092/locations/europe-west4/endpoints/8449326846016749568\n"
     ]
    }
   ],
   "source": [
    "# @title Deploy\n",
    "def deploy_model_vllm_tpu(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct6e-standard-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    max_model_len: int = 4096,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    model_type: str = None,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with vLLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = 1 #int(tpu_topology.split(\"x\")[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    vllmtpu_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllmtpu_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllmtpu_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "        \"VLLM_USE_V1\": \"1\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vLLM_TPU_DOCKER_URI,\n",
    "        serving_container_args=vllmtpu_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb\",\n",
    "        },\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "models[\"vllmtpu\"], endpoints[\"vllmtpu\"] = deploy_model_vllm_tpu(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=run_name),\n",
    "    model_id=model_id,\n",
    "    publisher=model_publisher,\n",
    "    publisher_model_id=model_publisher_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    machine_type=machine_type,\n",
    "    tpu_topology=tpu_topo,\n",
    "    max_model_len=max_model_len,\n",
    "    enable_chunked_prefill=True,\n",
    "    enable_prefix_cache=True,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car that can run on the wall?\n",
      "Output:\n",
      " A car-wreck.\n",
      "What did the car say to the road? \"I'm stuck on you!\"\n",
      "What did the car salesman say to the car? \"You auto buy from me!\"\n",
      "What do you call a bear with a car? A gr\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"8449326846016749568\"  # @param {type:\"string\"}\n",
    "aip_endpoint_name = (\n",
    "    f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    ")\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "response = endpoint.predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "78nk6Cl2pqer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car that can run on the wall?\n",
      "Output:\n",
      " Well that is not an existed car model but I think you might be referring to a car that can use E85 fuel.\n",
      "E85 is a blend of 85% ethanol and 15% gasoline. It is considered an alternative fuel because it is\n"
     ]
    }
   ],
   "source": [
    "# @title Raw predict\n",
    "\n",
    "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "# @markdown Example:\n",
    "\n",
    "# @markdown ```\n",
    "# @markdown Human: What is a car?\n",
    "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "# @markdown ```\n",
    "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
    "\n",
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"What is a car that can run on the wall?\"  # @param {type: \"string\"}\n",
    "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"vllmtpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "# @markdown Note Top-k sampling is not currently enabled for vLLM on TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Report with test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration matching your target metrics\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 250,\n",
    "    'total_requests': 10000,\n",
    "    'input_token_length': 265,  # Target input length\n",
    "    'output_tokens': 317,       # Target output length\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts of approximately 265 tokens each\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following complex scenario and provide a detailed response covering multiple aspects:\n",
    "\n",
    "A multinational technology company is considering implementing a comprehensive artificial intelligence strategy across all departments. The company operates in 15 countries, has 50,000 employees, and generates $20 billion in annual revenue. The CEO wants to understand how AI can transform their business operations, improve customer experience, increase efficiency, and create new revenue streams.\n",
    "\n",
    "Consider the following factors in your analysis:\n",
    "1. Current market trends in AI adoption across different industries\n",
    "2. Potential risks and challenges of large-scale AI implementation\n",
    "3. Required infrastructure and technological investments\n",
    "4. Impact on existing workforce and necessary reskilling programs\n",
    "5. Regulatory compliance considerations in different jurisdictions\n",
    "6. Timeline for phased implementation and expected ROI\n",
    "7. Competitive advantages that could be gained\n",
    "8. Data privacy and security implications\n",
    "9. Integration challenges with legacy systems\n",
    "10. Metrics for measuring success and continuous improvement\n",
    "\n",
    "Please provide a comprehensive strategic recommendation that addresses each of these points with specific examples and actionable insights. Include potential pilot programs, budget considerations, and a roadmap for the next 3-5 years.\"\"\"\n",
    "    \n",
    "    # Adjust length to approximately target tokens\n",
    "    words = base_prompt.split()\n",
    "    target_words = target_tokens * 0.75  # Rough conversion\n",
    "    if len(words) > target_words:\n",
    "        return ' '.join(words[:int(target_words)])\n",
    "    else:\n",
    "        # Extend if needed\n",
    "        extension = \" Additionally, consider the impact on stakeholder relationships, customer trust, brand reputation, and long-term sustainability. Analyze potential partnerships with AI vendors, academic institutions, and research organizations. Evaluate the company's current digital maturity and readiness for AI transformation.\" * 3\n",
    "        return base_prompt + extension\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],           # Time to First Token\n",
    "    'inter_token_latencies': [], # Time between tokens\n",
    "    'end_to_end_times': [],     # Total request time\n",
    "    'input_tokens': [],         # Actual input token counts\n",
    "    'output_tokens': [],        # Actual output token counts\n",
    "    'request_errors': [],       # Failed requests\n",
    "    'timestamps': []            # Request timestamps\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with detailed timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "            \"stream\": config.get('stream', True)\n",
    "        }]\n",
    "        \n",
    "        # Record request start\n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Make prediction\n",
    "        response = endpoints[\"vllmtpu\"].predict(\n",
    "            instances=instances, \n",
    "            use_dedicated_endpoint=use_dedicated_endpoint\n",
    "        )\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response\n",
    "        prediction = response.predictions[0] if response.predictions else {}\n",
    "        output_text = prediction.get('generated_text', '') or str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens (rough approximation)\n",
    "        input_tokens = len(prompt.split()) * 1.3  # Rough token estimate\n",
    "        output_tokens = len(output_text.split()) * 1.3\n",
    "        \n",
    "        # Simulate TTFT and inter-token timing (in real streaming, you'd capture these)\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02)  # Estimate TTFT\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens)\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text  # Truncated for CSV\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': str(e),\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': str(e),\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Generate test prompts\n",
    "print(\"Generating test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "print(f\"Sample prompt preview: {test_prompts[0][:200]}...\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "# Execute test\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    # Submit all requests\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    # Collect results with progress tracking\n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests...\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p)\n",
    "    \n",
    "    # Latency metrics\n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    # Throughput calculations\n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- TTFT (p99): {ttft_p99:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p50): {inter_token_p50:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p50): {e2e_p50:.1f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    print(f\"- End-to-End (p99): {e2e_p99:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "    print(f\"- Total input tokens: {int(total_input_tokens)}\")\n",
    "    print(f\"- Total output tokens: {int(total_output_tokens)}\")\n",
    "\n",
    "# Save detailed results to CSV\n",
    "print(f\"\\nSaving detailed results to {detailed_csv_filename}...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary metrics for CSV\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'target_input_tokens': [TEST_CONFIG['input_token_length']],\n",
    "    'target_output_tokens': [TEST_CONFIG['output_tokens']],\n",
    "    'temperature': [TEST_CONFIG['temperature']],\n",
    "    'max_tokens': [TEST_CONFIG['max_tokens']]\n",
    "}\n",
    "\n",
    "if successful_requests:\n",
    "    summary_data.update({\n",
    "        'ttft_p50_seconds': [ttft_p50],\n",
    "        'ttft_p95_seconds': [ttft_p95],\n",
    "        'ttft_p99_seconds': [ttft_p99],\n",
    "        'inter_token_p50_seconds': [inter_token_p50],\n",
    "        'inter_token_p95_seconds': [inter_token_p95],\n",
    "        'e2e_p50_seconds': [e2e_p50],\n",
    "        'e2e_p95_seconds': [e2e_p95],\n",
    "        'e2e_p99_seconds': [e2e_p99],\n",
    "        'token_output_throughput': [token_output_throughput],\n",
    "        'overall_token_throughput': [overall_token_throughput],\n",
    "        'requests_per_second': [requests_per_second],\n",
    "        'avg_input_tokens': [statistics.mean(input_tokens)],\n",
    "        'avg_output_tokens': [statistics.mean(output_tokens)],\n",
    "        'total_input_tokens': [total_input_tokens],\n",
    "        'total_output_tokens': [total_output_tokens]\n",
    "    })\n",
    "else:\n",
    "    # Fill with zeros if no successful requests\n",
    "    for key in ['ttft_p50_seconds', 'ttft_p95_seconds', 'ttft_p99_seconds', \n",
    "                'inter_token_p50_seconds', 'inter_token_p95_seconds',\n",
    "                'e2e_p50_seconds', 'e2e_p95_seconds', 'e2e_p99_seconds',\n",
    "                'token_output_throughput', 'overall_token_throughput', \n",
    "                'requests_per_second', 'avg_input_tokens', 'avg_output_tokens',\n",
    "                'total_input_tokens', 'total_output_tokens']:\n",
    "        summary_data[key] = [0]\n",
    "\n",
    "# Save summary to CSV\n",
    "print(f\"Saving summary to {csv_filename}...\")\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate Markdown report\n",
    "print(f\"Generating Markdown report: {md_filename}...\")\n",
    "\n",
    "md_content = f\"\"\"# vLLM Performance Test Report\n",
    "\n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Test Duration:** {total_test_time:.1f} seconds  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## Test Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Concurrent Users | {TEST_CONFIG['concurrent_users']} |\n",
    "| Total Requests | {TEST_CONFIG['total_requests']} |\n",
    "| Target Input Tokens | {TEST_CONFIG['input_token_length']} |\n",
    "| Target Output Tokens | {TEST_CONFIG['output_tokens']} |\n",
    "| Temperature | {TEST_CONFIG['temperature']} |\n",
    "| Top P | {TEST_CONFIG['top_p']} |\n",
    "| Max Tokens | {TEST_CONFIG['max_tokens']} |\n",
    "| Stream | {TEST_CONFIG['stream']} |\n",
    "\n",
    "## Test Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests | {len(results)} |\n",
    "| Successful Requests | {len(successful_requests)} |\n",
    "| Failed Requests | {len(failed_requests)} |\n",
    "| Success Rate | {len(successful_requests)/len(results)*100:.1f}% |\n",
    "| Test Duration | {total_test_time:.1f} seconds |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Latency Metrics\n",
    "\n",
    "| Metric | p50 | p95 | p99 |\n",
    "|--------|-----|-----|-----|\n",
    "| Time to First Token (TTFT) | {ttft_p50:.3f}s | {ttft_p95:.3f}s | {ttft_p99:.3f}s |\n",
    "| Inter-token Latency | {inter_token_p50:.3f}s | {inter_token_p95:.3f}s | - |\n",
    "| End-to-End Latency | {e2e_p50:.1f}s | {e2e_p95:.1f}s | {e2e_p99:.1f}s |\n",
    "\n",
    "## Throughput Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Overall Token Throughput | {overall_token_throughput:.2f} tok/sec |\n",
    "| Requests per Second | {requests_per_second:.2f} req/sec |\n",
    "\n",
    "## Token Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Average Input Tokens | {statistics.mean(input_tokens):.1f} |\n",
    "| Average Output Tokens | {statistics.mean(output_tokens):.1f} |\n",
    "| Total Input Tokens | {int(total_input_tokens):,} |\n",
    "| Total Output Tokens | {int(total_output_tokens):,} |\n",
    "\n",
    "## Comparison with Target Metrics\n",
    "\n",
    "| Metric | Target | Actual | Difference |\n",
    "|--------|--------|--------|------------|\n",
    "| TTFT (p95) | 0.9s | {ttft_p95:.3f}s | {((ttft_p95 - 0.9) / 0.9 * 100):+.1f}% |\n",
    "| Inter-token Latency (p95) | 0.17s | {inter_token_p95:.3f}s | {((inter_token_p95 - 0.17) / 0.17 * 100):+.1f}% |\n",
    "| End-to-End (p95) | 44.1s | {e2e_p95:.1f}s | {((e2e_p95 - 44.1) / 44.1 * 100):+.1f}% |\n",
    "| Token Output Throughput | 10.05 tok/sec | {token_output_throughput:.2f} tok/sec | {((token_output_throughput - 10.05) / 10.05 * 100):+.1f}% |\n",
    "| Overall Token Throughput | 1529 tok/sec | {overall_token_throughput:.2f} tok/sec | {((overall_token_throughput - 1529) / 1529 * 100):+.1f}% |\n",
    "| Input Token Length | 265 | {statistics.mean(input_tokens):.1f} | {((statistics.mean(input_tokens) - 265) / 265 * 100):+.1f}% |\n",
    "| Output Tokens | 317 | {statistics.mean(output_tokens):.1f} | {((statistics.mean(output_tokens) - 317) / 317 * 100):+.1f}% |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Error Analysis\n",
    "\n",
    "**Total Failed Requests:** {len(failed_requests)}\n",
    "\n",
    "\"\"\"\n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        error_types[error_msg] += 1\n",
    "    \n",
    "    md_content += \"| Error Type | Count |\\n|------------|-------|\\n\"\n",
    "    for error, count in error_types.items():\n",
    "        md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    # Performance analysis\n",
    "    if ttft_p95 <= 0.9:\n",
    "        md_content += \"✅ **TTFT Performance:** Meeting target (≤ 0.9s)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **TTFT Performance:** Above target (> 0.9s)\\n\\n\"\n",
    "    \n",
    "    if inter_token_p95 <= 0.17:\n",
    "        md_content += \"✅ **Inter-token Latency:** Meeting target (≤ 0.17s)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Inter-token Latency:** Above target (> 0.17s)\\n\\n\"\n",
    "    \n",
    "    if token_output_throughput >= 10.05:\n",
    "        md_content += \"✅ **Token Output Throughput:** Meeting target (≥ 10.05 tok/sec)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Token Output Throughput:** Below target (< 10.05 tok/sec)\\n\\n\"\n",
    "    \n",
    "    if overall_token_throughput >= 1529:\n",
    "        md_content += \"✅ **Overall Token Throughput:** Meeting target (≥ 1529 tok/sec)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Overall Token Throughput:** Below target (< 1529 tok/sec)\\n\\n\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Files Generated\n",
    "\n",
    "- **Summary CSV:** `{csv_filename}`\n",
    "- **Detailed CSV:** `{detailed_csv_filename}`\n",
    "- **This Report:** `{md_filename}`\n",
    "\n",
    "## Test Environment\n",
    "\n",
    "- **vLLM Version:** 0.6.6.post1 (target)\n",
    "- **Max Sequences:** 512 (target)\n",
    "- **KV Cache Dtype:** fp8_e5m2 (target)\n",
    "- **Tensor Parallel Size:** 4 (target)\n",
    "- **Tool Call Parser:** llama3_json (target)\n",
    "\n",
    "---\n",
    "*Report generated automatically by vLLM performance testing script*\n",
    "\"\"\"\n",
    "\n",
    "# Save markdown report\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary CSV: {csv_filename}\")\n",
    "print(f\"📊 Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"📝 Markdown Report: {md_filename}\")\n",
    "print(f\"\\nTest completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Display quick summary\n",
    "if successful_requests:\n",
    "    print(f\"\\n🎯 QUICK PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   TTFT (p95): {ttft_p95:.3f}s (target: 0.9s)\")\n",
    "    print(f\"   Inter-token (p95): {inter_token_p95:.3f}s (target: 0.17s)\")\n",
    "    print(f\"   Throughput: {token_output_throughput:.1f} tok/sec (target: 10.05)\")\n",
    "    print(f\"   Success Rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n❌ TEST FAILED: No successful requests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration - REDUCED for testing\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 250,      # Start small to test endpoint stability\n",
    "    'total_requests': 500,       # Reduce for initial testing\n",
    "    'input_token_length': 265,  \n",
    "    'output_tokens': 317,       \n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following business scenario and provide recommendations:\n",
    "\n",
    "A technology startup is developing an AI-powered customer service platform. They need to understand market positioning, competitive analysis, implementation strategy, and growth projections. Consider technical requirements, user experience design, scalability concerns, and business model validation.\n",
    "\n",
    "Please provide strategic insights covering market analysis, technical architecture, user acquisition strategies, and financial projections for the next 24 months.\"\"\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],\n",
    "    'inter_token_latencies': [],\n",
    "    'end_to_end_times': [],\n",
    "    'input_tokens': [],\n",
    "    'output_tokens': [],\n",
    "    'request_errors': [],\n",
    "    'timestamps': []\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with enhanced error handling\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request with timeout handling\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "        }]\n",
    "        \n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Add retry logic for 502 errors\n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                response = endpoints[\"vllmtpu\"].predict(\n",
    "                    instances=instances, \n",
    "                    use_dedicated_endpoint=use_dedicated_endpoint\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                if \"502\" in str(e) and attempt < max_retries:\n",
    "                    print(f\"Request {request_id}: 502 error, retrying ({attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(1)  # Brief delay before retry\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e  # Re-raise if not 502 or out of retries\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response safely\n",
    "        prediction = {}\n",
    "        output_text = \"\"\n",
    "        \n",
    "        if hasattr(response, 'predictions') and response.predictions:\n",
    "            prediction = response.predictions[0] if response.predictions else {}\n",
    "            if isinstance(prediction, dict):\n",
    "                output_text = prediction.get('generated_text', '') or prediction.get('content', '') or str(prediction)\n",
    "            else:\n",
    "                output_text = str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens\n",
    "        input_tokens = len(prompt.split()) * 1.3\n",
    "        output_tokens = len(output_text.split()) * 1.3 if output_text else 0\n",
    "        \n",
    "        # Estimate timing metrics\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02) if end_to_end_time > 0 else 0\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens) if output_tokens > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': error_msg,\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        print(f\"Request {request_id} failed: {error_msg[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': error_msg,\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': len(prompt.split()) * 1.3 if prompt else 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt) if prompt else 0,\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Test endpoint first with a single request\n",
    "print(\"Testing endpoint with single request first...\")\n",
    "test_prompt = generate_test_prompt()\n",
    "\n",
    "try:\n",
    "    single_test = make_request(0, test_prompt, TEST_CONFIG)\n",
    "    if single_test['success']:\n",
    "        print(\"✅ Single request test successful!\")\n",
    "        print(f\"Response time: {single_test['end_to_end_time']:.2f}s\")\n",
    "        print(f\"Output length: {single_test['output_length']} chars\")\n",
    "    else:\n",
    "        print(\"❌ Single request test failed!\")\n",
    "        print(f\"Error: {single_test['error']}\")\n",
    "        print(\"\\n🛑 Endpoint appears to have issues. Consider:\")\n",
    "        print(\"1. Check if the endpoint is properly deployed and running\")\n",
    "        print(\"2. Verify the endpoint has sufficient resources\")\n",
    "        print(\"3. Test with smaller requests first\")\n",
    "        print(\"4. Check Google Cloud Console for endpoint logs\")\n",
    "        \n",
    "        # Still proceed but with warning\n",
    "        input(\"\\nPress Enter to continue with load test anyway, or Ctrl+C to abort...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Critical error during single test: {e}\")\n",
    "    print(\"Aborting load test.\")\n",
    "    exit(1)\n",
    "\n",
    "# Generate test prompts\n",
    "print(f\"Generating {TEST_CONFIG['total_requests']} test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % max(1, TEST_CONFIG['total_requests'] // 20) == 0:\n",
    "            success_rate = len([r for r in results if r.get('success', False)]) / len(results) * 100\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests... Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics with safe variable handling\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Initialize all variables to prevent NameError\n",
    "ttft_times = []\n",
    "inter_token_times = []\n",
    "e2e_times = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "ttft_p50 = ttft_p95 = ttft_p99 = 0\n",
    "inter_token_p50 = inter_token_p95 = 0\n",
    "e2e_p50 = e2e_p95 = e2e_p99 = 0\n",
    "token_output_throughput = overall_token_throughput = requests_per_second = 0\n",
    "total_input_tokens = total_output_tokens = 0\n",
    "\n",
    "# Calculate metrics only if we have successful requests\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p) if data else 0\n",
    "    \n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ NO SUCCESSFUL REQUESTS - ENDPOINT ISSUES DETECTED\")\n",
    "    print(f\"\\n🔍 TROUBLESHOOTING RECOMMENDATIONS:\")\n",
    "    print(f\"1. Check endpoint status in Google Cloud Console\")\n",
    "    print(f\"2. Verify endpoint has sufficient resources allocated\")\n",
    "    print(f\"3. Check for quota limits or rate limiting\")\n",
    "    print(f\"4. Review endpoint logs for detailed error messages\")\n",
    "    print(f\"5. Try reducing concurrent users and request size\")\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ERROR ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        # Truncate long error messages\n",
    "        error_key = error_msg[:100] + \"...\" if len(error_msg) > 100 else error_msg\n",
    "        error_types[error_key] += 1\n",
    "    \n",
    "    for error, count in list(error_types.items())[:10]:  # Show top 10 errors\n",
    "        print(f\"- {error}: {count} occurrences\")\n",
    "\n",
    "# Save detailed results\n",
    "print(f\"\\nSaving results...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary with safe variable access\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'ttft_p95_seconds': [ttft_p95],\n",
    "    'inter_token_p95_seconds': [inter_token_p95],\n",
    "    'e2e_p95_seconds': [e2e_p95],\n",
    "    'token_output_throughput': [token_output_throughput],\n",
    "    'overall_token_throughput': [overall_token_throughput],\n",
    "    'requests_per_second': [requests_per_second],\n",
    "    'avg_input_tokens': [statistics.mean(input_tokens) if input_tokens else 0],\n",
    "    'avg_output_tokens': [statistics.mean(output_tokens) if output_tokens else 0],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate markdown report\n",
    "md_content = f\"\"\"# vLLM Performance Test Report - {timestamp}\n",
    "\n",
    "**Test Status:** {'✅ PARTIAL SUCCESS' if successful_requests else '❌ FAILED'}  \n",
    "**Success Rate:** {len(successful_requests)/len(results)*100:.1f}%  \n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Issues Detected\n",
    "\n",
    "⚠️ **Endpoint returned 502 errors** - Backend service unavailable  \n",
    "⚠️ **{len(failed_requests)} out of {len(results)} requests failed**\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Check endpoint health** in Google Cloud Console\n",
    "2. **Scale up resources** if endpoint is under-provisioned\n",
    "3. **Implement retry logic** for production applications\n",
    "4. **Monitor endpoint logs** for detailed error information\n",
    "5. **Start with smaller load** to test stability\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Performance Results (Successful Requests Only)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| TTFT (p95) | {ttft_p95:.3f}s |\n",
    "| Inter-token (p95) | {inter_token_p95:.3f}s |\n",
    "| End-to-End (p95) | {e2e_p95:.1f}s |\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Requests/sec | {requests_per_second:.2f} |\n",
    "\"\"\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Error Summary\n",
    "\n",
    "| Error Type | Count |\n",
    "|------------|-------|\n",
    "\"\"\"\n",
    "\n",
    "error_types = defaultdict(int)\n",
    "for req in failed_requests:\n",
    "    error_msg = req.get('error', 'Unknown error')\n",
    "    error_key = error_msg[:50] + \"...\" if len(error_msg) > 50 else error_msg\n",
    "    error_types[error_key] += 1\n",
    "\n",
    "for error, count in list(error_types.items())[:5]:\n",
    "    md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "# Save markdown\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary: {csv_filename}\")\n",
    "print(f\"📊 Details: {detailed_csv_filename}\")\n",
    "print(f\"📝 Report: {md_filename}\")\n",
    "\n",
    "if len(successful_requests) == 0:\n",
    "    print(f\"\\n🚨 CRITICAL: All requests failed. Check your endpoint!\")\n",
    "else:\n",
    "    print(f\"\\n📊 Partial results saved. Success rate: {len(successful_requests)/len(results)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using chat complitions API (adds some inputs ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_id = \"1029620071644790784\"\n",
    "# LOCATION=\"europe-west4\",\n",
    "\n",
    "# client_options = {\"api_endpoint\": api_endpoint}\n",
    "\n",
    "# client = aiplatform.gapic.PredictionServiceClient(\n",
    "#   client_options=client_options\n",
    "# )\n",
    "    \n",
    "# endpoint = client.endpoint_path(\n",
    "#   project=PROJECT_ID, location=LOCATION, endpoint=endpoint_id\n",
    "# )\n",
    "# response = client.predict(\n",
    "#   endpoint=endpoint, instances=instances, parameters=parameters\n",
    "# )\n",
    "# print(\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple types of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Test scenarios configuration\n",
    "TEST_SCENARIOS = {\n",
    "    'email_generation': {\n",
    "        'name': 'Email Generation',\n",
    "        'description': 'Short prompts generating long emails',\n",
    "        'concurrent_users': 100,\n",
    "        'total_requests': 2000,\n",
    "        'input_token_length': 80,\n",
    "        'output_tokens': 350,\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 400,\n",
    "        'raw_response': True  # Match your parameter structure\n",
    "    },\n",
    "    'summarization': {\n",
    "        'name': 'Text Summarization',\n",
    "        'description': 'Long text summarization to concise output',\n",
    "        'concurrent_users': 150,\n",
    "        'total_requests': 3000,\n",
    "        'input_token_length': 500,\n",
    "        'output_tokens': 80,\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 120,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'rewrite_small': {\n",
    "        'name': 'Content Rewriting (Small)',\n",
    "        'description': 'Small content rewriting and improvement',\n",
    "        'concurrent_users': 200,\n",
    "        'total_requests': 4000,\n",
    "        'input_token_length': 250,\n",
    "        'output_tokens': 250,\n",
    "        'temperature': 0.5,\n",
    "        'max_tokens': 300,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'rewrite_large': {\n",
    "        'name': 'Content Rewriting (Large)',\n",
    "        'description': 'Large document rewriting and enhancement',\n",
    "        'concurrent_users': 80,\n",
    "        'total_requests': 1500,\n",
    "        'input_token_length': 1000,\n",
    "        'output_tokens': 1000,\n",
    "        'temperature': 0.4,\n",
    "        'max_tokens': 1200,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'code_generation': {\n",
    "        'name': 'Code Generation',\n",
    "        'description': 'Code generation from specifications',\n",
    "        'concurrent_users': 120,\n",
    "        'total_requests': 2500,\n",
    "        'input_token_length': 200,\n",
    "        'output_tokens': 400,\n",
    "        'temperature': 0.2,\n",
    "        'max_tokens': 500,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'conversation': {\n",
    "        'name': 'Conversational AI',\n",
    "        'description': 'Multi-turn conversation simulation',\n",
    "        'concurrent_users': 180,\n",
    "        'total_requests': 5000,\n",
    "        'input_token_length': 150,\n",
    "        'output_tokens': 200,\n",
    "        'temperature': 0.8,\n",
    "        'max_tokens': 250,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'qa_long_context': {\n",
    "        'name': 'Long Context Q&A',\n",
    "        'description': 'Question answering with long context',\n",
    "        'concurrent_users': 60,\n",
    "        'total_requests': 1000,\n",
    "        'input_token_length': 1500,\n",
    "        'output_tokens': 300,\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 400,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'creative_writing': {\n",
    "        'name': 'Creative Writing',\n",
    "        'description': 'Story and creative content generation',\n",
    "        'concurrent_users': 100,\n",
    "        'total_requests': 2000,\n",
    "        'input_token_length': 120,\n",
    "        'output_tokens': 600,\n",
    "        'temperature': 0.9,\n",
    "        'max_tokens': 700,\n",
    "        'raw_response': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prompt generators for different scenarios\n",
    "def generate_email_prompt(target_tokens=80):\n",
    "    \"\"\"Generate email generation prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"Write a professional follow-up email to a client who hasn't responded to our proposal in 2 weeks. Include next steps and maintain a friendly tone.\",\n",
    "        \"Create a welcome email for new employees joining our tech startup. Include company culture, first-day instructions, and contact information.\",\n",
    "        \"Draft an apology email to customers about a service outage that lasted 3 hours. Explain what happened and our prevention measures.\",\n",
    "        \"Write a promotional email for our new AI software product launch. Highlight key features and include a special discount offer.\",\n",
    "        \"Create a meeting request email for quarterly business review with key stakeholders. Include agenda items and time options.\",\n",
    "        \"Draft a thank you email to conference speakers and sponsors after successful event completion.\",\n",
    "        \"Write a customer onboarding email explaining how to get started with our platform and available resources.\",\n",
    "        \"Create a re-engagement email for inactive users offering special incentives to return to our service.\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "def generate_summarization_prompt(target_tokens=500):\n",
    "    \"\"\"Generate summarization prompts with long content\"\"\"\n",
    "    base_content = \"\"\"\n",
    "    In today's rapidly evolving business landscape, companies are increasingly turning to artificial intelligence and machine learning technologies to gain competitive advantages and streamline their operations. The implementation of AI systems has become a critical factor in determining organizational success across various industries, from healthcare and finance to retail and manufacturing.\n",
    "\n",
    "    The adoption of AI technologies brings numerous benefits including improved efficiency, enhanced decision-making capabilities, cost reduction, and the ability to process vast amounts of data in real-time. However, organizations also face significant challenges when implementing these systems, including data privacy concerns, integration complexities, workforce adaptation requirements, and substantial initial investment costs.\n",
    "\n",
    "    Recent studies indicate that companies successfully implementing AI solutions report an average increase in productivity of 40% and cost savings of up to 30% within the first two years of deployment. These improvements are primarily attributed to automation of repetitive tasks, enhanced predictive analytics, and improved customer service through chatbots and virtual assistants.\n",
    "\n",
    "    The key to successful AI implementation lies in developing a comprehensive strategy that addresses technical requirements, organizational readiness, and change management processes. Companies must invest in employee training, establish clear governance frameworks, and ensure compliance with relevant regulations and ethical guidelines.\n",
    "\n",
    "    Looking ahead, the future of AI in business appears promising, with emerging technologies such as generative AI, computer vision, and natural language processing offering new opportunities for innovation and growth. Organizations that proactively embrace these technologies while addressing associated challenges will be better positioned to thrive in the digital economy.\n",
    "\n",
    "    The integration of AI systems also requires careful consideration of data quality, security measures, and ongoing maintenance requirements. Companies must establish robust data management practices, implement appropriate security protocols, and develop sustainable support structures to ensure long-term success of their AI initiatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extend content to reach target tokens\n",
    "    extended_content = base_content\n",
    "    while len(extended_content.split()) < target_tokens * 0.75:\n",
    "        extended_content += \" \" + base_content\n",
    "    \n",
    "    return f\"Please provide a concise summary of the following business analysis:\\n\\n{extended_content[:int(target_tokens*4)]}\\n\\nSummary:\"\n",
    "\n",
    "def generate_rewrite_prompt(target_tokens=250):\n",
    "    \"\"\"Generate content rewriting prompts\"\"\"\n",
    "    if target_tokens <= 300:\n",
    "        content = \"\"\"\n",
    "        Our company has been working on developing new software solutions for the past several years. We have created various applications that help businesses manage their operations more effectively. The software includes features for inventory management, customer relationship management, and financial reporting.\n",
    "\n",
    "        The development team consists of experienced programmers who use modern programming languages and frameworks. We follow agile development methodologies to ensure quick delivery of high-quality software products. Our testing procedures include both automated and manual testing to identify and fix any issues before deployment.\n",
    "\n",
    "        Customer feedback has been very positive, with many users praising the user-friendly interface and robust functionality. We continue to add new features based on user requests and market demands. Our support team provides excellent customer service to help users maximize the benefits of our software solutions.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        content = \"\"\"\n",
    "        Our organization has been dedicated to the development and deployment of innovative software solutions for the past decade, establishing ourselves as a leader in the enterprise software market. We have successfully created and launched a comprehensive suite of applications designed to help businesses across various industries manage their complex operations more effectively and efficiently.\n",
    "\n",
    "        The software portfolio includes advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is built with scalability in mind, allowing businesses to grow without worrying about system limitations.\n",
    "\n",
    "        Our development team consists of highly experienced software engineers, architects, and designers who utilize cutting-edge programming languages, frameworks, and development tools. We follow industry best practices including agile development methodologies, continuous integration, and deployment practices to ensure rapid delivery of high-quality software products that meet the evolving needs of our clients.\n",
    "\n",
    "        Our comprehensive testing procedures include automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing to identify and resolve any issues before deployment. We maintain strict quality assurance standards throughout the development lifecycle.\n",
    "\n",
    "        Customer feedback has been overwhelmingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set. We continuously gather user feedback and market intelligence to guide our product development roadmap.\n",
    "\n",
    "        We maintain a dedicated customer support team that provides exceptional service through multiple channels including phone, email, chat, and an extensive knowledge base. Our support team helps users maximize the benefits of our software solutions and ensures smooth implementation and adoption.\n",
    "        \"\"\"\n",
    "    \n",
    "    return f\"Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n{content}\\n\\nImproved version:\"\n",
    "\n",
    "def generate_code_prompt(target_tokens=200):\n",
    "    \"\"\"Generate code generation prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"Create a Python function that implements a binary search algorithm with error handling, type hints, and comprehensive documentation. Include unit tests.\",\n",
    "        \"Write a JavaScript React component for a responsive navigation bar with dropdown menus, mobile hamburger menu, and smooth animations.\",\n",
    "        \"Develop a SQL query to analyze customer purchase patterns including total spend, frequency, and product categories with performance optimization.\",\n",
    "        \"Create a Python class for managing database connections with connection pooling, error handling, and transaction management.\",\n",
    "        \"Write a REST API endpoint in Python Flask for user authentication with JWT tokens, rate limiting, and input validation.\",\n",
    "        \"Implement a sorting algorithm visualization in JavaScript with HTML5 Canvas showing step-by-step execution and performance metrics.\",\n",
    "        \"Create a data validation function in TypeScript for form inputs with custom error messages and real-time validation feedback.\",\n",
    "        \"Write a Python script for automated testing of API endpoints with comprehensive test cases and detailed reporting.\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "def generate_conversation_prompt(target_tokens=150):\n",
    "    \"\"\"Generate conversational prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"I'm planning a career change from marketing to data science. Can you help me understand the key skills I need to develop and create a learning roadmap?\",\n",
    "        \"I'm having trouble with my team's productivity. We're missing deadlines and communication seems poor. What strategies would you recommend?\",\n",
    "        \"I want to start a small business selling handmade crafts online. Can you guide me through the essential steps and considerations?\",\n",
    "        \"I'm preparing for a job interview for a senior management position. What questions should I expect and how should I prepare?\",\n",
    "        \"I need to improve my public speaking skills for upcoming presentations. Can you provide practical tips and practice exercises?\",\n",
    "        \"I'm considering investing in renewable energy stocks. What factors should I consider and what are the current market trends?\",\n",
    "        \"I want to learn a new programming language to advance my career. Which language would you recommend and why?\",\n",
    "        \"I'm struggling with work-life balance as a remote worker. Can you suggest strategies to maintain productivity and well-being?\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "def generate_qa_long_context_prompt(target_tokens=1500):\n",
    "    \"\"\"Generate Q&A prompts with long context\"\"\"\n",
    "    context = \"\"\"\n",
    "    The history of artificial intelligence dates back to ancient times, with myths and stories of artificial beings endowed with intelligence or consciousness by master craftsmen. The formal field of AI research was founded at a conference at Dartmouth College in 1956, where the term \"artificial intelligence\" was coined.\n",
    "\n",
    "    Early AI research focused on problem-solving and symbolic methods. In the 1960s, the US Department of Defense took interest in this type of work and began training computers to mimic basic human reasoning. This early work paved the way for the automation and formal reasoning that we see in computers today.\n",
    "\n",
    "    The field experienced several boom and bust cycles, known as \"AI winters,\" when funding and interest waned due to overinflated expectations and limited practical applications. However, the field has experienced a renaissance since the 2000s, driven by advances in machine learning, particularly deep learning, and the availability of large datasets and powerful computing resources.\n",
    "\n",
    "    Machine learning, a subset of AI, involves training algorithms on data to make predictions or decisions without being explicitly programmed for every scenario. Deep learning, a subset of machine learning, uses neural networks with multiple layers to model and understand complex patterns in data.\n",
    "\n",
    "    The current wave of AI advancement is characterized by breakthrough applications in computer vision, natural language processing, speech recognition, and game playing. Notable achievements include IBM's Deep Blue defeating world chess champion Garry Kasparov in 1997, IBM's Watson winning at Jeopardy! in 2011, and Google's AlphaGo defeating the world champion Go player in 2016.\n",
    "\n",
    "    Modern AI applications are ubiquitous in our daily lives, from recommendation systems on streaming platforms and e-commerce sites to virtual assistants like Siri and Alexa, autonomous vehicles, and medical diagnosis tools. The technology continues to advance rapidly, with new breakthroughs in generative AI, large language models, and multimodal AI systems.\n",
    "\n",
    "    The development of AI raises important ethical and societal questions about privacy, job displacement, bias in algorithms, and the concentration of power among tech companies. There are ongoing debates about AI governance, safety, and the need for regulation to ensure AI benefits humanity while minimizing risks.\n",
    "\n",
    "    Looking to the future, AI is expected to continue advancing rapidly, with potential developments in artificial general intelligence (AGI) that could match or exceed human intelligence across all domains. This prospect brings both tremendous opportunities and significant challenges that society must address.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extend context to reach target tokens\n",
    "    extended_context = context\n",
    "    while len(extended_context.split()) < target_tokens * 0.75:\n",
    "        extended_context += \" \" + context\n",
    "    \n",
    "    questions = [\n",
    "        \"Based on the provided context, what were the key factors that led to the AI winters, and how did the field recover?\",\n",
    "        \"Explain the relationship between artificial intelligence, machine learning, and deep learning as described in the context.\",\n",
    "        \"What are the major ethical and societal concerns raised by AI development according to the passage?\",\n",
    "        \"Describe the evolution of AI from its early days to modern applications, highlighting major milestones.\",\n",
    "        \"What role did computing power and data availability play in the recent AI renaissance?\"\n",
    "    ]\n",
    "    \n",
    "    question = questions[hash(str(target_tokens)) % len(questions)]\n",
    "    return f\"Context:\\n{extended_context[:int(target_tokens*4)]}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "def generate_creative_writing_prompt(target_tokens=120):\n",
    "    \"\"\"Generate creative writing prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"Write a short story about a time traveler who discovers that changing the past has unexpected consequences in the present.\",\n",
    "        \"Create a dramatic monologue from the perspective of the last tree in a deforested world speaking to humanity.\",\n",
    "        \"Write a humorous story about a superhero whose power is the ability to make anyone laugh uncontrollably at inappropriate times.\",\n",
    "        \"Compose a mystery story where the detective realizes they are actually the criminal they've been hunting.\",\n",
    "        \"Write a science fiction story about first contact between humans and an alien species that communicates through colors.\",\n",
    "        \"Create a fantasy tale about a young wizard who discovers their magic only works when they're telling the truth.\",\n",
    "        \"Write a story about a librarian who discovers that books in their library can transport readers into the stories.\",\n",
    "        \"Compose a thriller about a person who receives messages from their future self warning about upcoming dangers.\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "# Prompt generator mapping\n",
    "PROMPT_GENERATORS = {\n",
    "    'email_generation': generate_email_prompt,\n",
    "    'summarization': generate_summarization_prompt,\n",
    "    'rewrite_small': generate_rewrite_prompt,\n",
    "    'rewrite_large': generate_rewrite_prompt,\n",
    "    'code_generation': generate_code_prompt,\n",
    "    'conversation': generate_conversation_prompt,\n",
    "    'qa_long_context': generate_qa_long_context_prompt,\n",
    "    'creative_writing': generate_creative_writing_prompt\n",
    "}\n",
    "\n",
    "class TPUBenchmarkSuite:\n",
    "    def __init__(self, endpoint, use_dedicated_endpoint=True):\n",
    "        self.endpoint = endpoint  # Single endpoint object, not a dictionary\n",
    "        self.use_dedicated_endpoint = use_dedicated_endpoint\n",
    "        self.metrics = {}\n",
    "        self.metrics_lock = threading.Lock()\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset metrics for new test\"\"\"\n",
    "        self.metrics = {\n",
    "            'ttft_times': [],\n",
    "            'inter_token_latencies': [],\n",
    "            'end_to_end_times': [],\n",
    "            'input_tokens': [],\n",
    "            'output_tokens': [],\n",
    "            'request_errors': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "    \n",
    "    def make_request(self, request_id, prompt, config):\n",
    "        \"\"\"Single request function with detailed timing\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Prepare request exactly like your working code\n",
    "            instances = [{\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": config['max_tokens'],\n",
    "                \"temperature\": config['temperature'],\n",
    "                \"raw_response\": config.get('raw_response', True)  # Match your structure\n",
    "            }]\n",
    "            \n",
    "            # Debug: Print first few requests to see what's being sent\n",
    "            if request_id < 3:\n",
    "                print(f\"DEBUG Request {request_id}: {instances[0]}\")\n",
    "            \n",
    "            # Record request start\n",
    "            request_start = time.time()\n",
    "            \n",
    "            # Make prediction using your exact method\n",
    "            response = self.endpoint.predict(\n",
    "                instances=instances, \n",
    "                use_dedicated_endpoint=self.use_dedicated_endpoint\n",
    "            )\n",
    "            \n",
    "            request_end = time.time()\n",
    "            \n",
    "            # Debug: Print first few responses\n",
    "            if request_id < 3:\n",
    "                print(f\"DEBUG Response {request_id}: {response}\")\n",
    "                print(f\"DEBUG Predictions: {response.predictions if hasattr(response, 'predictions') else 'No predictions attr'}\")\n",
    "            \n",
    "            # Parse response like your code\n",
    "            if hasattr(response, 'predictions') and response.predictions:\n",
    "                # Handle both string and dict responses\n",
    "                prediction = response.predictions[0]\n",
    "                if isinstance(prediction, dict):\n",
    "                    output_text = prediction.get('generated_text', '') or prediction.get('output', '') or str(prediction)\n",
    "                else:\n",
    "                    output_text = str(prediction)\n",
    "            else:\n",
    "                output_text = \"\"\n",
    "                if request_id < 3:\n",
    "                    print(f\"DEBUG: No predictions in response for request {request_id}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            end_to_end_time = request_end - request_start\n",
    "            \n",
    "            # Estimate tokens (rough approximation - you can adjust this)\n",
    "            input_tokens = len(prompt.split()) * 1.3\n",
    "            output_tokens = len(output_text.split()) * 1.3\n",
    "            \n",
    "            # Simulate TTFT and inter-token timing (estimates)\n",
    "            estimated_ttft = min(0.5, end_to_end_time * 0.02)\n",
    "            estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens)\n",
    "            \n",
    "            # Store metrics\n",
    "            with self.metrics_lock:\n",
    "                self.metrics['ttft_times'].append(estimated_ttft)\n",
    "                self.metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "                self.metrics['end_to_end_times'].append(end_to_end_time)\n",
    "                self.metrics['input_tokens'].append(input_tokens)\n",
    "                self.metrics['output_tokens'].append(output_tokens)\n",
    "                self.metrics['timestamps'].append(request_start)\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'success': True,\n",
    "                'timestamp': request_start,\n",
    "                'end_to_end_time': end_to_end_time,\n",
    "                'ttft': estimated_ttft,\n",
    "                'inter_token_latency': estimated_inter_token,\n",
    "                'input_tokens': input_tokens,\n",
    "                'output_tokens': output_tokens,\n",
    "                'output_length': len(output_text),\n",
    "                'prompt_length': len(prompt),\n",
    "                'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text,\n",
    "                'raw_prediction': str(prediction)[:100] + \"...\" if len(str(prediction)) > 100 else str(prediction)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            \n",
    "            # Debug: Print first few errors\n",
    "            if request_id < 5:\n",
    "                print(f\"DEBUG Error {request_id}: {str(e)}\")\n",
    "                import traceback\n",
    "                print(f\"DEBUG Traceback: {traceback.format_exc()}\")\n",
    "            \n",
    "            with self.metrics_lock:\n",
    "                self.metrics['request_errors'].append({\n",
    "                    'request_id': request_id,\n",
    "                    'error': str(e),\n",
    "                    'time': error_time\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': start_time,\n",
    "                'error': str(e),\n",
    "                'time': error_time,\n",
    "                'end_to_end_time': error_time,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': len(prompt),\n",
    "                'output_text': \"\",\n",
    "                'raw_prediction': f\"Error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def run_scenario_test(self, scenario_name):\n",
    "        \"\"\"Run a specific test scenario\"\"\"\n",
    "        if scenario_name not in TEST_SCENARIOS:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario_name}\")\n",
    "        \n",
    "        config = TEST_SCENARIOS[scenario_name]\n",
    "        prompt_generator = PROMPT_GENERATORS[scenario_name]\n",
    "        \n",
    "        # Reset metrics\n",
    "        self.reset_metrics()\n",
    "        \n",
    "        # Create timestamped filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"tpu_benchmark_results_{scenario_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        csv_filename = f\"{output_dir}/{scenario_name}_summary_{timestamp}.csv\"\n",
    "        detailed_csv_filename = f\"{output_dir}/{scenario_name}_detailed_{timestamp}.csv\"\n",
    "        md_filename = f\"{output_dir}/{scenario_name}_report_{timestamp}.md\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🚀 STARTING TEST: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Description: {config['description']}\")\n",
    "        print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # Generate test prompts\n",
    "        print(f\"\\n📝 Generating test prompts...\")\n",
    "        test_prompts = []\n",
    "        for i in range(config['total_requests']):\n",
    "            prompt = prompt_generator(config['input_token_length'])\n",
    "            test_prompts.append(prompt)\n",
    "        \n",
    "        print(f\"✅ Generated {len(test_prompts)} test prompts\")\n",
    "        print(f\"📊 Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "        print(f\"🎯 Target input tokens: {config['input_token_length']}\")\n",
    "        print(f\"🎯 Target output tokens: {config['output_tokens']}\")\n",
    "        \n",
    "        # Run load test\n",
    "        print(f\"\\n🔥 Starting load test:\")\n",
    "        print(f\"   - Concurrent users: {config['concurrent_users']}\")\n",
    "        print(f\"   - Total requests: {config['total_requests']}\")\n",
    "        print(f\"   - Temperature: {config['temperature']}\")\n",
    "        print(f\"   - Max tokens: {config['max_tokens']}\")\n",
    "        \n",
    "        # Execute test\n",
    "        test_start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=config['concurrent_users']) as executor:\n",
    "            # Submit all requests\n",
    "            future_to_id = {\n",
    "                executor.submit(self.make_request, i, test_prompts[i % len(test_prompts)], config): i \n",
    "                for i in range(config['total_requests'])\n",
    "            }\n",
    "            \n",
    "            # Collect results with progress tracking\n",
    "            completed = 0\n",
    "            for future in as_completed(future_to_id):\n",
    "                request_id = future_to_id[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        'request_id': request_id,\n",
    "                        'success': False,\n",
    "                        'timestamp': time.time(),\n",
    "                        'error': str(e),\n",
    "                        'end_to_end_time': 0,\n",
    "                        'ttft': 0,\n",
    "                        'inter_token_latency': 0,\n",
    "                        'input_tokens': 0,\n",
    "                        'output_tokens': 0,\n",
    "                        'output_length': 0,\n",
    "                        'prompt_length': 0,\n",
    "                        'output_text': \"\"\n",
    "                    })\n",
    "                \n",
    "                completed += 1\n",
    "                if completed % 100 == 0:\n",
    "                    print(f\"   ⚡ Completed {completed}/{config['total_requests']} requests...\")\n",
    "        \n",
    "        test_end_time = time.time()\n",
    "        total_test_time = test_end_time - test_start_time\n",
    "        \n",
    "        # Calculate and display results\n",
    "        self.calculate_and_display_results(results, total_test_time, config, scenario_name)\n",
    "        \n",
    "        # Save results\n",
    "        self.save_results(results, config, scenario_name, timestamp, total_test_time,\n",
    "                         csv_filename, detailed_csv_filename, md_filename)\n",
    "        \n",
    "        print(f\"\\n✅ Test completed successfully!\")\n",
    "        print(f\"📁 Files saved in: {output_dir}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_and_display_results(self, results, total_test_time, config, scenario_name):\n",
    "        \"\"\"Calculate and display test results\"\"\"\n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        failed_requests = [r for r in results if not r.get('success', False)]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 TEST RESULTS: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\n📈 Test Summary:\")\n",
    "        print(f\"   - Total requests: {len(results)}\")\n",
    "        print(f\"   - Successful requests: {len(successful_requests)}\")\n",
    "        print(f\"   - Failed requests: {len(failed_requests)}\")\n",
    "        print(f\"   - Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "        print(f\"   - Total test time: {total_test_time:.1f} seconds\")\n",
    "        \n",
    "        if successful_requests:\n",
    "            # Calculate metrics\n",
    "            ttft_times = [r['ttft'] for r in successful_requests]\n",
    "            inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "            e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "            input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "            output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "            \n",
    "            def percentile(data, p):\n",
    "                return np.percentile(data, p)\n",
    "            \n",
    "            # Latency metrics\n",
    "            ttft_p50 = percentile(ttft_times, 50)\n",
    "            ttft_p95 = percentile(ttft_times, 95)\n",
    "            ttft_p99 = percentile(ttft_times, 99)\n",
    "            inter_token_p50 = percentile(inter_token_times, 50)\n",
    "            inter_token_p95 = percentile(inter_token_times, 95)\n",
    "            e2e_p50 = percentile(e2e_times, 50)\n",
    "            e2e_p95 = percentile(e2e_times, 95)\n",
    "            e2e_p99 = percentile(e2e_times, 99)\n",
    "            \n",
    "            # Throughput calculations\n",
    "            total_output_tokens = sum(output_tokens)\n",
    "            total_input_tokens = sum(input_tokens)\n",
    "            total_tokens = total_output_tokens + total_input_tokens\n",
    "            \n",
    "            token_output_throughput = total_output_tokens / total_test_time\n",
    "            overall_token_throughput = total_tokens / total_test_time\n",
    "            requests_per_second = len(successful_requests) / total_test_time\n",
    "            \n",
    "            print(f\"\\n⚡ Latency Metrics:\")\n",
    "            print(f\"   - TTFT (p50): {ttft_p50:.3f}s\")\n",
    "            print(f\"   - TTFT (p95): {ttft_p95:.3f}s\")\n",
    "            print(f\"   - TTFT (p99): {ttft_p99:.3f}s\")\n",
    "            print(f\"   - Inter-token (p50): {inter_token_p50:.3f}s\")\n",
    "            print(f\"   - Inter-token (p95): {inter_token_p95:.3f}s\")\n",
    "            print(f\"   - End-to-End (p50): {e2e_p50:.1f}s\")\n",
    "            print(f\"   - End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "            print(f\"   - End-to-End (p99): {e2e_p99:.1f}s\")\n",
    "            \n",
    "            print(f\"\\n🚀 Throughput Metrics:\")\n",
    "            print(f\"   - Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "            print(f\"   - Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "            print(f\"   - Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "            \n",
    "            print(f\"\\n📊 Token Statistics:\")\n",
    "            print(f\"   - Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "            print(f\"   - Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "            print(f\"   - Total input tokens: {int(total_input_tokens):,}\")\n",
    "            print(f\"   - Total output tokens: {int(total_output_tokens):,}\")\n",
    "    \n",
    "    def save_results(self, results, config, scenario_name, timestamp, total_test_time,\n",
    "                    csv_filename, detailed_csv_filename, md_filename):\n",
    "        \"\"\"Save test results to files\"\"\"\n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        failed_requests = [r for r in results if not r.get('success', False)]\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(detailed_csv_filename, index=False)\n",
    "        \n",
    "        # Create summary data\n",
    "        summary_data = {\n",
    "            'scenario': [scenario_name],\n",
    "            'timestamp': [timestamp],\n",
    "            'test_duration_seconds': [total_test_time],\n",
    "            'total_requests': [len(results)],\n",
    "            'successful_requests': [len(successful_requests)],\n",
    "            'failed_requests': [len(failed_requests)],\n",
    "            'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "            'concurrent_users': [config['concurrent_users']],\n",
    "            'target_input_tokens': [config['input_token_length']],\n",
    "            'target_output_tokens': [config['output_tokens']],\n",
    "            'temperature': [config['temperature']],\n",
    "            'max_tokens': [config['max_tokens']]\n",
    "        }\n",
    "        \n",
    "        if successful_requests:\n",
    "            # Calculate metrics for summary\n",
    "            ttft_times = [r['ttft'] for r in successful_requests]\n",
    "            inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "            e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "            input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "            output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "            \n",
    "            summary_data.update({\n",
    "                'ttft_p50_seconds': [np.percentile(ttft_times, 50)],\n",
    "                'ttft_p95_seconds': [np.percentile(ttft_times, 95)],\n",
    "                'ttft_p99_seconds': [np.percentile(ttft_times, 99)],\n",
    "                'inter_token_p50_seconds': [np.percentile(inter_token_times, 50)],\n",
    "                'inter_token_p95_seconds': [np.percentile(inter_token_times, 95)],\n",
    "                'e2e_p50_seconds': [np.percentile(e2e_times, 50)],\n",
    "                'e2e_p95_seconds': [np.percentile(e2e_times, 95)],\n",
    "                'e2e_p99_seconds': [np.percentile(e2e_times, 99)],\n",
    "                'token_output_throughput': [sum(output_tokens) / total_test_time],\n",
    "                'overall_token_throughput': [(sum(output_tokens) + sum(input_tokens)) / total_test_time],\n",
    "                'requests_per_second': [len(successful_requests) / total_test_time],\n",
    "                'avg_input_tokens': [statistics.mean(input_tokens)],\n",
    "                'avg_output_tokens': [statistics.mean(output_tokens)],\n",
    "                'total_input_tokens': [sum(input_tokens)],\n",
    "                'total_output_tokens': [sum(output_tokens)]\n",
    "            })\n",
    "        else:\n",
    "            # Fill with zeros if no successful requests\n",
    "            for key in ['ttft_p50_seconds', 'ttft_p95_seconds', 'ttft_p99_seconds', \n",
    "                        'inter_token_p50_seconds', 'inter_token_p95_seconds',\n",
    "                        'e2e_p50_seconds', 'e2e_p95_seconds', 'e2e_p99_seconds',\n",
    "                        'token_output_throughput', 'overall_token_throughput', \n",
    "                        'requests_per_second', 'avg_input_tokens', 'avg_output_tokens',\n",
    "                        'total_input_tokens', 'total_output_tokens']:\n",
    "                summary_data[key] = [0]\n",
    "        \n",
    "        # Save summary CSV\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        # Generate markdown report\n",
    "        self.generate_markdown_report(results, config, scenario_name, timestamp, \n",
    "                                    total_test_time, md_filename)\n",
    "    \n",
    "    def generate_markdown_report(self, results, config, scenario_name, timestamp, \n",
    "                               total_test_time, md_filename):\n",
    "        \"\"\"Generate detailed markdown report\"\"\"\n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        failed_requests = [r for r in results if not r.get('success', False)]\n",
    "        \n",
    "        md_content = f\"\"\"# TPU Benchmark Report: {config['name']}\n",
    "\n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Scenario:** {scenario_name}  \n",
    "**Test Duration:** {total_test_time:.1f} seconds  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## 📋 Test Scenario Description\n",
    "\n",
    "{config['description']}\n",
    "\n",
    "## ⚙️ Test Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Scenario | {config['name']} |\n",
    "| Concurrent Users | {config['concurrent_users']} |\n",
    "| Total Requests | {config['total_requests']} |\n",
    "| Target Input Tokens | {config['input_token_length']} |\n",
    "| Target Output Tokens | {config['output_tokens']} |\n",
    "| Temperature | {config['temperature']} |\n",
    "| Max Tokens | {config['max_tokens']} |\n",
    "| Raw Response | {config['raw_response']} |\n",
    "\n",
    "## 📊 Test Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests | {len(results)} |\n",
    "| Successful Requests | {len(successful_requests)} |\n",
    "| Failed Requests | {len(failed_requests)} |\n",
    "| Success Rate | {len(successful_requests)/len(results)*100:.1f}% |\n",
    "| Test Duration | {total_test_time:.1f} seconds |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        if successful_requests:\n",
    "            # Calculate detailed metrics\n",
    "            ttft_times = [r['ttft'] for r in successful_requests]\n",
    "            inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "            e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "            input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "            output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "            \n",
    "            ttft_p50 = np.percentile(ttft_times, 50)\n",
    "            ttft_p95 = np.percentile(ttft_times, 95)\n",
    "            ttft_p99 = np.percentile(ttft_times, 99)\n",
    "            inter_token_p50 = np.percentile(inter_token_times, 50)\n",
    "            inter_token_p95 = np.percentile(inter_token_times, 95)\n",
    "            e2e_p50 = np.percentile(e2e_times, 50)\n",
    "            e2e_p95 = np.percentile(e2e_times, 95)\n",
    "            e2e_p99 = np.percentile(e2e_times, 99)\n",
    "            \n",
    "            total_output_tokens = sum(output_tokens)\n",
    "            total_input_tokens = sum(input_tokens)\n",
    "            token_output_throughput = total_output_tokens / total_test_time\n",
    "            overall_token_throughput = (total_output_tokens + total_input_tokens) / total_test_time\n",
    "            requests_per_second = len(successful_requests) / total_test_time\n",
    "            \n",
    "            md_content += f\"\"\"\n",
    "## ⚡ Latency Metrics\n",
    "\n",
    "| Metric | p50 | p95 | p99 |\n",
    "|--------|-----|-----|-----|\n",
    "| Time to First Token (TTFT) | {ttft_p50:.3f}s | {ttft_p95:.3f}s | {ttft_p99:.3f}s |\n",
    "| Inter-token Latency | {inter_token_p50:.3f}s | {inter_token_p95:.3f}s | - |\n",
    "| End-to-End Latency | {e2e_p50:.1f}s | {e2e_p95:.1f}s | {e2e_p99:.1f}s |\n",
    "\n",
    "## 🚀 Throughput Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Overall Token Throughput | {overall_token_throughput:.2f} tok/sec |\n",
    "| Requests per Second | {requests_per_second:.2f} req/sec |\n",
    "\n",
    "## 📊 Token Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Average Input Tokens | {statistics.mean(input_tokens):.1f} |\n",
    "| Average Output Tokens | {statistics.mean(output_tokens):.1f} |\n",
    "| Total Input Tokens | {int(total_input_tokens):,} |\n",
    "| Total Output Tokens | {int(total_output_tokens):,} |\n",
    "| Input/Output Ratio | {statistics.mean(input_tokens)/statistics.mean(output_tokens):.2f} |\n",
    "\n",
    "## 🎯 Target vs Actual Comparison\n",
    "\n",
    "| Metric | Target | Actual | Difference |\n",
    "|--------|--------|--------|------------|\n",
    "| Input Token Length | {config['input_token_length']} | {statistics.mean(input_tokens):.1f} | {((statistics.mean(input_tokens) - config['input_token_length']) / config['input_token_length'] * 100):+.1f}% |\n",
    "| Output Tokens | {config['output_tokens']} | {statistics.mean(output_tokens):.1f} | {((statistics.mean(output_tokens) - config['output_tokens']) / config['output_tokens'] * 100):+.1f}% |\n",
    "\n",
    "## 📈 Performance Analysis\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            # Performance analysis based on scenario\n",
    "            if scenario_name == 'email_generation':\n",
    "                md_content += f\"\"\"\n",
    "### Email Generation Performance Analysis\n",
    "\n",
    "This scenario tests the model's ability to generate long-form emails from short prompts, simulating common business use cases.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Efficiency Ratio:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.1f}:1 (output:input tokens)\n",
    "- **Content Generation Speed:** {token_output_throughput:.1f} tokens/sec for email content\n",
    "- **Practical Throughput:** Can generate ~{requests_per_second*60:.0f} emails per minute\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if token_output_throughput > 50 else \"⚠️ Moderate\" if token_output_throughput > 20 else \"❌ Poor\"} for real-time email generation\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'summarization':\n",
    "                md_content += f\"\"\"\n",
    "### Summarization Performance Analysis\n",
    "\n",
    "This scenario tests the model's ability to condense large amounts of text into concise summaries.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Compression Ratio:** {statistics.mean(input_tokens)/statistics.mean(output_tokens):.1f}:1 (input:output tokens)\n",
    "- **Processing Speed:** {overall_token_throughput:.1f} total tokens/sec\n",
    "- **Summarization Efficiency:** {requests_per_second:.1f} documents/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if requests_per_second > 2 else \"⚠️ Moderate\" if requests_per_second > 1 else \"❌ Poor\"} for batch document processing\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'rewrite_small':\n",
    "                md_content += f\"\"\"\n",
    "### Small Content Rewriting Performance Analysis\n",
    "\n",
    "This scenario tests balanced input/output rewriting for content improvement.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Balanced Processing:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.2f}:1 ratio\n",
    "- **Rewriting Speed:** {token_output_throughput:.1f} tokens/sec output generation\n",
    "- **Content Throughput:** {requests_per_second:.1f} documents/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if requests_per_second > 1.5 else \"⚠️ Moderate\" if requests_per_second > 0.8 else \"❌ Poor\"} for content editing workflows\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'rewrite_large':\n",
    "                md_content += f\"\"\"\n",
    "### Large Content Rewriting Performance Analysis\n",
    "\n",
    "This scenario tests the model's ability to handle substantial document rewriting tasks.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Large Document Handling:** {statistics.mean(input_tokens):.0f} avg input tokens processed\n",
    "- **Comprehensive Rewriting:** {statistics.mean(output_tokens):.0f} avg output tokens generated\n",
    "- **Processing Efficiency:** {overall_token_throughput:.1f} total tokens/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if overall_token_throughput > 800 else \"⚠️ Moderate\" if overall_token_throughput > 400 else \"❌ Poor\"} for enterprise document processing\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'code_generation':\n",
    "                md_content += f\"\"\"\n",
    "### Code Generation Performance Analysis\n",
    "\n",
    "This scenario tests programming assistance and code generation capabilities.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Code Generation Ratio:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.1f}:1\n",
    "- **Code Output Speed:** {token_output_throughput:.1f} tokens/sec\n",
    "- **Developer Assistance Rate:** {requests_per_second:.1f} code requests/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if token_output_throughput > 40 else \"⚠️ Moderate\" if token_output_throughput > 20 else \"❌ Poor\"} for IDE integration\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'conversation':\n",
    "                md_content += f\"\"\"\n",
    "### Conversational AI Performance Analysis\n",
    "\n",
    "This scenario tests interactive conversation capabilities for chatbot applications.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Response Generation:** {statistics.mean(output_tokens):.0f} avg tokens per response\n",
    "- **Conversation Speed:** {token_output_throughput:.1f} tokens/sec\n",
    "- **User Interaction Rate:** {requests_per_second:.1f} conversations/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if requests_per_second > 3 else \"⚠️ Moderate\" if requests_per_second > 1.5 else \"❌ Poor\"} for real-time chat applications\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'qa_long_context':\n",
    "                md_content += f\"\"\"\n",
    "### Long Context Q&A Performance Analysis\n",
    "\n",
    "This scenario tests question answering with extensive context processing.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Context Processing:** {statistics.mean(input_tokens):.0f} avg context tokens\n",
    "- **Answer Generation:** {statistics.mean(output_tokens):.0f} avg answer tokens\n",
    "- **Knowledge Processing:** {overall_token_throughput:.1f} total tokens/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if overall_token_throughput > 600 else \"⚠️ Moderate\" if overall_token_throughput > 300 else \"❌ Poor\"} for knowledge base applications\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'creative_writing':\n",
    "                md_content += f\"\"\"\n",
    "### Creative Writing Performance Analysis\n",
    "\n",
    "This scenario tests creative content generation for storytelling and narrative creation.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Creative Expansion:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.1f}:1 expansion ratio\n",
    "- **Story Generation Speed:** {token_output_throughput:.1f} tokens/sec\n",
    "- **Creative Throughput:** {requests_per_second:.1f} stories/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if token_output_throughput > 60 else \"⚠️ Moderate\" if token_output_throughput > 30 else \"❌ Poor\"} for content creation platforms\n",
    "\"\"\"\n",
    "\n",
    "        # Error analysis\n",
    "        if failed_requests:\n",
    "            md_content += f\"\"\"\n",
    "## ❌ Error Analysis\n",
    "\n",
    "**Total Failed Requests:** {len(failed_requests)}\n",
    "\n",
    "\"\"\"\n",
    "            error_types = defaultdict(int)\n",
    "            for req in failed_requests:\n",
    "                error_msg = req.get('error', 'Unknown error')\n",
    "                error_types[error_msg] += 1\n",
    "            \n",
    "            md_content += \"| Error Type | Count | Percentage |\\n|------------|-------|------------|\\n\"\n",
    "            for error, count in error_types.items():\n",
    "                percentage = count / len(results) * 100\n",
    "                md_content += f\"| {error} | {count} | {percentage:.1f}% |\\n\"\n",
    "\n",
    "        md_content += f\"\"\"\n",
    "\n",
    "## 🔧 Technical Environment\n",
    "\n",
    "- **Model Hosting:** TPU-based vLLM deployment\n",
    "- **Test Framework:** Multi-scenario benchmarking suite\n",
    "- **Concurrency Model:** ThreadPoolExecutor\n",
    "- **Metrics Collection:** Real-time latency and throughput tracking\n",
    "\n",
    "## 📁 Generated Files\n",
    "\n",
    "- **Summary CSV:** `{os.path.basename(csv_filename)}`\n",
    "- **Detailed CSV:** `{os.path.basename(detailed_csv_filename)}`\n",
    "- **This Report:** `{os.path.basename(md_filename)}`\n",
    "\n",
    "---\n",
    "*Report generated automatically by TPU Benchmark Suite v2.0*\n",
    "*Test scenario: {scenario_name} | Timestamp: {timestamp}*\n",
    "\"\"\"\n",
    "\n",
    "        # Save markdown report\n",
    "        with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_content)\n",
    "\n",
    "def run_all_scenarios(endpoint, use_dedicated_endpoint=True, scenarios=None):\n",
    "    \"\"\"Run all test scenarios or specified scenarios\"\"\"\n",
    "    if scenarios is None:\n",
    "        scenarios = list(TEST_SCENARIOS.keys())\n",
    "    \n",
    "    suite = TPUBenchmarkSuite(endpoint, use_dedicated_endpoint)\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"\\n🚀 STARTING COMPREHENSIVE TPU BENCHMARK SUITE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Scenarios to run: {', '.join(scenarios)}\")\n",
    "    print(f\"Total scenarios: {len(scenarios)}\")\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"\\n🔄 Running scenario {i}/{len(scenarios)}: {scenario}\")\n",
    "        try:\n",
    "            results = suite.run_scenario_test(scenario)\n",
    "            all_results[scenario] = results\n",
    "            print(f\"✅ Scenario {scenario} completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Scenario {scenario} failed: {e}\")\n",
    "            all_results[scenario] = None\n",
    "    \n",
    "    # Generate comparative report\n",
    "    generate_comparative_report(all_results)\n",
    "    \n",
    "    print(f\"\\n🎉 ALL BENCHMARKS COMPLETED!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_comparative_report(all_results):\n",
    "    \"\"\"Generate a comparative report across all scenarios\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"tpu_benchmark_comparative\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    comparative_csv = f\"{output_dir}/comparative_summary_{timestamp}.csv\"\n",
    "    comparative_md = f\"{output_dir}/comparative_report_{timestamp}.md\"\n",
    "    \n",
    "    # Collect summary data from all scenarios\n",
    "    comparative_data = []\n",
    "    \n",
    "    for scenario_name, results in all_results.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "            \n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        if not successful_requests:\n",
    "            continue\n",
    "            \n",
    "        config = TEST_SCENARIOS[scenario_name]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ttft_times = [r['ttft'] for r in successful_requests]\n",
    "        inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "        e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "        input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "        output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "        \n",
    "        total_test_time = max([r['timestamp'] for r in results]) - min([r['timestamp'] for r in results])\n",
    "        if total_test_time == 0:\n",
    "            total_test_time = 1  # Avoid division by zero\n",
    "        \n",
    "        comparative_data.append({\n",
    "            'scenario': scenario_name,\n",
    "            'scenario_name': config['name'],\n",
    "            'description': config['description'],\n",
    "            'concurrent_users': config['concurrent_users'],\n",
    "            'total_requests': len(results),\n",
    "            'successful_requests': len(successful_requests),\n",
    "            'success_rate': len(successful_requests) / len(results) * 100,\n",
    "            'ttft_p95': np.percentile(ttft_times, 95),\n",
    "            'inter_token_p95': np.percentile(inter_token_times, 95),\n",
    "            'e2e_p95': np.percentile(e2e_times, 95),\n",
    "            'token_output_throughput': sum(output_tokens) / total_test_time,\n",
    "            'overall_token_throughput': (sum(output_tokens) + sum(input_tokens)) / total_test_time,\n",
    "            'requests_per_second': len(successful_requests) / total_test_time,\n",
    "            'avg_input_tokens': statistics.mean(input_tokens),\n",
    "            'avg_output_tokens': statistics.mean(output_tokens),\n",
    "            'target_input_tokens': config['input_token_length'],\n",
    "            'target_output_tokens': config['output_tokens']\n",
    "        })\n",
    "    \n",
    "    # Save comparative CSV\n",
    "    if comparative_data:\n",
    "        comp_df = pd.DataFrame(comparative_data)\n",
    "        comp_df.to_csv(comparative_csv, index=False)\n",
    "        \n",
    "        # Generate comparative markdown report\n",
    "        md_content = f\"\"\"# TPU Benchmark Comparative Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Scenarios Tested:** {len(comparative_data)}  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## 🎯 Executive Summary\n",
    "\n",
    "This report compares performance across different use case scenarios for TPU-hosted vLLM deployment.\n",
    "\n",
    "## 📊 Scenario Comparison Overview\n",
    "\n",
    "| Scenario | Use Case | Success Rate | Throughput (tok/sec) | TTFT p95 (s) | Requests/sec |\n",
    "|----------|----------|--------------|---------------------|---------------|--------------|\n",
    "\"\"\"\n",
    "        \n",
    "        for data in comparative_data:\n",
    "            md_content += f\"| {data['scenario_name']} | {data['description'][:30]}... | {data['success_rate']:.1f}% | {data['token_output_throughput']:.1f} | {data['ttft_p95']:.3f} | {data['requests_per_second']:.1f} |\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "## 🏆 Performance Rankings\n",
    "\n",
    "### Best Token Output Throughput\n",
    "\"\"\"\n",
    "        sorted_by_throughput = sorted(comparative_data, key=lambda x: x['token_output_throughput'], reverse=True)\n",
    "        for i, data in enumerate(sorted_by_throughput[:3], 1):\n",
    "            md_content += f\"{i}. **{data['scenario_name']}**: {data['token_output_throughput']:.1f} tok/sec\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "### Lowest Latency (TTFT p95)\n",
    "\"\"\"\n",
    "        sorted_by_ttft = sorted(comparative_data, key=lambda x: x['ttft_p95'])\n",
    "        for i, data in enumerate(sorted_by_ttft[:3], 1):\n",
    "            md_content += f\"{i}. **{data['scenario_name']}**: {data['ttft_p95']:.3f}s\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "### Highest Request Rate\n",
    "\"\"\"\n",
    "        sorted_by_rps = sorted(comparative_data, key=lambda x: x['requests_per_second'], reverse=True)\n",
    "        for i, data in enumerate(sorted_by_rps[:3], 1):\n",
    "            md_content += f\"{i}. **{data['scenario_name']}**: {data['requests_per_second']:.1f} req/sec\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "## 📈 Detailed Analysis by Use Case\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for data in comparative_data:\n",
    "            efficiency_ratio = data['avg_output_tokens'] / data['avg_input_tokens']\n",
    "            md_content += f\"\"\"\n",
    "### {data['scenario_name']}\n",
    "\n",
    "**Description:** {data['description']}\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Success Rate | {data['success_rate']:.1f}% |\n",
    "| Token Output Throughput | {data['token_output_throughput']:.1f} tok/sec |\n",
    "| Overall Token Throughput | {data['overall_token_throughput']:.1f} tok/sec |\n",
    "| Requests per Second | {data['requests_per_second']:.1f} req/sec |\n",
    "| TTFT p95 | {data['ttft_p95']:.3f}s |\n",
    "| Inter-token Latency p95 | {data['inter_token_p95']:.3f}s |\n",
    "| End-to-End p95 | {data['e2e_p95']:.1f}s |\n",
    "| Average Input Tokens | {data['avg_input_tokens']:.0f} |\n",
    "| Average Output Tokens | {data['avg_output_tokens']:.0f} |\n",
    "| Efficiency Ratio | {efficiency_ratio:.2f}:1 |\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "## 🎯 Recommendations\n",
    "\n",
    "### Production Deployment Suitability\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for data in comparative_data:\n",
    "            suitability = \"🟢 Excellent\" if data['success_rate'] > 95 and data['token_output_throughput'] > 30 else \"🟡 Good\" if data['success_rate'] > 90 and data['token_output_throughput'] > 15 else \"🔴 Needs Optimization\"\n",
    "            md_content += f\"- **{data['scenario_name']}**: {suitability}\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "### Optimization Priorities\n",
    "\n",
    "1. **Latency Optimization**: Focus on scenarios with TTFT p95 > 1.0s\n",
    "2. **Throughput Scaling**: Improve scenarios with < 20 tok/sec output throughput\n",
    "3. **Reliability**: Address scenarios with < 95% success rate\n",
    "\n",
    "## 📁 Files Generated\n",
    "\n",
    "- **Comparative CSV**: `{os.path.basename(comparative_csv)}`\n",
    "- **This Report**: `{os.path.basename(comparative_md)}`\n",
    "\n",
    "---\n",
    "*Comparative analysis generated by TPU Benchmark Suite v2.0*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(comparative_md, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_content)\n",
    "        \n",
    "        print(f\"\\n📊 Comparative report generated:\")\n",
    "        print(f\"   - CSV: {comparative_csv}\")\n",
    "        print(f\"   - Report: {comparative_md}\")\n",
    "\n",
    "def list_available_scenarios():\n",
    "    \"\"\"List all available test scenarios\"\"\"\n",
    "    print(\"\\n🎯 Available Test Scenarios:\")\n",
    "    print(\"=\"*60)\n",
    "    for key, config in TEST_SCENARIOS.items():\n",
    "        print(f\"📌 {key}: {config['name']}\")\n",
    "        print(f\"   Description: {config['description']}\")\n",
    "        print(f\"   Input: {config['input_token_length']} tokens → Output: {config['output_tokens']} tokens\")\n",
    "        print(f\"   Concurrency: {config['concurrent_users']} users, Requests: {config['total_requests']}\")\n",
    "        print()\n",
    "\n",
    "def test_single_request(endpoint, use_dedicated_endpoint):\n",
    "    \"\"\"Test a single request to verify endpoint compatibility\"\"\"\n",
    "    print(\"🔍 Testing single request with your endpoint...\")\n",
    "    \n",
    "    # Test with a simple prompt like your working example\n",
    "    test_prompt = \"What is a car that can run on the wall?\"\n",
    "    \n",
    "    instances = [{\n",
    "        \"prompt\": test_prompt,\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"raw_response\": True\n",
    "    }]\n",
    "    \n",
    "    try:\n",
    "        print(f\"📤 Sending request: {instances[0]}\")\n",
    "        \n",
    "        response = endpoint.predict(\n",
    "            instances=instances, \n",
    "            use_dedicated_endpoint=use_dedicated_endpoint\n",
    "        )\n",
    "        \n",
    "        print(f\"📥 Response type: {type(response)}\")\n",
    "        print(f\"📥 Response attributes: {dir(response)}\")\n",
    "        \n",
    "        if hasattr(response, 'predictions'):\n",
    "            print(f\"📥 Predictions: {response.predictions}\")\n",
    "            if response.predictions:\n",
    "                prediction = response.predictions[0]\n",
    "                print(f\"📥 First prediction type: {type(prediction)}\")\n",
    "                print(f\"📥 First prediction: {prediction}\")\n",
    "                return True\n",
    "        else:\n",
    "            print(\"❌ No 'predictions' attribute found\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        print(f\"❌ Traceback: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def run_benchmark_example():\n",
    "    \"\"\"Example function showing how to use the benchmark suite\"\"\"\n",
    "    print(\"🚀 TPU vLLM Multi-Scenario Benchmark Suite\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n📋 To use this benchmark suite, follow these steps:\")\n",
    "    print(\"\\n1. First, test your endpoint compatibility:\")\n",
    "    print(\"   test_single_request(endpoint, use_dedicated_endpoint)\")\n",
    "    print(\"\\n2. If that works, initialize the benchmark suite:\")\n",
    "    print(\"   suite = TPUBenchmarkSuite(endpoint, use_dedicated_endpoint)\")\n",
    "    print(\"\\n3. Run individual scenarios:\")\n",
    "    print(\"   results = suite.run_scenario_test('email_generation')\")\n",
    "    print(\"   results = suite.run_scenario_test('summarization')\")\n",
    "    print(\"   results = suite.run_scenario_test('rewrite_small')\")\n",
    "    print(\"   results = suite.run_scenario_test('rewrite_large')\")\n",
    "    print(\"\\n4. Or run all scenarios at once:\")\n",
    "    print(\"   all_results = run_all_scenarios(endpoint, use_dedicated_endpoint)\")\n",
    "    print(\"\\n5. Or run specific scenarios:\")\n",
    "    print(\"   selected = run_all_scenarios(endpoint, use_dedicated_endpoint,\")\n",
    "    print(\"                              scenarios=['email_generation', 'summarization'])\")\n",
    "    print(\"\\n💡 Available scenarios:\")\n",
    "    list_available_scenarios()\n",
    "\n",
    "# Main execution - notebook friendly\n",
    "def main():\n",
    "    \"\"\"Main function - notebook friendly version\"\"\"\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If we're in a notebook, just show the example\n",
    "        run_benchmark_example()\n",
    "    except NameError:\n",
    "        # We're in a regular Python script, use argparse\n",
    "        import sys\n",
    "        parser = argparse.ArgumentParser(description='TPU vLLM Multi-Scenario Benchmark Suite')\n",
    "        parser.add_argument('--scenarios', nargs='+', choices=list(TEST_SCENARIOS.keys()) + ['all'], \n",
    "                           default=['all'], help='Scenarios to run')\n",
    "        parser.add_argument('--list-scenarios', action='store_true', help='List available scenarios')\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        if args.list_scenarios:\n",
    "            list_available_scenarios()\n",
    "            return\n",
    "        \n",
    "        run_benchmark_example()\n",
    "\n",
    "# Auto-run example when imported in notebook\n",
    "try:\n",
    "    get_ipython()\n",
    "    print(\"📚 TPU Benchmark Suite loaded successfully!\")\n",
    "    print(\"💡 Run list_available_scenarios() to see all test scenarios\")\n",
    "    print(\"🚀 Run run_benchmark_example() to see usage instructions\")\n",
    "except NameError:\n",
    "    # Not in notebook\n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your existing working setup\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "# use_dedicated_endpoint is already defined in your environment\n",
    "\n",
    "# Test the endpoint first\n",
    "test_single_request(endpoint, use_dedicated_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the benchmark suite with your endpoint\n",
    "suite = TPUBenchmarkSuite(endpoint, use_dedicated_endpoint)\n",
    "\n",
    "# Test your specific use cases\n",
    "email_results = suite.run_scenario_test('email_generation')      # 80→350 tokens\n",
    "summary_results = suite.run_scenario_test('summarization')       # 500→80 tokens  \n",
    "small_rewrite = suite.run_scenario_test('rewrite_small')         # 250→250 tokens\n",
    "large_rewrite = suite.run_scenario_test('rewrite_large')         # 1000→1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scenarios = ['email_generation', 'summarization', 'rewrite_small', 'rewrite_large']\n",
    "all_results = run_all_scenarios(endpoint, use_dedicated_endpoint, scenarios=my_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLLM testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM-Style Benchmark Suite for TPU Endpoints\n",
    "Adapted from vLLM's official benchmark_serving.py for Google Cloud AI Platform endpoints\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, List, Dict, Union, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset classes adapted from vLLM\n",
    "@dataclass\n",
    "class BenchmarkRequest:\n",
    "    \"\"\"Request data structure for benchmarking\"\"\"\n",
    "    prompt: str\n",
    "    prompt_len: int\n",
    "    expected_output_len: int\n",
    "    request_id: int\n",
    "\n",
    "class RandomDataset:\n",
    "    \"\"\"Generate random prompts for benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int, range_ratio: float = 0.0):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "        self.range_ratio = range_ratio\n",
    "        \n",
    "    def _generate_random_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate a random prompt of specified length\"\"\"\n",
    "        # Create realistic text patterns\n",
    "        words = [\n",
    "            \"analyze\", \"consider\", \"evaluate\", \"examine\", \"investigate\", \"review\", \"assess\", \"study\",\n",
    "            \"business\", \"technology\", \"strategy\", \"development\", \"innovation\", \"implementation\", \"solution\",\n",
    "            \"market\", \"customer\", \"product\", \"service\", \"quality\", \"performance\", \"efficiency\", \"growth\",\n",
    "            \"data\", \"information\", \"process\", \"system\", \"method\", \"approach\", \"framework\", \"model\",\n",
    "            \"challenge\", \"opportunity\", \"risk\", \"benefit\", \"advantage\", \"improvement\", \"optimization\",\n",
    "            \"research\", \"analysis\", \"report\", \"recommendation\", \"conclusion\", \"insight\", \"finding\"\n",
    "        ]\n",
    "        \n",
    "        prompt_words = []\n",
    "        target_words = int(length * 0.75)  # Rough token-to-word conversion\n",
    "        \n",
    "        while len(prompt_words) < target_words:\n",
    "            prompt_words.append(random.choice(words))\n",
    "        \n",
    "        return \" \".join(prompt_words)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate benchmark requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            # Add variance if range_ratio > 0\n",
    "            if self.range_ratio > 0:\n",
    "                input_variance = int(self.input_len * self.range_ratio)\n",
    "                output_variance = int(self.output_len * self.range_ratio)\n",
    "                \n",
    "                actual_input_len = random.randint(\n",
    "                    max(1, self.input_len - input_variance),\n",
    "                    self.input_len + input_variance\n",
    "                )\n",
    "                actual_output_len = random.randint(\n",
    "                    max(1, self.output_len - output_variance),\n",
    "                    self.output_len + output_variance\n",
    "                )\n",
    "            else:\n",
    "                actual_input_len = self.input_len\n",
    "                actual_output_len = self.output_len\n",
    "            \n",
    "            prompt = self._generate_random_prompt(actual_input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=actual_input_len,\n",
    "                expected_output_len=actual_output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class ShareGPTDataset:\n",
    "    \"\"\"ShareGPT-style conversation dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, output_len: int, num_requests: int):\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "    \n",
    "    def _generate_conversation_prompt(self) -> str:\n",
    "        \"\"\"Generate conversation-style prompts\"\"\"\n",
    "        conversation_starters = [\n",
    "            \"I need help with creating a business plan for my startup. Can you guide me through the key components?\",\n",
    "            \"Explain the differences between machine learning and deep learning in simple terms.\",\n",
    "            \"What are the best practices for managing a remote software development team?\",\n",
    "            \"Help me understand the key principles of effective project management.\",\n",
    "            \"I'm preparing for a technical interview. Can you explain common algorithms and data structures?\",\n",
    "            \"What are the current trends in artificial intelligence and how might they impact businesses?\",\n",
    "            \"Explain the process of building a scalable web application from scratch.\",\n",
    "            \"I need advice on digital marketing strategies for a B2B software company.\",\n",
    "            \"Help me understand blockchain technology and its potential applications.\",\n",
    "            \"What are the key considerations when designing a user-friendly mobile application?\"\n",
    "        ]\n",
    "        \n",
    "        return random.choice(conversation_starters)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate conversation requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            prompt = self._generate_conversation_prompt()\n",
    "            # Estimate prompt length\n",
    "            prompt_len = len(prompt.split()) * 1.3  # Rough token estimate\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=int(prompt_len),\n",
    "                expected_output_len=self.output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class SonnetDataset:\n",
    "    \"\"\"Generate prompts for creative writing (sonnets)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "    \n",
    "    def _generate_creative_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate creative writing prompts\"\"\"\n",
    "        themes = [\n",
    "            \"love and loss\", \"nature and seasons\", \"time and memory\", \"hope and dreams\",\n",
    "            \"solitude and reflection\", \"journey and discovery\", \"friendship and loyalty\",\n",
    "            \"courage and adversity\", \"beauty and art\", \"wisdom and growth\"\n",
    "        ]\n",
    "        \n",
    "        styles = [\n",
    "            \"in the style of Shakespeare\", \"as a modern poem\", \"with vivid imagery\",\n",
    "            \"using metaphors and symbolism\", \"in free verse\", \"with a nostalgic tone\",\n",
    "            \"incorporating natural elements\", \"with emotional depth\"\n",
    "        ]\n",
    "        \n",
    "        theme = random.choice(themes)\n",
    "        style = random.choice(styles)\n",
    "        \n",
    "        base_prompt = f\"Write a creative piece about {theme} {style}. \"\n",
    "        \n",
    "        # Extend to target length\n",
    "        extensions = [\n",
    "            \"Consider the deeper meaning and universal themes. \",\n",
    "            \"Include rich descriptions and sensory details. \",\n",
    "            \"Explore the emotional complexity of the subject. \",\n",
    "            \"Use literary devices to enhance the narrative. \",\n",
    "            \"Create a compelling and memorable conclusion. \"\n",
    "        ]\n",
    "        \n",
    "        prompt = base_prompt\n",
    "        while len(prompt.split()) < length * 0.75:\n",
    "            prompt += random.choice(extensions)\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate creative writing requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            prompt = self._generate_creative_prompt(self.input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=self.input_len,\n",
    "                expected_output_len=self.output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Result of a single benchmark request\"\"\"\n",
    "    request_id: int\n",
    "    success: bool\n",
    "    prompt_len: int\n",
    "    output_len: int\n",
    "    ttft: float  # Time to first token\n",
    "    tpot: float  # Time per output token\n",
    "    itl: float   # Inter-token latency\n",
    "    e2e_latency: float  # End-to-end latency\n",
    "    error_msg: str = \"\"\n",
    "    timestamp: float = 0.0\n",
    "\n",
    "class TPUBenchmarkEngine:\n",
    "    \"\"\"Benchmark engine for TPU endpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint, use_dedicated_endpoint: bool = True):\n",
    "        self.endpoint = endpoint\n",
    "        self.use_dedicated_endpoint = use_dedicated_endpoint\n",
    "        self.results_lock = threading.Lock()\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "    \n",
    "    def _make_single_request(self, request: BenchmarkRequest, \n",
    "                           temperature: float = 0.7, \n",
    "                           max_tokens: int = None) -> BenchmarkResult:\n",
    "        \"\"\"Make a single request to the TPU endpoint\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if max_tokens is None:\n",
    "            max_tokens = request.expected_output_len + 50  # Add buffer\n",
    "        \n",
    "        try:\n",
    "            # Prepare request in your endpoint format\n",
    "            instances = [{\n",
    "                \"prompt\": request.prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"raw_response\": True\n",
    "            }]\n",
    "            \n",
    "            request_start = time.time()\n",
    "            \n",
    "            # Make prediction using your endpoint\n",
    "            response = self.endpoint.predict(\n",
    "                instances=instances,\n",
    "                use_dedicated_endpoint=self.use_dedicated_endpoint\n",
    "            )\n",
    "            \n",
    "            request_end = time.time()\n",
    "            \n",
    "            # Parse response\n",
    "            if hasattr(response, 'predictions') and response.predictions:\n",
    "                prediction = response.predictions[0]\n",
    "                if isinstance(prediction, dict):\n",
    "                    output_text = prediction.get('generated_text', '') or prediction.get('output', '') or str(prediction)\n",
    "                else:\n",
    "                    output_text = str(prediction)\n",
    "            else:\n",
    "                output_text = \"\"\n",
    "            \n",
    "            # Calculate metrics\n",
    "            e2e_latency = request_end - request_start\n",
    "            output_tokens = len(output_text.split()) * 1.3  # Rough token estimate\n",
    "            \n",
    "            # Estimate TTFT and TPOT (since we don't have streaming)\n",
    "            estimated_ttft = min(0.5, e2e_latency * 0.1)  # Estimate 10% for TTFT\n",
    "            if output_tokens > 1:\n",
    "                tpot = (e2e_latency - estimated_ttft) / output_tokens\n",
    "                itl = tpot  # Approximation\n",
    "            else:\n",
    "                tpot = e2e_latency\n",
    "                itl = e2e_latency\n",
    "            \n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=True,\n",
    "                prompt_len=request.prompt_len,\n",
    "                output_len=int(output_tokens),\n",
    "                ttft=estimated_ttft,\n",
    "                tpot=tpot,\n",
    "                itl=itl,\n",
    "                e2e_latency=e2e_latency,\n",
    "                timestamp=request_start\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=False,\n",
    "                prompt_len=request.prompt_len,\n",
    "                output_len=0,\n",
    "                ttft=0.0,\n",
    "                tpot=0.0,\n",
    "                itl=0.0,\n",
    "                e2e_latency=error_time,\n",
    "                error_msg=str(e),\n",
    "                timestamp=start_time\n",
    "            )\n",
    "    \n",
    "    def run_benchmark(self, \n",
    "                      requests: List[BenchmarkRequest],\n",
    "                      max_concurrency: int = 100,\n",
    "                      temperature: float = 0.7,\n",
    "                      max_tokens: int = None,\n",
    "                      request_rate: float = float('inf')) -> List[BenchmarkResult]:\n",
    "        \"\"\"Run benchmark with specified parameters\"\"\"\n",
    "        \n",
    "        print(f\"Starting benchmark with {len(requests)} requests...\")\n",
    "        print(f\"Max concurrency: {max_concurrency}\")\n",
    "        print(f\"Temperature: {temperature}\")\n",
    "        print(f\"Request rate: {request_rate}\")\n",
    "        \n",
    "        self.results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if request_rate == float('inf'):\n",
    "            # Send all requests as fast as possible\n",
    "            with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "                # Submit all requests\n",
    "                future_to_request = {\n",
    "                    executor.submit(self._make_single_request, req, temperature, max_tokens): req\n",
    "                    for req in requests\n",
    "                }\n",
    "                \n",
    "                # Collect results with progress bar\n",
    "                completed = 0\n",
    "                with tqdm(total=len(requests), desc=\"Processing requests\") as pbar:\n",
    "                    for future in as_completed(future_to_request):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            with self.results_lock:\n",
    "                                self.results.append(result)\n",
    "                        except Exception as e:\n",
    "                            # Handle futures that failed to execute\n",
    "                            request = future_to_request[future]\n",
    "                            error_result = BenchmarkResult(\n",
    "                                request_id=request.request_id,\n",
    "                                success=False,\n",
    "                                prompt_len=request.prompt_len,\n",
    "                                output_len=0,\n",
    "                                ttft=0.0,\n",
    "                                tpot=0.0,\n",
    "                                itl=0.0,\n",
    "                                e2e_latency=0.0,\n",
    "                                error_msg=f\"Future execution failed: {str(e)}\"\n",
    "                            )\n",
    "                            with self.results_lock:\n",
    "                                self.results.append(error_result)\n",
    "                        \n",
    "                        completed += 1\n",
    "                        pbar.update(1)\n",
    "        else:\n",
    "            # Rate-limited requests\n",
    "            request_interval = 1.0 / request_rate\n",
    "            with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "                futures = []\n",
    "                \n",
    "                for req in requests:\n",
    "                    future = executor.submit(self._make_single_request, req, temperature, max_tokens)\n",
    "                    futures.append(future)\n",
    "                    time.sleep(request_interval)\n",
    "                \n",
    "                # Collect results\n",
    "                with tqdm(total=len(requests), desc=\"Processing requests\") as pbar:\n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            with self.results_lock:\n",
    "                                self.results.append(result)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Request failed: {e}\")\n",
    "                        pbar.update(1)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Benchmark completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "class BenchmarkAnalyzer:\n",
    "    \"\"\"Analyze and report benchmark results\"\"\"\n",
    "    \n",
    "    def __init__(self, results: List[BenchmarkResult]):\n",
    "        self.results = results\n",
    "        self.successful_results = [r for r in results if r.success]\n",
    "        self.failed_results = [r for r in results if not r.success]\n",
    "        # For backward compatibility\n",
    "        self.failed_requests = self.failed_results\n",
    "    \n",
    "    def calculate_percentiles(self, values: List[float], percentiles: List[int]) -> Dict[int, float]:\n",
    "        \"\"\"Calculate percentiles for a list of values\"\"\"\n",
    "        if not values:\n",
    "            return {p: 0.0 for p in percentiles}\n",
    "        return {p: np.percentile(values, p) for p in percentiles}\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate benchmark summary statistics\"\"\"\n",
    "        if not self.successful_results:\n",
    "            return {\n",
    "                \"error\": \"No successful requests\",\n",
    "                \"total_requests\": len(self.results),\n",
    "                \"failed_requests\": len(self.failed_results)\n",
    "            }\n",
    "        \n",
    "        # Extract metrics\n",
    "        ttfts = [r.ttft * 1000 for r in self.successful_results]  # Convert to ms\n",
    "        tpots = [r.tpot * 1000 for r in self.successful_results]\n",
    "        itls = [r.itl * 1000 for r in self.successful_results]\n",
    "        e2e_latencies = [r.e2e_latency for r in self.successful_results]\n",
    "        \n",
    "        # Calculate total tokens\n",
    "        total_input_tokens = sum(r.prompt_len for r in self.successful_results)\n",
    "        total_output_tokens = sum(r.output_len for r in self.successful_results)\n",
    "        \n",
    "        # Calculate benchmark duration\n",
    "        if self.successful_results:\n",
    "            timestamps = [r.timestamp for r in self.successful_results]\n",
    "            benchmark_duration = max(timestamps) - min(timestamps) + max(e2e_latencies)\n",
    "        else:\n",
    "            benchmark_duration = 1.0  # Avoid division by zero\n",
    "        \n",
    "        # Throughput calculations\n",
    "        request_throughput = len(self.successful_results) / benchmark_duration\n",
    "        input_token_throughput = total_input_tokens / benchmark_duration\n",
    "        output_token_throughput = total_output_tokens / benchmark_duration\n",
    "        \n",
    "        # Percentiles to calculate\n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        \n",
    "        summary = {\n",
    "            \"successful_requests\": len(self.successful_results),\n",
    "            \"failed_requests\": len(self.failed_requests),\n",
    "            \"total_requests\": len(self.results),\n",
    "            \"benchmark_duration\": benchmark_duration,\n",
    "            \"request_throughput\": request_throughput,\n",
    "            \"input_token_throughput\": input_token_throughput,\n",
    "            \"output_token_throughput\": output_token_throughput,\n",
    "            \"total_input_tokens\": total_input_tokens,\n",
    "            \"total_output_tokens\": total_output_tokens,\n",
    "            \"ttft_percentiles\": self.calculate_percentiles(ttfts, percentiles),\n",
    "            \"tpot_percentiles\": self.calculate_percentiles(tpots, percentiles),\n",
    "            \"itl_percentiles\": self.calculate_percentiles(itls, percentiles),\n",
    "            \"e2e_latency_percentiles\": self.calculate_percentiles([l * 1000 for l in e2e_latencies], percentiles)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted benchmark summary\"\"\"\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            print(f\"❌ Benchmark failed: {summary['error']}\")\n",
    "            print(f\"Total requests: {summary['total_requests']}\")\n",
    "            print(f\"Failed requests: {summary['failed_requests']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"📊 BENCHMARK RESULTS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(f\"\\n📈 Request Statistics:\")\n",
    "        print(f\"   Successful requests: {summary['successful_requests']}\")\n",
    "        print(f\"   Failed requests: {summary['failed_requests']}\")\n",
    "        print(f\"   Success rate: {summary['successful_requests'] / summary['total_requests'] * 100:.1f}%\")\n",
    "        print(f\"   Benchmark duration: {summary['benchmark_duration']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n🚀 Throughput:\")\n",
    "        print(f\"   Request throughput: {summary['request_throughput']:.2f} req/s\")\n",
    "        print(f\"   Input token throughput: {summary['input_token_throughput']:.2f} tok/s\")\n",
    "        print(f\"   Output token throughput: {summary['output_token_throughput']:.2f} tok/s\")\n",
    "        \n",
    "        print(f\"\\n⏱️ Latency Metrics (ms):\")\n",
    "        print(\"   Metric        p50     p90     p95     p99\")\n",
    "        print(\"   \" + \"-\" * 45)\n",
    "        \n",
    "        ttft_p = summary['ttft_percentiles']\n",
    "        print(f\"   TTFT      {ttft_p[50]:7.1f} {ttft_p[90]:7.1f} {ttft_p[95]:7.1f} {ttft_p[99]:7.1f}\")\n",
    "        \n",
    "        tpot_p = summary['tpot_percentiles']\n",
    "        print(f\"   TPOT      {tpot_p[50]:7.1f} {tpot_p[90]:7.1f} {tpot_p[95]:7.1f} {tpot_p[99]:7.1f}\")\n",
    "        \n",
    "        itl_p = summary['itl_percentiles']\n",
    "        print(f\"   ITL       {itl_p[50]:7.1f} {itl_p[90]:7.1f} {itl_p[95]:7.1f} {itl_p[99]:7.1f}\")\n",
    "        \n",
    "        e2e_p = summary['e2e_latency_percentiles']\n",
    "        print(f\"   E2E       {e2e_p[50]:7.1f} {e2e_p[90]:7.1f} {e2e_p[95]:7.1f} {e2e_p[99]:7.1f}\")\n",
    "        \n",
    "        print(f\"\\n📊 Token Statistics:\")\n",
    "        print(f\"   Total input tokens: {summary['total_input_tokens']:,}\")\n",
    "        print(f\"   Total output tokens: {summary['total_output_tokens']:,}\")\n",
    "        print(f\"   Avg input tokens/req: {summary['total_input_tokens'] / summary['successful_requests']:.1f}\")\n",
    "        print(f\"   Avg output tokens/req: {summary['total_output_tokens'] / summary['successful_requests']:.1f}\")\n",
    "    \n",
    "    def save_results(self, \n",
    "                     output_dir: str = \"benchmark_results\",\n",
    "                     filename_prefix: str = \"tpu_benchmark\",\n",
    "                     metadata: Dict[str, Any] = None):\n",
    "        \"\"\"Save results to JSON and CSV files\"\"\"\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        # Add metadata\n",
    "        if metadata:\n",
    "            summary.update(metadata)\n",
    "        \n",
    "        # Save summary JSON\n",
    "        summary_file = os.path.join(output_dir, f\"{filename_prefix}_summary_{timestamp}.json\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Save detailed results CSV\n",
    "        if self.results:\n",
    "            detailed_data = []\n",
    "            for result in self.results:\n",
    "                row = asdict(result)\n",
    "                row['ttft_ms'] = result.ttft * 1000\n",
    "                row['tpot_ms'] = result.tpot * 1000\n",
    "                row['itl_ms'] = result.itl * 1000\n",
    "                row['e2e_latency_ms'] = result.e2e_latency * 1000\n",
    "                detailed_data.append(row)\n",
    "            \n",
    "            detailed_file = os.path.join(output_dir, f\"{filename_prefix}_detailed_{timestamp}.csv\")\n",
    "            df = pd.DataFrame(detailed_data)\n",
    "            df.to_csv(detailed_file, index=False)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved:\")\n",
    "        print(f\"   Summary: {summary_file}\")\n",
    "        if self.results:\n",
    "            print(f\"   Detailed: {detailed_file}\")\n",
    "\n",
    "def run_vllm_style_benchmark(endpoint,\n",
    "                             use_dedicated_endpoint: bool = True,\n",
    "                             dataset_name: str = \"random\",\n",
    "                             model_name: str = \"unknown\",\n",
    "                             num_prompts: int = 1000,\n",
    "                             max_concurrency: int = 100,\n",
    "                             request_rate: float = float('inf'),\n",
    "                             temperature: float = 0.7,\n",
    "                             random_input_len: int = 1024,\n",
    "                             random_output_len: int = 128,\n",
    "                             random_range_ratio: float = 0.0,\n",
    "                             sharegpt_output_len: int = 128,\n",
    "                             sonnet_input_len: int = 500,\n",
    "                             sonnet_output_len: int = 300,\n",
    "                             save_result: bool = True,\n",
    "                             result_dir: str = \"benchmark_results\",\n",
    "                             result_filename: str = None,\n",
    "                             metadata: Dict[str, Any] = None) -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Run vLLM-style benchmark on TPU endpoint\n",
    "    \n",
    "    Args:\n",
    "        endpoint: TPU endpoint object\n",
    "        use_dedicated_endpoint: Whether to use dedicated endpoint\n",
    "        dataset_name: Dataset to use (\"random\", \"sharegpt\", \"sonnet\")\n",
    "        model_name: Name of the model being benchmarked\n",
    "        num_prompts: Number of prompts to process\n",
    "        max_concurrency: Maximum concurrent requests\n",
    "        request_rate: Request rate (req/s), use inf for maximum rate\n",
    "        temperature: Sampling temperature\n",
    "        random_input_len: Input length for random dataset\n",
    "        random_output_len: Output length for random dataset\n",
    "        random_range_ratio: Range ratio for random variance\n",
    "        sharegpt_output_len: Output length for ShareGPT dataset\n",
    "        sonnet_input_len: Input length for sonnet dataset\n",
    "        sonnet_output_len: Output length for sonnet dataset\n",
    "        save_result: Whether to save results to files\n",
    "        result_dir: Directory to save results\n",
    "        result_filename: Custom filename prefix\n",
    "        metadata: Additional metadata to save\n",
    "    \n",
    "    Returns:\n",
    "        List of benchmark results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting vLLM-style TPU Benchmark\")\n",
    "    print(f\"📋 Configuration:\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Dataset: {dataset_name}\")\n",
    "    print(f\"   Requests: {num_prompts}\")\n",
    "    print(f\"   Concurrency: {max_concurrency}\")\n",
    "    print(f\"   Temperature: {temperature}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    if dataset_name == \"random\":\n",
    "        print(f\"   Input length: {random_input_len}\")\n",
    "        print(f\"   Output length: {random_output_len}\")\n",
    "        print(f\"   Range ratio: {random_range_ratio}\")\n",
    "        \n",
    "        dataset = RandomDataset(\n",
    "            input_len=random_input_len,\n",
    "            output_len=random_output_len,\n",
    "            num_requests=num_prompts,\n",
    "            range_ratio=random_range_ratio\n",
    "        )\n",
    "    elif dataset_name == \"sharegpt\":\n",
    "        print(f\"   Output length: {sharegpt_output_len}\")\n",
    "        \n",
    "        dataset = ShareGPTDataset(\n",
    "            output_len=sharegpt_output_len,\n",
    "            num_requests=num_prompts\n",
    "        )\n",
    "    elif dataset_name == \"sonnet\":\n",
    "        print(f\"   Input length: {sonnet_input_len}\")\n",
    "        print(f\"   Output length: {sonnet_output_len}\")\n",
    "        \n",
    "        dataset = SonnetDataset(\n",
    "            input_len=sonnet_input_len,\n",
    "            output_len=sonnet_output_len,\n",
    "            num_requests=num_prompts\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    # Generate requests\n",
    "    print(f\"\\n📝 Generating {num_prompts} requests...\")\n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine(endpoint, use_dedicated_endpoint)\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=max_concurrency,\n",
    "        temperature=temperature,\n",
    "        request_rate=request_rate\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_summary()\n",
    "    \n",
    "    # Save results\n",
    "    if save_result:\n",
    "        filename_prefix = result_filename or f\"vllm_tpu_{dataset_name}_{model_name.replace('/', '_')}\"\n",
    "        \n",
    "        # Prepare metadata\n",
    "        benchmark_metadata = {\n",
    "            \"model\": model_name,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"num_prompts\": num_prompts,\n",
    "            \"max_concurrency\": max_concurrency,\n",
    "            \"request_rate\": request_rate,\n",
    "            \"temperature\": temperature,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        if dataset_name == \"random\":\n",
    "            benchmark_metadata.update({\n",
    "                \"random_input_len\": random_input_len,\n",
    "                \"random_output_len\": random_output_len,\n",
    "                \"random_range_ratio\": random_range_ratio\n",
    "            })\n",
    "        elif dataset_name == \"sharegpt\":\n",
    "            benchmark_metadata[\"sharegpt_output_len\"] = sharegpt_output_len\n",
    "        elif dataset_name == \"sonnet\":\n",
    "            benchmark_metadata.update({\n",
    "                \"sonnet_input_len\": sonnet_input_len,\n",
    "                \"sonnet_output_len\": sonnet_output_len\n",
    "            })\n",
    "        \n",
    "        if metadata:\n",
    "            benchmark_metadata.update(metadata)\n",
    "        \n",
    "        analyzer.save_results(\n",
    "            output_dir=result_dir,\n",
    "            filename_prefix=filename_prefix,\n",
    "            metadata=benchmark_metadata\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with command line interface\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"vLLM-style benchmark for TPU endpoints\")\n",
    "    \n",
    "    # Dataset options\n",
    "    parser.add_argument(\"--dataset-name\", choices=[\"random\", \"sharegpt\", \"sonnet\"], \n",
    "                       default=\"random\", help=\"Dataset to benchmark on\")\n",
    "    parser.add_argument(\"--model\", required=True, help=\"Model name for identification\")\n",
    "    parser.add_argument(\"--num-prompts\", type=int, default=1000, \n",
    "                       help=\"Number of prompts to process\")\n",
    "    \n",
    "    # Request options\n",
    "    parser.add_argument(\"--max-concurrency\", type=int, default=100,\n",
    "                       help=\"Maximum number of concurrent requests\")\n",
    "    parser.add_argument(\"--request-rate\", type=float, default=float('inf'),\n",
    "                       help=\"Request rate in requests per second\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.7,\n",
    "                       help=\"Sampling temperature\")\n",
    "    \n",
    "    # Dataset-specific options\n",
    "    parser.add_argument(\"--random-input-len\", type=int, default=1024,\n",
    "                       help=\"Input length for random dataset\")\n",
    "    parser.add_argument(\"--random-output-len\", type=int, default=128,\n",
    "                       help=\"Output length for random dataset\")\n",
    "    parser.add_argument(\"--random-range-ratio\", type=float, default=0.0,\n",
    "                       help=\"Range ratio for random variance\")\n",
    "    parser.add_argument(\"--sharegpt-output-len\", type=int, default=128,\n",
    "                       help=\"Output length for ShareGPT dataset\")\n",
    "    parser.add_argument(\"--sonnet-input-len\", type=int, default=500,\n",
    "                       help=\"Input length for sonnet dataset\")\n",
    "    parser.add_argument(\"--sonnet-output-len\", type=int, default=300,\n",
    "                       help=\"Output length for sonnet dataset\")\n",
    "    \n",
    "    # Output options\n",
    "    parser.add_argument(\"--save-result\", action=\"store_true\",\n",
    "                       help=\"Save benchmark results\")\n",
    "    parser.add_argument(\"--result-dir\", default=\"benchmark_results\",\n",
    "                       help=\"Directory to save results\")\n",
    "    parser.add_argument(\"--result-filename\", default=None,\n",
    "                       help=\"Custom filename prefix for results\")\n",
    "    parser.add_argument(\"--metadata\", nargs=\"*\", metavar=\"KEY=VALUE\",\n",
    "                       help=\"Additional metadata (e.g., --metadata version=0.3.3 tp=1)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Parse metadata\n",
    "    metadata = {}\n",
    "    if args.metadata:\n",
    "        for item in args.metadata:\n",
    "            if \"=\" in item:\n",
    "                key, value = item.split(\"=\", 1)\n",
    "                metadata[key] = value\n",
    "    \n",
    "    print(\"⚠️  This benchmark requires your TPU endpoint to be configured.\")\n",
    "    print(\"   Please ensure you have:\")\n",
    "    print(\"   1. endpoint = aiplatform.Endpoint(aip_endpoint_name)\")\n",
    "    print(\"   2. use_dedicated_endpoint = True/False\")\n",
    "    print(\"   3. Then call: run_vllm_style_benchmark(endpoint, use_dedicated_endpoint, ...)\")\n",
    "    \n",
    "    return args\n",
    "\n",
    "# Preset benchmark configurations\n",
    "BENCHMARK_PRESETS = {\n",
    "    \"quick_test\": {\n",
    "        \"num_prompts\": 100,\n",
    "        \"max_concurrency\": 10,\n",
    "        \"random_input_len\": 512,\n",
    "        \"random_output_len\": 64,\n",
    "        \"description\": \"Quick test with small load\"\n",
    "    },\n",
    "    \"latency_test\": {\n",
    "        \"num_prompts\": 500,\n",
    "        \"max_concurrency\": 1,\n",
    "        \"random_input_len\": 1024,\n",
    "        \"random_output_len\": 128,\n",
    "        \"description\": \"Single-request latency measurement\"\n",
    "    },\n",
    "    \"throughput_test\": {\n",
    "        \"num_prompts\": 2000,\n",
    "        \"max_concurrency\": 200,\n",
    "        \"random_input_len\": 512,\n",
    "        \"random_output_len\": 128,\n",
    "        \"description\": \"High-throughput test\"\n",
    "    },\n",
    "    \"long_context\": {\n",
    "        \"num_prompts\": 100,\n",
    "        \"max_concurrency\": 20,\n",
    "        \"random_input_len\": 4096,\n",
    "        \"random_output_len\": 256,\n",
    "        \"description\": \"Long context processing test\"\n",
    "    },\n",
    "    \"conversation\": {\n",
    "        \"dataset_name\": \"sharegpt\",\n",
    "        \"num_prompts\": 1000,\n",
    "        \"max_concurrency\": 50,\n",
    "        \"sharegpt_output_len\": 200,\n",
    "        \"description\": \"Conversational AI test\"\n",
    "    },\n",
    "    \"creative_writing\": {\n",
    "        \"dataset_name\": \"sonnet\",\n",
    "        \"num_prompts\": 500,\n",
    "        \"max_concurrency\": 30,\n",
    "        \"sonnet_input_len\": 300,\n",
    "        \"sonnet_output_len\": 500,\n",
    "        \"description\": \"Creative writing test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_preset_benchmark(endpoint, \n",
    "                        use_dedicated_endpoint: bool,\n",
    "                        preset_name: str,\n",
    "                        model_name: str = \"unknown\",\n",
    "                        **kwargs) -> List[BenchmarkResult]:\n",
    "    \"\"\"Run a preset benchmark configuration\"\"\"\n",
    "    \n",
    "    if preset_name not in BENCHMARK_PRESETS:\n",
    "        available = \", \".join(BENCHMARK_PRESETS.keys())\n",
    "        raise ValueError(f\"Unknown preset '{preset_name}'. Available: {available}\")\n",
    "    \n",
    "    preset_config = BENCHMARK_PRESETS[preset_name].copy()\n",
    "    description = preset_config.pop(\"description\", \"\")\n",
    "    \n",
    "    # Override with any provided kwargs\n",
    "    preset_config.update(kwargs)\n",
    "    \n",
    "    print(f\"🎯 Running preset benchmark: {preset_name}\")\n",
    "    print(f\"📝 Description: {description}\")\n",
    "    \n",
    "    return run_vllm_style_benchmark(\n",
    "        endpoint=endpoint,\n",
    "        use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "        model_name=model_name,\n",
    "        **preset_config\n",
    "    )\n",
    "\n",
    "def run_comprehensive_benchmark_suite(endpoint,\n",
    "                                    use_dedicated_endpoint: bool,\n",
    "                                    model_name: str = \"unknown\",\n",
    "                                    save_results: bool = True) -> Dict[str, List[BenchmarkResult]]:\n",
    "    \"\"\"Run a comprehensive benchmark suite with multiple configurations\"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting Comprehensive TPU Benchmark Suite\")\n",
    "    print(f\"📋 Model: {model_name}\")\n",
    "    print(f\"🎯 Running {len(BENCHMARK_PRESETS)} preset configurations\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for preset_name in BENCHMARK_PRESETS.keys():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔄 Running preset: {preset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            results = run_preset_benchmark(\n",
    "                endpoint=endpoint,\n",
    "                use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "                preset_name=preset_name,\n",
    "                model_name=model_name,\n",
    "                save_result=save_results,\n",
    "                result_filename=f\"comprehensive_{preset_name}_{model_name.replace('/', '_')}\"\n",
    "            )\n",
    "            all_results[preset_name] = results\n",
    "            print(f\"✅ Completed preset: {preset_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed preset {preset_name}: {e}\")\n",
    "            all_results[preset_name] = []\n",
    "    \n",
    "    # Generate comparative report\n",
    "    if save_results:\n",
    "        generate_comparative_suite_report(all_results, model_name)\n",
    "    \n",
    "    print(f\"\\n🎉 Comprehensive benchmark suite completed!\")\n",
    "    print(f\"📊 Results available for {len([k for k, v in all_results.items() if v])} configurations\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_comparative_suite_report(all_results: Dict[str, List[BenchmarkResult]], \n",
    "                                    model_name: str):\n",
    "    \"\"\"Generate a comparative report across all benchmark configurations\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"benchmark_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    report_file = os.path.join(output_dir, f\"comprehensive_report_{model_name.replace('/', '_')}_{timestamp}.md\")\n",
    "    \n",
    "    # Analyze each configuration\n",
    "    config_summaries = {}\n",
    "    for preset_name, results in all_results.items():\n",
    "        if results:\n",
    "            analyzer = BenchmarkAnalyzer(results)\n",
    "            config_summaries[preset_name] = analyzer.generate_summary()\n",
    "    \n",
    "    # Generate markdown report\n",
    "    md_content = f\"\"\"# Comprehensive TPU Benchmark Report\n",
    "\n",
    "**Model:** {model_name}  \n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Configurations Tested:** {len(config_summaries)}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents comprehensive benchmarking results across multiple test configurations for the TPU-hosted model.\n",
    "\n",
    "## Configuration Comparison\n",
    "\n",
    "| Configuration | Requests/s | Output tok/s | TTFT p95 (ms) | TPOT p95 (ms) | Success Rate |\n",
    "|---------------|------------|--------------|---------------|---------------|--------------|\n",
    "\"\"\"\n",
    "    \n",
    "    # Add comparison table rows\n",
    "    for preset_name, summary in config_summaries.items():\n",
    "        if \"error\" not in summary:\n",
    "            success_rate = summary['successful_requests'] / summary['total_requests'] * 100\n",
    "            md_content += f\"| {preset_name} | {summary['request_throughput']:.2f} | {summary['output_token_throughput']:.2f} | {summary['ttft_percentiles'][95]:.1f} | {summary['tpot_percentiles'][95]:.1f} | {success_rate:.1f}% |\\n\"\n",
    "    \n",
    "    md_content += f\"\"\"\n",
    "\n",
    "## Detailed Results by Configuration\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add detailed results for each configuration\n",
    "    for preset_name, summary in config_summaries.items():\n",
    "        preset_config = BENCHMARK_PRESETS[preset_name]\n",
    "        md_content += f\"\"\"\n",
    "### {preset_name.replace('_', ' ').title()}\n",
    "\n",
    "**Description:** {preset_config.get('description', 'N/A')}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            md_content += f\"❌ **Status:** Failed - {summary['error']}\\n\\n\"\n",
    "            continue\n",
    "        \n",
    "        md_content += f\"\"\"**Status:** ✅ Successful\n",
    "\n",
    "**Request Statistics:**\n",
    "- Total requests: {summary['total_requests']}\n",
    "- Successful requests: {summary['successful_requests']}\n",
    "- Failed requests: {summary['failed_requests']}\n",
    "- Success rate: {summary['successful_requests'] / summary['total_requests'] * 100:.1f}%\n",
    "- Duration: {summary['benchmark_duration']:.2f}s\n",
    "\n",
    "**Throughput:**\n",
    "- Request throughput: {summary['request_throughput']:.2f} req/s\n",
    "- Input token throughput: {summary['input_token_throughput']:.2f} tok/s\n",
    "- Output token throughput: {summary['output_token_throughput']:.2f} tok/s\n",
    "\n",
    "**Latency (ms):**\n",
    "- TTFT p95: {summary['ttft_percentiles'][95]:.1f}ms\n",
    "- TPOT p95: {summary['tpot_percentiles'][95]:.1f}ms\n",
    "- ITL p95: {summary['itl_percentiles'][95]:.1f}ms\n",
    "- E2E p95: {summary['e2e_latency_percentiles'][95]:.1f}ms\n",
    "\n",
    "**Token Statistics:**\n",
    "- Total input tokens: {summary['total_input_tokens']:,}\n",
    "- Total output tokens: {summary['total_output_tokens']:,}\n",
    "- Avg input tokens/req: {summary['total_input_tokens'] / summary['successful_requests']:.1f}\n",
    "- Avg output tokens/req: {summary['total_output_tokens'] / summary['successful_requests']:.1f}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add recommendations\n",
    "    md_content += f\"\"\"\n",
    "## Performance Analysis & Recommendations\n",
    "\n",
    "### Best Performing Configurations\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Find best configurations by different metrics\n",
    "    if config_summaries:\n",
    "        # Best throughput\n",
    "        best_throughput = max(config_summaries.items(), \n",
    "                            key=lambda x: x[1].get('output_token_throughput', 0) if 'error' not in x[1] else 0)\n",
    "        \n",
    "        # Best latency\n",
    "        best_latency = min(config_summaries.items(),\n",
    "                          key=lambda x: x[1].get('ttft_percentiles', {}).get(95, float('inf')) if 'error' not in x[1] else float('inf'))\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "**🚀 Highest Throughput:** {best_throughput[0]} ({best_throughput[1].get('output_token_throughput', 0):.2f} tok/s)  \n",
    "**⚡ Lowest Latency:** {best_latency[0]} ({best_latency[1].get('ttft_percentiles', {}).get(95, 0):.1f}ms TTFT p95)\n",
    "\n",
    "### Use Case Recommendations\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Provide recommendations based on results\n",
    "        for preset_name, summary in config_summaries.items():\n",
    "            if \"error\" in summary:\n",
    "                continue\n",
    "                \n",
    "            preset_config = BENCHMARK_PRESETS[preset_name]\n",
    "            use_case = \"\"\n",
    "            \n",
    "            if preset_name == \"quick_test\":\n",
    "                use_case = \"Development and testing\"\n",
    "            elif preset_name == \"latency_test\":\n",
    "                use_case = \"Low-latency applications requiring fast response times\"\n",
    "            elif preset_name == \"throughput_test\":\n",
    "                use_case = \"High-volume batch processing\"\n",
    "            elif preset_name == \"long_context\":\n",
    "                use_case = \"Document analysis and long-form content processing\"\n",
    "            elif preset_name == \"conversation\":\n",
    "                use_case = \"Interactive chatbots and conversational AI\"\n",
    "            elif preset_name == \"creative_writing\":\n",
    "                use_case = \"Content generation and creative applications\"\n",
    "            \n",
    "            success_rate = summary['successful_requests'] / summary['total_requests'] * 100\n",
    "            suitability = \"✅ Excellent\" if success_rate > 95 and summary['output_token_throughput'] > 10 else \"⚠️ Good\" if success_rate > 90 else \"❌ Needs optimization\"\n",
    "            \n",
    "            md_content += f\"\"\"\n",
    "**{preset_name.replace('_', ' ').title()}:** {suitability}  \n",
    "*Use case:* {use_case}  \n",
    "*Performance:* {summary['output_token_throughput']:.1f} tok/s, {summary['ttft_percentiles'][95]:.1f}ms TTFT p95  \n",
    "\"\"\"\n",
    "    \n",
    "    md_content += f\"\"\"\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "**Model:** {model_name}  \n",
    "**Test Framework:** vLLM-style TPU Benchmark Suite  \n",
    "**Timestamp:** {timestamp}  \n",
    "\n",
    "---\n",
    "*Report generated automatically by TPU Benchmark Suite*\n",
    "\"\"\"\n",
    "    \n",
    "    # Save the report\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(md_content)\n",
    "    \n",
    "    print(f\"\\n📊 Comprehensive report saved: {report_file}\")\n",
    "\n",
    "# Example usage functions\n",
    "def benchmark_examples():\n",
    "    \"\"\"Show example usage patterns\"\"\"\n",
    "    print(\"\"\"\n",
    "🚀 vLLM-Style TPU Benchmark Examples\n",
    "\n",
    "# 1. Quick test (your existing endpoint setup)\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# 2. Run quick benchmark\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e_base\",\n",
    "    dataset_name=\"random\",\n",
    "    num_prompts=100,\n",
    "    max_concurrency=10,\n",
    "    random_input_len=512,\n",
    "    random_output_len=128\n",
    ")\n",
    "\n",
    "# 3. Run preset benchmarks\n",
    "results = run_preset_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    preset_name=\"throughput_test\",\n",
    "    model_name=\"llama3.3_tpuv6e_base\"\n",
    ")\n",
    "\n",
    "# 4. Run comprehensive suite\n",
    "all_results = run_comprehensive_benchmark_suite(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e_base\"\n",
    ")\n",
    "\n",
    "# 5. Custom configuration\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"your-model-name\",\n",
    "    dataset_name=\"sharegpt\",  # or \"sonnet\"\n",
    "    num_prompts=1000,\n",
    "    max_concurrency=50,\n",
    "    request_rate=10.0,  # 10 req/s instead of unlimited\n",
    "    temperature=0.7,\n",
    "    save_result=True,\n",
    "    metadata={\"version\": \"1.0\", \"notes\": \"production test\"}\n",
    ")\n",
    "\n",
    "Available presets: \"\"\" + \", \".join(BENCHMARK_PRESETS.keys()) + \"\"\"\n",
    "Available datasets: random, sharegpt, sonnet\n",
    "\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If in notebook, show examples\n",
    "        benchmark_examples()\n",
    "    except NameError:\n",
    "        # If not in notebook, run CLI\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your existing endpoint setup\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# 1. Quick test (like your use cases)\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    dataset_name=\"random\",\n",
    "    num_prompts=500,\n",
    "    max_concurrency=50,\n",
    "    random_input_len=80,      # Email use case\n",
    "    random_output_len=350,\n",
    "    save_result=True\n",
    ")\n",
    "\n",
    "# 2. Run preset configurations\n",
    "results = run_preset_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    preset_name=\"throughput_test\",\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")\n",
    "\n",
    "# 3. Comprehensive suite (all configurations)\n",
    "all_results = run_comprehensive_benchmark_suite(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 250 concurrency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your existing endpoint setup\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# 1. Quick test (like your use cases)\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    dataset_name=\"random\",\n",
    "    num_prompts=500,\n",
    "    max_concurrency=250,\n",
    "    random_input_len=80,      # Email use case\n",
    "    random_output_len=350,\n",
    "    save_result=True\n",
    ")\n",
    "\n",
    "# 2. Run preset configurations\n",
    "results = run_preset_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    preset_name=\"throughput_test\",\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")\n",
    "\n",
    "# 3. Comprehensive suite (all configurations)\n",
    "all_results = run_comprehensive_benchmark_suite(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxaKC69ypQds"
   },
   "source": [
    "## Clean up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "HYNmBz8PdcvJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # @title Delete the models and endpoints\n",
    "# # @markdown  Delete the experiment models and endpoints to recycle the resources\n",
    "# # @markdown  and avoid unnecessary continuous charges that may incur.\n",
    "\n",
    "# # Undeploy model and delete endpoint.\n",
    "# for endpoint in endpoints.values():\n",
    "#     endpoint.delete(force=True)\n",
    "\n",
    "# # Delete models.\n",
    "# for model in models.values():\n",
    "#     model.delete()\n",
    "\n",
    "# delete_bucket = F   # @param {type:\"boolean\"}\n",
    "# if delete_bucket:\n",
    "#     ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
