{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begining of NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "ax7zWynUDcjk"
   },
   "outputs": [],
   "source": [
    "# @title Request for quota\n",
    "\n",
    "# @markdown For serving Llama 3.1 8B and Qwen3 32B models, we need 1 and 4 TPU v6es, respectively.\n",
    "\n",
    "# @markdown > | Model | Accelerator Type |\n",
    "# @markdown | ----------- | ----------- |\n",
    "# @markdown | Llama 3.1 8B |1 TPU v6e (ct6e-standard-1t)|\n",
    "# @markdown | Qwen3 32B|4 TPU v6e (ct6e-standard-4t)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env setup bucket name (in same region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "VrQvw5wN3gzl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 16:13:54.492753: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-07 16:13:54.493697: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-07 16:13:54.496628: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-07 16:13:54.505135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751904834.519846   11159 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751904834.524110   11159 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751904834.535161   11159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751904834.535179   11159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751904834.535181   11159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751904834.535182   11159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-07 16:13:54.539009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-87995179092-7b820def-6119-42f1-a411-8844efc21777\" finished successfully.\n",
      "Using this GCS Bucket: gs://llama31_training-europe\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 87995179092-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Google Cloud project\n",
    "\n",
    "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
    "\n",
    "# BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "# REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'\n",
    "\n",
    "# Import the necessary packages\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"europe-west4\" #\"us-south1\" #\"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "\n",
    "BUCKET_URI = \"gs://llama31_training-europe\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = LOCATION # \"us-south1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
    "        raise ValueError(\n",
    "            \"REGION must be set. See\"\n",
    "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
    "            \" available cloud locations.\"\n",
    "        )\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"vllm_tpu\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "# ! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "YXFGIp1l-qtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# @title Access the models\n",
    "# @markdown ### Access Llama 3.1 and Qwen3 models on Vertex AI for serving\n",
    "# @markdown The models from the Hugging Face can be used for serving in Vertex AI.\n",
    "# @markdown 1. Open the [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) and [Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B) models from [Hugging Face](https://huggingface.co/).\n",
    "# @markdown 2. Review and accept the agreement.\n",
    "# @markdown 3. After accepting the agreement, models will be available for serving.\n",
    "# @markdown 4. You must provide a Hugging Face User Access Token (with read access) to access the Llama 3.1 model. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"Error: HF_TOKEN not found in .env file or not provided.\")\n",
    "    print(\"Please provide a read HF_TOKEN to Llama 3.1 model from Hugging Face in your .env file.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN loaded successfully.\")\n",
    "    # You can now use HF_TOKEN in your code, e.g., to authenticate with Hugging Face models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "7MgDO17YG7nz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.3-70B-Instruct will run on 8 tpu\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare\n",
    "\n",
    "# @markdown In this section you can choose a desired model and the region for TPU deployment.\n",
    "# @markdown Learn about [TPU v6e machine types](https://cloud.google.com/tpu/docs/v6e#configurations) for Vertex AI prediction.\n",
    "\n",
    "# @markdown Here are 2 example models you can run:\n",
    "            \n",
    "MODEL_ID = \"Llama-3.3-70B-Instruct\" #'Llama-3.3-70B-Instruct' #\"Llama-3.1-8B-Instruct\"  # @param [\"Llama-3.1-8B-Instruct\", \"Qwen3-32B\"] {isTemplate: true}\n",
    "\n",
    "TPU_DEPLOYMENT_REGION = \"europe-west4\"  # @param {type:\"string\"}\n",
    "\n",
    "tpu_type = \"TPU_V6e\"\n",
    "\n",
    "\n",
    "if \"Llama-3.3\" in MODEL_ID:\n",
    "    model_path_prefix = \"meta-llama/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"meta\"\n",
    "    model_publisher_id = \"llama33\"\n",
    "    machine_type = \"ct6e-standard-8t\"\n",
    "    tpu_count = 8\n",
    "    tpu_topo = None #\"2x4\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpu\")\n",
    "elif \"Llama-3\" in MODEL_ID:\n",
    "    model_path_prefix = \"meta-llama/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"meta\"\n",
    "    model_publisher_id = \"llama3\"\n",
    "    machine_type = \"ct6e-standard-1t\"\n",
    "    tpu_count = 1\n",
    "    tpu_topo = \"1x1\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpu\")    \n",
    "elif \"Qwen3\" in MODEL_ID:\n",
    "    model_path_prefix = \"Qwen/\"\n",
    "    model_id = os.path.join(model_path_prefix, MODEL_ID)\n",
    "    model_publisher = \"qwen\"\n",
    "    model_publisher_id = \"qwen3\"\n",
    "    machine_type = \"ct6e-standard-4t\"\n",
    "    tpu_count = 4\n",
    "    tpu_topo = \"2x2\"\n",
    "    print(MODEL_ID, \"will run on\", tpu_count, \"tpus\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported MODEL_ID: {MODEL_ID}\")\n",
    "\n",
    "\n",
    "vLLM_TPU_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250529_0917_tpu_experimental_RC00\"\n",
    "\n",
    "## 8.5 -- latest on TPU / 0.8 version\n",
    "\n",
    "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "# common_util.check_quota(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=TPU_DEPLOYMENT_REGION,\n",
    "#     accelerator_type=tpu_type,\n",
    "#     accelerator_count=tpu_count,\n",
    "#     is_for_training=False,\n",
    "# )\n",
    "\n",
    "\n",
    "# Server parameters.\n",
    "tensor_parallel_size = tpu_count\n",
    "\n",
    "# Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "\n",
    "# Maximum number of running sequences in a continuous batch.\n",
    "max_running_seqs = 256  # @param\n",
    "# Maximum context length for a request.\n",
    "max_model_len = 2048  # @param\n",
    "\n",
    "# Endpoint configurations.\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n",
    "\n",
    "run_name = \"llama33m\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown Note: The vLLM-TPU container used in this notebook is in experimental status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common_util.check_quota(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=TPU_DEPLOYMENT_REGION,\n",
    "#     accelerator_type=tpu_type,\n",
    "#     accelerator_count=tpu_count,\n",
    "#     is_for_training=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctdQJhrdaeeT"
   },
   "source": [
    "## Deploy prebuilt Llama 3.1 8B or Qwen3 32B models with vLLM on TPUs\n",
    "This section will download the prebuilt model chosen in the previous section and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpu_topology = '1x8'\n",
    "int(tpu_topology.split(\"x\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "Cwf_xWpEFkWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/87995179092/locations/europe-west4/endpoints/1029620071644790784/operations/3103298039557652480\n",
      "Endpoint created. Resource name: projects/87995179092/locations/europe-west4/endpoints/1029620071644790784\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/87995179092/locations/europe-west4/endpoints/1029620071644790784')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/87995179092/locations/europe-west4/models/7065305589437431808/operations/5863160186205634560\n",
      "Model created. Resource name: projects/87995179092/locations/europe-west4/models/7065305589437431808@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/87995179092/locations/europe-west4/models/7065305589437431808@1')\n",
      "Deploying model to Endpoint : projects/87995179092/locations/europe-west4/endpoints/1029620071644790784\n",
      "Deploy Endpoint model backing LRO: projects/87995179092/locations/europe-west4/endpoints/1029620071644790784/operations/2915554230091644928\n",
      "Endpoint model deployed. Resource name: projects/87995179092/locations/europe-west4/endpoints/1029620071644790784\n"
     ]
    }
   ],
   "source": [
    "# @title Deploy\n",
    "def deploy_model_vllm_tpu(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct6e-standard-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    max_model_len: int = 4096,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    model_type: str = None,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with vLLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = 1 #int(tpu_topology.split(\"x\")[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    vllmtpu_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllmtpu_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllmtpu_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "        \"VLLM_USE_V1\": \"1\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=vLLM_TPU_DOCKER_URI,\n",
    "        serving_container_args=vllmtpu_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb\",\n",
    "        },\n",
    "    )\n",
    "    return model, endpoint\n",
    "\n",
    "\n",
    "models[\"vllmtpu\"], endpoints[\"vllmtpu\"] = deploy_model_vllm_tpu(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=run_name),\n",
    "    model_id=model_id,\n",
    "    publisher=model_publisher,\n",
    "    publisher_model_id=model_publisher_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    machine_type=machine_type,\n",
    "    tpu_topology=tpu_topo,\n",
    "    max_model_len=max_model_len,\n",
    "    enable_chunked_prefill=True,\n",
    "    enable_prefix_cache=True,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car that can run on the wall?\n",
      "Output:\n",
      " That's right, a car wall-able.\n",
      "-What is a man that does not have all his pie? broken. (broke and)\n",
      "-What is a car that can run on the wall? That's right, a car wall-\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"1029620071644790784\"  # @param {type:\"string\"}\n",
    "aip_endpoint_name = (\n",
    "    f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    ")\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "\n",
    "response = endpoint.predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cellView": "form",
    "id": "78nk6Cl2pqer"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Overrides parameters for inferences.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m instances \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     {\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     },\n\u001b[1;32m     39\u001b[0m ]\n\u001b[0;32m---> 40\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mendpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvllmtpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(\n\u001b[1;32m     41\u001b[0m     instances\u001b[38;5;241m=\u001b[39minstances, use_dedicated_endpoint\u001b[38;5;241m=\u001b[39muse_dedicated_endpoint\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mpredictions:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# @title Raw predict\n",
    "\n",
    "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "# @markdown Example:\n",
    "\n",
    "# @markdown ```\n",
    "# @markdown Human: What is a car?\n",
    "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "# @markdown ```\n",
    "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
    "\n",
    "# Loads an existing endpoint instance using the endpoint name:\n",
    "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
    "#   endpoint name of the endpoint `endpoint` created in the cell\n",
    "#   above.\n",
    "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "#   an existing endpoint with the ID 1234567890123456789.\n",
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"What is a car that can run on the wall?\"  # @param {type: \"string\"}\n",
    "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"vllmtpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "# @markdown Note Top-k sampling is not currently enabled for vLLM on TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Report with test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test started at: 2025-07-07 16:44:37\n",
      "Output files will be saved as:\n",
      "- Summary CSV: vllm_performance_tests/vllm_test_20250707_164437.csv\n",
      "- Detailed CSV: vllm_performance_tests/vllm_detailed_20250707_164437.csv\n",
      "- Report MD: vllm_performance_tests/vllm_report_20250707_164437.md\n",
      "Generating test prompts...\n",
      "Generated 10000 test prompts\n",
      "Sample prompt length: 289 words\n",
      "Sample prompt preview: Analyze the following complex scenario and provide a detailed response covering multiple aspects:\n",
      "\n",
      "A multinational technology company is considering implementing a comprehensive artificial intelligenc...\n",
      "\n",
      "Starting load test:\n",
      "- Concurrent users: 250\n",
      "- Total requests: 10000\n",
      "- Target output tokens: 317\n",
      "Completed 5100/10000 requests...\n",
      "Completed 5200/10000 requests...\n",
      "Completed 5300/10000 requests...\n",
      "Completed 5400/10000 requests...\n",
      "Completed 5500/10000 requests...\n",
      "Completed 5600/10000 requests...\n",
      "Completed 5700/10000 requests...\n",
      "Completed 5800/10000 requests...\n",
      "Completed 5900/10000 requests...\n",
      "Completed 6000/10000 requests...\n",
      "Completed 6100/10000 requests...\n",
      "Completed 6200/10000 requests...\n",
      "Completed 6300/10000 requests...\n",
      "Completed 6400/10000 requests...\n",
      "Completed 6500/10000 requests...\n",
      "Completed 6600/10000 requests...\n",
      "Completed 6700/10000 requests...\n",
      "Completed 6800/10000 requests...\n",
      "Completed 6900/10000 requests...\n",
      "Completed 7000/10000 requests...\n",
      "Completed 7100/10000 requests...\n",
      "Completed 7200/10000 requests...\n",
      "Completed 7300/10000 requests...\n",
      "Completed 7400/10000 requests...\n",
      "Completed 7500/10000 requests...\n",
      "Completed 7600/10000 requests...\n",
      "Completed 7700/10000 requests...\n",
      "Completed 7800/10000 requests...\n",
      "Completed 7900/10000 requests...\n",
      "Completed 8000/10000 requests...\n",
      "Completed 8100/10000 requests...\n",
      "Completed 8200/10000 requests...\n",
      "Completed 8300/10000 requests...\n",
      "Completed 8400/10000 requests...\n",
      "Completed 8500/10000 requests...\n",
      "Completed 8600/10000 requests...\n",
      "Completed 8700/10000 requests...\n",
      "Completed 8800/10000 requests...\n",
      "Completed 8900/10000 requests...\n",
      "Completed 9000/10000 requests...\n",
      "Completed 9100/10000 requests...\n",
      "Completed 9200/10000 requests...\n",
      "Completed 9300/10000 requests...\n",
      "Completed 9400/10000 requests...\n",
      "Completed 9500/10000 requests...\n",
      "Completed 9600/10000 requests...\n",
      "Completed 9700/10000 requests...\n",
      "Completed 9800/10000 requests...\n",
      "Completed 9900/10000 requests...\n",
      "Completed 10000/10000 requests...\n",
      "\n",
      "============================================================\n",
      "LOAD TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Summary:\n",
      "- Total requests: 10000\n",
      "- Successful requests: 0\n",
      "- Failed requests: 10000\n",
      "- Success rate: 0.0%\n",
      "- Total test time: 2197.4 seconds\n",
      "\n",
      "Saving detailed results to vllm_performance_tests/vllm_detailed_20250707_164437.csv...\n",
      "Saving summary to vllm_performance_tests/vllm_test_20250707_164437.csv...\n",
      "Generating Markdown report: vllm_performance_tests/vllm_report_20250707_164437.md...\n",
      "\n",
      "============================================================\n",
      "FILES SAVED SUCCESSFULLY\n",
      "============================================================\n",
      "📄 Summary CSV: vllm_performance_tests/vllm_test_20250707_164437.csv\n",
      "📊 Detailed CSV: vllm_performance_tests/vllm_detailed_20250707_164437.csv\n",
      "📝 Markdown Report: vllm_performance_tests/vllm_report_20250707_164437.md\n",
      "\n",
      "Test completed at: 2025-07-07 17:21:14\n",
      "\n",
      "❌ TEST FAILED: No successful requests completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration matching your target metrics\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 250,\n",
    "    'total_requests': 10000,\n",
    "    'input_token_length': 265,  # Target input length\n",
    "    'output_tokens': 317,       # Target output length\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts of approximately 265 tokens each\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following complex scenario and provide a detailed response covering multiple aspects:\n",
    "\n",
    "A multinational technology company is considering implementing a comprehensive artificial intelligence strategy across all departments. The company operates in 15 countries, has 50,000 employees, and generates $20 billion in annual revenue. The CEO wants to understand how AI can transform their business operations, improve customer experience, increase efficiency, and create new revenue streams.\n",
    "\n",
    "Consider the following factors in your analysis:\n",
    "1. Current market trends in AI adoption across different industries\n",
    "2. Potential risks and challenges of large-scale AI implementation\n",
    "3. Required infrastructure and technological investments\n",
    "4. Impact on existing workforce and necessary reskilling programs\n",
    "5. Regulatory compliance considerations in different jurisdictions\n",
    "6. Timeline for phased implementation and expected ROI\n",
    "7. Competitive advantages that could be gained\n",
    "8. Data privacy and security implications\n",
    "9. Integration challenges with legacy systems\n",
    "10. Metrics for measuring success and continuous improvement\n",
    "\n",
    "Please provide a comprehensive strategic recommendation that addresses each of these points with specific examples and actionable insights. Include potential pilot programs, budget considerations, and a roadmap for the next 3-5 years.\"\"\"\n",
    "    \n",
    "    # Adjust length to approximately target tokens\n",
    "    words = base_prompt.split()\n",
    "    target_words = target_tokens * 0.75  # Rough conversion\n",
    "    if len(words) > target_words:\n",
    "        return ' '.join(words[:int(target_words)])\n",
    "    else:\n",
    "        # Extend if needed\n",
    "        extension = \" Additionally, consider the impact on stakeholder relationships, customer trust, brand reputation, and long-term sustainability. Analyze potential partnerships with AI vendors, academic institutions, and research organizations. Evaluate the company's current digital maturity and readiness for AI transformation.\" * 3\n",
    "        return base_prompt + extension\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],           # Time to First Token\n",
    "    'inter_token_latencies': [], # Time between tokens\n",
    "    'end_to_end_times': [],     # Total request time\n",
    "    'input_tokens': [],         # Actual input token counts\n",
    "    'output_tokens': [],        # Actual output token counts\n",
    "    'request_errors': [],       # Failed requests\n",
    "    'timestamps': []            # Request timestamps\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with detailed timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "            \"stream\": config.get('stream', True)\n",
    "        }]\n",
    "        \n",
    "        # Record request start\n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Make prediction\n",
    "        response = endpoints[\"vllmtpu\"].predict(\n",
    "            instances=instances, \n",
    "            use_dedicated_endpoint=use_dedicated_endpoint\n",
    "        )\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response\n",
    "        prediction = response.predictions[0] if response.predictions else {}\n",
    "        output_text = prediction.get('generated_text', '') or str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens (rough approximation)\n",
    "        input_tokens = len(prompt.split()) * 1.3  # Rough token estimate\n",
    "        output_tokens = len(output_text.split()) * 1.3\n",
    "        \n",
    "        # Simulate TTFT and inter-token timing (in real streaming, you'd capture these)\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02)  # Estimate TTFT\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens)\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text  # Truncated for CSV\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': str(e),\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': str(e),\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Generate test prompts\n",
    "print(\"Generating test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "print(f\"Sample prompt preview: {test_prompts[0][:200]}...\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "# Execute test\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    # Submit all requests\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    # Collect results with progress tracking\n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests...\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p)\n",
    "    \n",
    "    # Latency metrics\n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    # Throughput calculations\n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- TTFT (p99): {ttft_p99:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p50): {inter_token_p50:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p50): {e2e_p50:.1f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    print(f\"- End-to-End (p99): {e2e_p99:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "    print(f\"- Total input tokens: {int(total_input_tokens)}\")\n",
    "    print(f\"- Total output tokens: {int(total_output_tokens)}\")\n",
    "\n",
    "# Save detailed results to CSV\n",
    "print(f\"\\nSaving detailed results to {detailed_csv_filename}...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary metrics for CSV\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'target_input_tokens': [TEST_CONFIG['input_token_length']],\n",
    "    'target_output_tokens': [TEST_CONFIG['output_tokens']],\n",
    "    'temperature': [TEST_CONFIG['temperature']],\n",
    "    'max_tokens': [TEST_CONFIG['max_tokens']]\n",
    "}\n",
    "\n",
    "if successful_requests:\n",
    "    summary_data.update({\n",
    "        'ttft_p50_seconds': [ttft_p50],\n",
    "        'ttft_p95_seconds': [ttft_p95],\n",
    "        'ttft_p99_seconds': [ttft_p99],\n",
    "        'inter_token_p50_seconds': [inter_token_p50],\n",
    "        'inter_token_p95_seconds': [inter_token_p95],\n",
    "        'e2e_p50_seconds': [e2e_p50],\n",
    "        'e2e_p95_seconds': [e2e_p95],\n",
    "        'e2e_p99_seconds': [e2e_p99],\n",
    "        'token_output_throughput': [token_output_throughput],\n",
    "        'overall_token_throughput': [overall_token_throughput],\n",
    "        'requests_per_second': [requests_per_second],\n",
    "        'avg_input_tokens': [statistics.mean(input_tokens)],\n",
    "        'avg_output_tokens': [statistics.mean(output_tokens)],\n",
    "        'total_input_tokens': [total_input_tokens],\n",
    "        'total_output_tokens': [total_output_tokens]\n",
    "    })\n",
    "else:\n",
    "    # Fill with zeros if no successful requests\n",
    "    for key in ['ttft_p50_seconds', 'ttft_p95_seconds', 'ttft_p99_seconds', \n",
    "                'inter_token_p50_seconds', 'inter_token_p95_seconds',\n",
    "                'e2e_p50_seconds', 'e2e_p95_seconds', 'e2e_p99_seconds',\n",
    "                'token_output_throughput', 'overall_token_throughput', \n",
    "                'requests_per_second', 'avg_input_tokens', 'avg_output_tokens',\n",
    "                'total_input_tokens', 'total_output_tokens']:\n",
    "        summary_data[key] = [0]\n",
    "\n",
    "# Save summary to CSV\n",
    "print(f\"Saving summary to {csv_filename}...\")\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate Markdown report\n",
    "print(f\"Generating Markdown report: {md_filename}...\")\n",
    "\n",
    "md_content = f\"\"\"# vLLM Performance Test Report\n",
    "\n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Test Duration:** {total_test_time:.1f} seconds  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## Test Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Concurrent Users | {TEST_CONFIG['concurrent_users']} |\n",
    "| Total Requests | {TEST_CONFIG['total_requests']} |\n",
    "| Target Input Tokens | {TEST_CONFIG['input_token_length']} |\n",
    "| Target Output Tokens | {TEST_CONFIG['output_tokens']} |\n",
    "| Temperature | {TEST_CONFIG['temperature']} |\n",
    "| Top P | {TEST_CONFIG['top_p']} |\n",
    "| Max Tokens | {TEST_CONFIG['max_tokens']} |\n",
    "| Stream | {TEST_CONFIG['stream']} |\n",
    "\n",
    "## Test Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests | {len(results)} |\n",
    "| Successful Requests | {len(successful_requests)} |\n",
    "| Failed Requests | {len(failed_requests)} |\n",
    "| Success Rate | {len(successful_requests)/len(results)*100:.1f}% |\n",
    "| Test Duration | {total_test_time:.1f} seconds |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Latency Metrics\n",
    "\n",
    "| Metric | p50 | p95 | p99 |\n",
    "|--------|-----|-----|-----|\n",
    "| Time to First Token (TTFT) | {ttft_p50:.3f}s | {ttft_p95:.3f}s | {ttft_p99:.3f}s |\n",
    "| Inter-token Latency | {inter_token_p50:.3f}s | {inter_token_p95:.3f}s | - |\n",
    "| End-to-End Latency | {e2e_p50:.1f}s | {e2e_p95:.1f}s | {e2e_p99:.1f}s |\n",
    "\n",
    "## Throughput Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Overall Token Throughput | {overall_token_throughput:.2f} tok/sec |\n",
    "| Requests per Second | {requests_per_second:.2f} req/sec |\n",
    "\n",
    "## Token Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Average Input Tokens | {statistics.mean(input_tokens):.1f} |\n",
    "| Average Output Tokens | {statistics.mean(output_tokens):.1f} |\n",
    "| Total Input Tokens | {int(total_input_tokens):,} |\n",
    "| Total Output Tokens | {int(total_output_tokens):,} |\n",
    "\n",
    "## Comparison with Target Metrics\n",
    "\n",
    "| Metric | Target | Actual | Difference |\n",
    "|--------|--------|--------|------------|\n",
    "| TTFT (p95) | 0.9s | {ttft_p95:.3f}s | {((ttft_p95 - 0.9) / 0.9 * 100):+.1f}% |\n",
    "| Inter-token Latency (p95) | 0.17s | {inter_token_p95:.3f}s | {((inter_token_p95 - 0.17) / 0.17 * 100):+.1f}% |\n",
    "| End-to-End (p95) | 44.1s | {e2e_p95:.1f}s | {((e2e_p95 - 44.1) / 44.1 * 100):+.1f}% |\n",
    "| Token Output Throughput | 10.05 tok/sec | {token_output_throughput:.2f} tok/sec | {((token_output_throughput - 10.05) / 10.05 * 100):+.1f}% |\n",
    "| Overall Token Throughput | 1529 tok/sec | {overall_token_throughput:.2f} tok/sec | {((overall_token_throughput - 1529) / 1529 * 100):+.1f}% |\n",
    "| Input Token Length | 265 | {statistics.mean(input_tokens):.1f} | {((statistics.mean(input_tokens) - 265) / 265 * 100):+.1f}% |\n",
    "| Output Tokens | 317 | {statistics.mean(output_tokens):.1f} | {((statistics.mean(output_tokens) - 317) / 317 * 100):+.1f}% |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Error Analysis\n",
    "\n",
    "**Total Failed Requests:** {len(failed_requests)}\n",
    "\n",
    "\"\"\"\n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        error_types[error_msg] += 1\n",
    "    \n",
    "    md_content += \"| Error Type | Count |\\n|------------|-------|\\n\"\n",
    "    for error, count in error_types.items():\n",
    "        md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    # Performance analysis\n",
    "    if ttft_p95 <= 0.9:\n",
    "        md_content += \"✅ **TTFT Performance:** Meeting target (≤ 0.9s)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **TTFT Performance:** Above target (> 0.9s)\\n\\n\"\n",
    "    \n",
    "    if inter_token_p95 <= 0.17:\n",
    "        md_content += \"✅ **Inter-token Latency:** Meeting target (≤ 0.17s)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Inter-token Latency:** Above target (> 0.17s)\\n\\n\"\n",
    "    \n",
    "    if token_output_throughput >= 10.05:\n",
    "        md_content += \"✅ **Token Output Throughput:** Meeting target (≥ 10.05 tok/sec)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Token Output Throughput:** Below target (< 10.05 tok/sec)\\n\\n\"\n",
    "    \n",
    "    if overall_token_throughput >= 1529:\n",
    "        md_content += \"✅ **Overall Token Throughput:** Meeting target (≥ 1529 tok/sec)\\n\\n\"\n",
    "    else:\n",
    "        md_content += \"❌ **Overall Token Throughput:** Below target (< 1529 tok/sec)\\n\\n\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Files Generated\n",
    "\n",
    "- **Summary CSV:** `{csv_filename}`\n",
    "- **Detailed CSV:** `{detailed_csv_filename}`\n",
    "- **This Report:** `{md_filename}`\n",
    "\n",
    "## Test Environment\n",
    "\n",
    "- **vLLM Version:** 0.6.6.post1 (target)\n",
    "- **Max Sequences:** 512 (target)\n",
    "- **KV Cache Dtype:** fp8_e5m2 (target)\n",
    "- **Tensor Parallel Size:** 4 (target)\n",
    "- **Tool Call Parser:** llama3_json (target)\n",
    "\n",
    "---\n",
    "*Report generated automatically by vLLM performance testing script*\n",
    "\"\"\"\n",
    "\n",
    "# Save markdown report\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary CSV: {csv_filename}\")\n",
    "print(f\"📊 Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"📝 Markdown Report: {md_filename}\")\n",
    "print(f\"\\nTest completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Display quick summary\n",
    "if successful_requests:\n",
    "    print(f\"\\n🎯 QUICK PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   TTFT (p95): {ttft_p95:.3f}s (target: 0.9s)\")\n",
    "    print(f\"   Inter-token (p95): {inter_token_p95:.3f}s (target: 0.17s)\")\n",
    "    print(f\"   Throughput: {token_output_throughput:.1f} tok/sec (target: 10.05)\")\n",
    "    print(f\"   Success Rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n❌ TEST FAILED: No successful requests completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test started at: 2025-07-07 23:52:28\n",
      "Output files will be saved as:\n",
      "- Summary CSV: vllm_performance_tests/vllm_test_20250707_235228.csv\n",
      "- Detailed CSV: vllm_performance_tests/vllm_detailed_20250707_235228.csv\n",
      "- Report MD: vllm_performance_tests/vllm_report_20250707_235228.md\n",
      "Testing endpoint with single request first...\n",
      "Request 0 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "❌ Single request test failed!\n",
      "Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "🛑 Endpoint appears to have issues. Consider:\n",
      "1. Check if the endpoint is properly deployed and running\n",
      "2. Verify the endpoint has sufficient resources\n",
      "3. Test with smaller requests first\n",
      "4. Check Google Cloud Console for endpoint logs\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to continue with load test anyway, or Ctrl+C to abort... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 500 test prompts...\n",
      "Generated 500 test prompts\n",
      "Sample prompt length: 63 words\n",
      "\n",
      "Starting load test:\n",
      "- Concurrent users: 250\n",
      "- Total requests: 500\n",
      "- Target output tokens: 317\n",
      "Request 1 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "Request 2 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "Request 3 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "Request 7 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "Request 8 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "Request 11 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))...\n",
      "Request 13 failed: HTTPSConnectionPool(host='1029620071644790784.europe-west4-87995179092.prediction.vertexai.goog', po...\n",
      "Request 9 failed: HTTPSConnectionPool(host='1029620071644790784.europe-west4-87995179092.prediction.vertexai.goog', po...\n",
      "Request 14 failed: HTTPSConnectionPool(host='1029620071644790784.europe-west4-87995179092.prediction.vertexai.goog', po...\n",
      "Completed 25/500 requests... Success rate: 64.0%\n",
      "Completed 50/500 requests... Success rate: 82.0%\n",
      "Completed 75/500 requests... Success rate: 88.0%\n",
      "Completed 100/500 requests... Success rate: 91.0%\n",
      "Completed 125/500 requests... Success rate: 92.8%\n",
      "Completed 150/500 requests... Success rate: 94.0%\n",
      "Completed 175/500 requests... Success rate: 94.9%\n",
      "Completed 200/500 requests... Success rate: 95.5%\n",
      "Completed 225/500 requests... Success rate: 96.0%\n",
      "Completed 250/500 requests... Success rate: 96.4%\n",
      "Completed 275/500 requests... Success rate: 96.7%\n",
      "Completed 300/500 requests... Success rate: 97.0%\n",
      "Completed 325/500 requests... Success rate: 97.2%\n",
      "Completed 350/500 requests... Success rate: 97.4%\n",
      "Completed 375/500 requests... Success rate: 97.6%\n",
      "Completed 400/500 requests... Success rate: 97.8%\n",
      "Completed 425/500 requests... Success rate: 97.9%\n",
      "Completed 450/500 requests... Success rate: 98.0%\n",
      "Completed 475/500 requests... Success rate: 98.1%\n",
      "Completed 500/500 requests... Success rate: 98.2%\n",
      "\n",
      "============================================================\n",
      "LOAD TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Summary:\n",
      "- Total requests: 500\n",
      "- Successful requests: 491\n",
      "- Failed requests: 9\n",
      "- Success rate: 98.2%\n",
      "- Total test time: 112.2 seconds\n",
      "\n",
      "Latency Metrics:\n",
      "- TTFT (p50): 0.500s\n",
      "- TTFT (p95): 0.500s\n",
      "- Inter-token Latency (p95): 0.179s\n",
      "- End-to-End (p95): 57.2s\n",
      "\n",
      "Throughput Metrics:\n",
      "- Token Output Throughput: 1523.49 tok/sec\n",
      "- Overall Token Throughput: 1882.01 tok/sec\n",
      "- Requests per second: 4.38 req/sec\n",
      "\n",
      "Token Statistics:\n",
      "- Average input tokens: 81.9\n",
      "- Average output tokens: 348.0\n",
      "\n",
      "============================================================\n",
      "ERROR ANALYSIS\n",
      "============================================================\n",
      "- ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')): 6 occurrences\n",
      "- HTTPSConnectionPool(host='1029620071644790784.europe-west4-87995179092.prediction.vertexai.goog', po...: 3 occurrences\n",
      "\n",
      "Saving results...\n",
      "\n",
      "============================================================\n",
      "FILES SAVED\n",
      "============================================================\n",
      "📄 Summary: vllm_performance_tests/vllm_test_20250707_235228.csv\n",
      "📊 Details: vllm_performance_tests/vllm_detailed_20250707_235228.csv\n",
      "📝 Report: vllm_performance_tests/vllm_report_20250707_235228.md\n",
      "\n",
      "📊 Partial results saved. Success rate: 98.2%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Test configuration - REDUCED for testing\n",
    "TEST_CONFIG = {\n",
    "    'concurrent_users': 250,      # Start small to test endpoint stability\n",
    "    'total_requests': 500,       # Reduce for initial testing\n",
    "    'input_token_length': 265,  \n",
    "    'output_tokens': 317,       \n",
    "    'temperature': 0.7,\n",
    "    'top_p': 1.0,\n",
    "    'max_tokens': 350,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "# Create timestamped filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = \"vllm_performance_tests\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"{output_dir}/vllm_test_{timestamp}.csv\"\n",
    "detailed_csv_filename = f\"{output_dir}/vllm_detailed_{timestamp}.csv\"\n",
    "md_filename = f\"{output_dir}/vllm_report_{timestamp}.md\"\n",
    "\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output files will be saved as:\")\n",
    "print(f\"- Summary CSV: {csv_filename}\")\n",
    "print(f\"- Detailed CSV: {detailed_csv_filename}\")\n",
    "print(f\"- Report MD: {md_filename}\")\n",
    "\n",
    "# Generate test prompts\n",
    "def generate_test_prompt(target_tokens=265):\n",
    "    base_prompt = \"\"\"Analyze the following business scenario and provide recommendations:\n",
    "\n",
    "A technology startup is developing an AI-powered customer service platform. They need to understand market positioning, competitive analysis, implementation strategy, and growth projections. Consider technical requirements, user experience design, scalability concerns, and business model validation.\n",
    "\n",
    "Please provide strategic insights covering market analysis, technical architecture, user acquisition strategies, and financial projections for the next 24 months.\"\"\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Metrics collection\n",
    "metrics = {\n",
    "    'ttft_times': [],\n",
    "    'inter_token_latencies': [],\n",
    "    'end_to_end_times': [],\n",
    "    'input_tokens': [],\n",
    "    'output_tokens': [],\n",
    "    'request_errors': [],\n",
    "    'timestamps': []\n",
    "}\n",
    "\n",
    "metrics_lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id, prompt, config):\n",
    "    \"\"\"Single request function with enhanced error handling\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Prepare request with timeout handling\n",
    "        instances = [{\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": config['max_tokens'],\n",
    "            \"temperature\": config['temperature'],\n",
    "            \"top_p\": config.get('top_p', 1.0),\n",
    "            \"raw_response\": True,\n",
    "        }]\n",
    "        \n",
    "        request_start = time.time()\n",
    "        \n",
    "        # Add retry logic for 502 errors\n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                response = endpoints[\"vllmtpu\"].predict(\n",
    "                    instances=instances, \n",
    "                    use_dedicated_endpoint=use_dedicated_endpoint\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                if \"502\" in str(e) and attempt < max_retries:\n",
    "                    print(f\"Request {request_id}: 502 error, retrying ({attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(1)  # Brief delay before retry\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e  # Re-raise if not 502 or out of retries\n",
    "        \n",
    "        request_end = time.time()\n",
    "        \n",
    "        # Parse response safely\n",
    "        prediction = {}\n",
    "        output_text = \"\"\n",
    "        \n",
    "        if hasattr(response, 'predictions') and response.predictions:\n",
    "            prediction = response.predictions[0] if response.predictions else {}\n",
    "            if isinstance(prediction, dict):\n",
    "                output_text = prediction.get('generated_text', '') or prediction.get('content', '') or str(prediction)\n",
    "            else:\n",
    "                output_text = str(prediction)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_to_end_time = request_end - request_start\n",
    "        \n",
    "        # Estimate tokens\n",
    "        input_tokens = len(prompt.split()) * 1.3\n",
    "        output_tokens = len(output_text.split()) * 1.3 if output_text else 0\n",
    "        \n",
    "        # Estimate timing metrics\n",
    "        estimated_ttft = min(0.5, end_to_end_time * 0.02) if end_to_end_time > 0 else 0\n",
    "        estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens) if output_tokens > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        with metrics_lock:\n",
    "            metrics['ttft_times'].append(estimated_ttft)\n",
    "            metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "            metrics['end_to_end_times'].append(end_to_end_time)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['timestamps'].append(request_start)\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'timestamp': request_start,\n",
    "            'end_to_end_time': end_to_end_time,\n",
    "            'ttft': estimated_ttft,\n",
    "            'inter_token_latency': estimated_inter_token,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'output_length': len(output_text),\n",
    "            'prompt_length': len(prompt),\n",
    "            'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        with metrics_lock:\n",
    "            metrics['request_errors'].append({\n",
    "                'request_id': request_id,\n",
    "                'error': error_msg,\n",
    "                'time': error_time\n",
    "            })\n",
    "        \n",
    "        print(f\"Request {request_id} failed: {error_msg[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'timestamp': start_time,\n",
    "            'error': error_msg,\n",
    "            'time': error_time,\n",
    "            'end_to_end_time': error_time,\n",
    "            'ttft': 0,\n",
    "            'inter_token_latency': 0,\n",
    "            'input_tokens': len(prompt.split()) * 1.3 if prompt else 0,\n",
    "            'output_tokens': 0,\n",
    "            'output_length': 0,\n",
    "            'prompt_length': len(prompt) if prompt else 0,\n",
    "            'output_text': \"\"\n",
    "        }\n",
    "\n",
    "# Test endpoint first with a single request\n",
    "print(\"Testing endpoint with single request first...\")\n",
    "test_prompt = generate_test_prompt()\n",
    "\n",
    "try:\n",
    "    single_test = make_request(0, test_prompt, TEST_CONFIG)\n",
    "    if single_test['success']:\n",
    "        print(\"✅ Single request test successful!\")\n",
    "        print(f\"Response time: {single_test['end_to_end_time']:.2f}s\")\n",
    "        print(f\"Output length: {single_test['output_length']} chars\")\n",
    "    else:\n",
    "        print(\"❌ Single request test failed!\")\n",
    "        print(f\"Error: {single_test['error']}\")\n",
    "        print(\"\\n🛑 Endpoint appears to have issues. Consider:\")\n",
    "        print(\"1. Check if the endpoint is properly deployed and running\")\n",
    "        print(\"2. Verify the endpoint has sufficient resources\")\n",
    "        print(\"3. Test with smaller requests first\")\n",
    "        print(\"4. Check Google Cloud Console for endpoint logs\")\n",
    "        \n",
    "        # Still proceed but with warning\n",
    "        input(\"\\nPress Enter to continue with load test anyway, or Ctrl+C to abort...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Critical error during single test: {e}\")\n",
    "    print(\"Aborting load test.\")\n",
    "    exit(1)\n",
    "\n",
    "# Generate test prompts\n",
    "print(f\"Generating {TEST_CONFIG['total_requests']} test prompts...\")\n",
    "test_prompts = [generate_test_prompt(TEST_CONFIG['input_token_length']) \n",
    "                for _ in range(TEST_CONFIG['total_requests'])]\n",
    "\n",
    "print(f\"Generated {len(test_prompts)} test prompts\")\n",
    "print(f\"Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "\n",
    "# Run load test\n",
    "print(f\"\\nStarting load test:\")\n",
    "print(f\"- Concurrent users: {TEST_CONFIG['concurrent_users']}\")\n",
    "print(f\"- Total requests: {TEST_CONFIG['total_requests']}\")\n",
    "print(f\"- Target output tokens: {TEST_CONFIG['output_tokens']}\")\n",
    "\n",
    "test_start_time = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=TEST_CONFIG['concurrent_users']) as executor:\n",
    "    future_to_id = {\n",
    "        executor.submit(make_request, i, test_prompts[i % len(test_prompts)], TEST_CONFIG): i \n",
    "        for i in range(TEST_CONFIG['total_requests'])\n",
    "    }\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_id):\n",
    "        request_id = future_to_id[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e),\n",
    "                'end_to_end_time': 0,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': 0,\n",
    "                'output_text': \"\"\n",
    "            })\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % max(1, TEST_CONFIG['total_requests'] // 20) == 0:\n",
    "            success_rate = len([r for r in results if r.get('success', False)]) / len(results) * 100\n",
    "            print(f\"Completed {completed}/{TEST_CONFIG['total_requests']} requests... Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "test_end_time = time.time()\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "# Calculate performance metrics with safe variable handling\n",
    "successful_requests = [r for r in results if r.get('success', False)]\n",
    "failed_requests = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LOAD TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"- Total requests: {len(results)}\")\n",
    "print(f\"- Successful requests: {len(successful_requests)}\")\n",
    "print(f\"- Failed requests: {len(failed_requests)}\")\n",
    "print(f\"- Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "print(f\"- Total test time: {total_test_time:.1f} seconds\")\n",
    "\n",
    "# Initialize all variables to prevent NameError\n",
    "ttft_times = []\n",
    "inter_token_times = []\n",
    "e2e_times = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "ttft_p50 = ttft_p95 = ttft_p99 = 0\n",
    "inter_token_p50 = inter_token_p95 = 0\n",
    "e2e_p50 = e2e_p95 = e2e_p99 = 0\n",
    "token_output_throughput = overall_token_throughput = requests_per_second = 0\n",
    "total_input_tokens = total_output_tokens = 0\n",
    "\n",
    "# Calculate metrics only if we have successful requests\n",
    "if successful_requests:\n",
    "    ttft_times = [r['ttft'] for r in successful_requests]\n",
    "    inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "    e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "    input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "    output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "    \n",
    "    def percentile(data, p):\n",
    "        return np.percentile(data, p) if data else 0\n",
    "    \n",
    "    ttft_p50 = percentile(ttft_times, 50)\n",
    "    ttft_p95 = percentile(ttft_times, 95)\n",
    "    ttft_p99 = percentile(ttft_times, 99)\n",
    "    inter_token_p50 = percentile(inter_token_times, 50)\n",
    "    inter_token_p95 = percentile(inter_token_times, 95)\n",
    "    e2e_p50 = percentile(e2e_times, 50)\n",
    "    e2e_p95 = percentile(e2e_times, 95)\n",
    "    e2e_p99 = percentile(e2e_times, 99)\n",
    "    \n",
    "    total_output_tokens = sum(output_tokens)\n",
    "    total_input_tokens = sum(input_tokens)\n",
    "    total_tokens = total_output_tokens + total_input_tokens\n",
    "    \n",
    "    token_output_throughput = total_output_tokens / total_test_time\n",
    "    overall_token_throughput = total_tokens / total_test_time\n",
    "    requests_per_second = len(successful_requests) / total_test_time\n",
    "    \n",
    "    print(f\"\\nLatency Metrics:\")\n",
    "    print(f\"- TTFT (p50): {ttft_p50:.3f}s\")\n",
    "    print(f\"- TTFT (p95): {ttft_p95:.3f}s\")\n",
    "    print(f\"- Inter-token Latency (p95): {inter_token_p95:.3f}s\")\n",
    "    print(f\"- End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"- Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "    print(f\"- Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "    \n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"- Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "    print(f\"- Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n❌ NO SUCCESSFUL REQUESTS - ENDPOINT ISSUES DETECTED\")\n",
    "    print(f\"\\n🔍 TROUBLESHOOTING RECOMMENDATIONS:\")\n",
    "    print(f\"1. Check endpoint status in Google Cloud Console\")\n",
    "    print(f\"2. Verify endpoint has sufficient resources allocated\")\n",
    "    print(f\"3. Check for quota limits or rate limiting\")\n",
    "    print(f\"4. Review endpoint logs for detailed error messages\")\n",
    "    print(f\"5. Try reducing concurrent users and request size\")\n",
    "\n",
    "# Error analysis\n",
    "if failed_requests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ERROR ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    error_types = defaultdict(int)\n",
    "    for req in failed_requests:\n",
    "        error_msg = req.get('error', 'Unknown error')\n",
    "        # Truncate long error messages\n",
    "        error_key = error_msg[:100] + \"...\" if len(error_msg) > 100 else error_msg\n",
    "        error_types[error_key] += 1\n",
    "    \n",
    "    for error, count in list(error_types.items())[:10]:  # Show top 10 errors\n",
    "        print(f\"- {error}: {count} occurrences\")\n",
    "\n",
    "# Save detailed results\n",
    "print(f\"\\nSaving results...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(detailed_csv_filename, index=False)\n",
    "\n",
    "# Create summary with safe variable access\n",
    "summary_data = {\n",
    "    'timestamp': [timestamp],\n",
    "    'test_duration_seconds': [total_test_time],\n",
    "    'total_requests': [len(results)],\n",
    "    'successful_requests': [len(successful_requests)],\n",
    "    'failed_requests': [len(failed_requests)],\n",
    "    'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "    'concurrent_users': [TEST_CONFIG['concurrent_users']],\n",
    "    'ttft_p95_seconds': [ttft_p95],\n",
    "    'inter_token_p95_seconds': [inter_token_p95],\n",
    "    'e2e_p95_seconds': [e2e_p95],\n",
    "    'token_output_throughput': [token_output_throughput],\n",
    "    'overall_token_throughput': [overall_token_throughput],\n",
    "    'requests_per_second': [requests_per_second],\n",
    "    'avg_input_tokens': [statistics.mean(input_tokens) if input_tokens else 0],\n",
    "    'avg_output_tokens': [statistics.mean(output_tokens) if output_tokens else 0],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Generate markdown report\n",
    "md_content = f\"\"\"# vLLM Performance Test Report - {timestamp}\n",
    "\n",
    "**Test Status:** {'✅ PARTIAL SUCCESS' if successful_requests else '❌ FAILED'}  \n",
    "**Success Rate:** {len(successful_requests)/len(results)*100:.1f}%  \n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Issues Detected\n",
    "\n",
    "⚠️ **Endpoint returned 502 errors** - Backend service unavailable  \n",
    "⚠️ **{len(failed_requests)} out of {len(results)} requests failed**\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Check endpoint health** in Google Cloud Console\n",
    "2. **Scale up resources** if endpoint is under-provisioned\n",
    "3. **Implement retry logic** for production applications\n",
    "4. **Monitor endpoint logs** for detailed error information\n",
    "5. **Start with smaller load** to test stability\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if successful_requests:\n",
    "    md_content += f\"\"\"\n",
    "## Performance Results (Successful Requests Only)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| TTFT (p95) | {ttft_p95:.3f}s |\n",
    "| Inter-token (p95) | {inter_token_p95:.3f}s |\n",
    "| End-to-End (p95) | {e2e_p95:.1f}s |\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Requests/sec | {requests_per_second:.2f} |\n",
    "\"\"\"\n",
    "\n",
    "md_content += f\"\"\"\n",
    "## Error Summary\n",
    "\n",
    "| Error Type | Count |\n",
    "|------------|-------|\n",
    "\"\"\"\n",
    "\n",
    "error_types = defaultdict(int)\n",
    "for req in failed_requests:\n",
    "    error_msg = req.get('error', 'Unknown error')\n",
    "    error_key = error_msg[:50] + \"...\" if len(error_msg) > 50 else error_msg\n",
    "    error_types[error_key] += 1\n",
    "\n",
    "for error, count in list(error_types.items())[:5]:\n",
    "    md_content += f\"| {error} | {count} |\\n\"\n",
    "\n",
    "# Save markdown\n",
    "with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FILES SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📄 Summary: {csv_filename}\")\n",
    "print(f\"📊 Details: {detailed_csv_filename}\")\n",
    "print(f\"📝 Report: {md_filename}\")\n",
    "\n",
    "if len(successful_requests) == 0:\n",
    "    print(f\"\\n🚨 CRITICAL: All requests failed. Check your endpoint!\")\n",
    "else:\n",
    "    print(f\"\\n📊 Partial results saved. Success rate: {len(successful_requests)/len(results)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using chat complitions API (adds some inputs ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_id = \"1029620071644790784\"\n",
    "# LOCATION=\"europe-west4\",\n",
    "\n",
    "# client_options = {\"api_endpoint\": api_endpoint}\n",
    "\n",
    "# client = aiplatform.gapic.PredictionServiceClient(\n",
    "#   client_options=client_options\n",
    "# )\n",
    "    \n",
    "# endpoint = client.endpoint_path(\n",
    "#   project=PROJECT_ID, location=LOCATION, endpoint=endpoint_id\n",
    "# )\n",
    "# response = client.predict(\n",
    "#   endpoint=endpoint, instances=instances, parameters=parameters\n",
    "# )\n",
    "# print(\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple types of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 TPU Benchmark Suite loaded successfully!\n",
      "💡 Run list_available_scenarios() to see all test scenarios\n",
      "🚀 Run run_benchmark_example() to see usage instructions\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Test scenarios configuration\n",
    "TEST_SCENARIOS = {\n",
    "    'email_generation': {\n",
    "        'name': 'Email Generation',\n",
    "        'description': 'Short prompts generating long emails',\n",
    "        'concurrent_users': 100,\n",
    "        'total_requests': 2000,\n",
    "        'input_token_length': 80,\n",
    "        'output_tokens': 350,\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 400,\n",
    "        'raw_response': True  # Match your parameter structure\n",
    "    },\n",
    "    'summarization': {\n",
    "        'name': 'Text Summarization',\n",
    "        'description': 'Long text summarization to concise output',\n",
    "        'concurrent_users': 150,\n",
    "        'total_requests': 3000,\n",
    "        'input_token_length': 500,\n",
    "        'output_tokens': 80,\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 120,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'rewrite_small': {\n",
    "        'name': 'Content Rewriting (Small)',\n",
    "        'description': 'Small content rewriting and improvement',\n",
    "        'concurrent_users': 200,\n",
    "        'total_requests': 4000,\n",
    "        'input_token_length': 250,\n",
    "        'output_tokens': 250,\n",
    "        'temperature': 0.5,\n",
    "        'max_tokens': 300,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'rewrite_large': {\n",
    "        'name': 'Content Rewriting (Large)',\n",
    "        'description': 'Large document rewriting and enhancement',\n",
    "        'concurrent_users': 80,\n",
    "        'total_requests': 1500,\n",
    "        'input_token_length': 1000,\n",
    "        'output_tokens': 1000,\n",
    "        'temperature': 0.4,\n",
    "        'max_tokens': 1200,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'code_generation': {\n",
    "        'name': 'Code Generation',\n",
    "        'description': 'Code generation from specifications',\n",
    "        'concurrent_users': 120,\n",
    "        'total_requests': 2500,\n",
    "        'input_token_length': 200,\n",
    "        'output_tokens': 400,\n",
    "        'temperature': 0.2,\n",
    "        'max_tokens': 500,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'conversation': {\n",
    "        'name': 'Conversational AI',\n",
    "        'description': 'Multi-turn conversation simulation',\n",
    "        'concurrent_users': 180,\n",
    "        'total_requests': 5000,\n",
    "        'input_token_length': 150,\n",
    "        'output_tokens': 200,\n",
    "        'temperature': 0.8,\n",
    "        'max_tokens': 250,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'qa_long_context': {\n",
    "        'name': 'Long Context Q&A',\n",
    "        'description': 'Question answering with long context',\n",
    "        'concurrent_users': 60,\n",
    "        'total_requests': 1000,\n",
    "        'input_token_length': 1500,\n",
    "        'output_tokens': 300,\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 400,\n",
    "        'raw_response': True\n",
    "    },\n",
    "    'creative_writing': {\n",
    "        'name': 'Creative Writing',\n",
    "        'description': 'Story and creative content generation',\n",
    "        'concurrent_users': 100,\n",
    "        'total_requests': 2000,\n",
    "        'input_token_length': 120,\n",
    "        'output_tokens': 600,\n",
    "        'temperature': 0.9,\n",
    "        'max_tokens': 700,\n",
    "        'raw_response': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prompt generators for different scenarios\n",
    "def generate_email_prompt(target_tokens=80):\n",
    "    \"\"\"Generate email generation prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"Write a professional follow-up email to a client who hasn't responded to our proposal in 2 weeks. Include next steps and maintain a friendly tone.\",\n",
    "        \"Create a welcome email for new employees joining our tech startup. Include company culture, first-day instructions, and contact information.\",\n",
    "        \"Draft an apology email to customers about a service outage that lasted 3 hours. Explain what happened and our prevention measures.\",\n",
    "        \"Write a promotional email for our new AI software product launch. Highlight key features and include a special discount offer.\",\n",
    "        \"Create a meeting request email for quarterly business review with key stakeholders. Include agenda items and time options.\",\n",
    "        \"Draft a thank you email to conference speakers and sponsors after successful event completion.\",\n",
    "        \"Write a customer onboarding email explaining how to get started with our platform and available resources.\",\n",
    "        \"Create a re-engagement email for inactive users offering special incentives to return to our service.\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "def generate_summarization_prompt(target_tokens=500):\n",
    "    \"\"\"Generate summarization prompts with long content\"\"\"\n",
    "    base_content = \"\"\"\n",
    "    In today's rapidly evolving business landscape, companies are increasingly turning to artificial intelligence and machine learning technologies to gain competitive advantages and streamline their operations. The implementation of AI systems has become a critical factor in determining organizational success across various industries, from healthcare and finance to retail and manufacturing.\n",
    "\n",
    "    The adoption of AI technologies brings numerous benefits including improved efficiency, enhanced decision-making capabilities, cost reduction, and the ability to process vast amounts of data in real-time. However, organizations also face significant challenges when implementing these systems, including data privacy concerns, integration complexities, workforce adaptation requirements, and substantial initial investment costs.\n",
    "\n",
    "    Recent studies indicate that companies successfully implementing AI solutions report an average increase in productivity of 40% and cost savings of up to 30% within the first two years of deployment. These improvements are primarily attributed to automation of repetitive tasks, enhanced predictive analytics, and improved customer service through chatbots and virtual assistants.\n",
    "\n",
    "    The key to successful AI implementation lies in developing a comprehensive strategy that addresses technical requirements, organizational readiness, and change management processes. Companies must invest in employee training, establish clear governance frameworks, and ensure compliance with relevant regulations and ethical guidelines.\n",
    "\n",
    "    Looking ahead, the future of AI in business appears promising, with emerging technologies such as generative AI, computer vision, and natural language processing offering new opportunities for innovation and growth. Organizations that proactively embrace these technologies while addressing associated challenges will be better positioned to thrive in the digital economy.\n",
    "\n",
    "    The integration of AI systems also requires careful consideration of data quality, security measures, and ongoing maintenance requirements. Companies must establish robust data management practices, implement appropriate security protocols, and develop sustainable support structures to ensure long-term success of their AI initiatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extend content to reach target tokens\n",
    "    extended_content = base_content\n",
    "    while len(extended_content.split()) < target_tokens * 0.75:\n",
    "        extended_content += \" \" + base_content\n",
    "    \n",
    "    return f\"Please provide a concise summary of the following business analysis:\\n\\n{extended_content[:int(target_tokens*4)]}\\n\\nSummary:\"\n",
    "\n",
    "def generate_rewrite_prompt(target_tokens=250):\n",
    "    \"\"\"Generate content rewriting prompts\"\"\"\n",
    "    if target_tokens <= 300:\n",
    "        content = \"\"\"\n",
    "        Our company has been working on developing new software solutions for the past several years. We have created various applications that help businesses manage their operations more effectively. The software includes features for inventory management, customer relationship management, and financial reporting.\n",
    "\n",
    "        The development team consists of experienced programmers who use modern programming languages and frameworks. We follow agile development methodologies to ensure quick delivery of high-quality software products. Our testing procedures include both automated and manual testing to identify and fix any issues before deployment.\n",
    "\n",
    "        Customer feedback has been very positive, with many users praising the user-friendly interface and robust functionality. We continue to add new features based on user requests and market demands. Our support team provides excellent customer service to help users maximize the benefits of our software solutions.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        content = \"\"\"\n",
    "        Our organization has been dedicated to the development and deployment of innovative software solutions for the past decade, establishing ourselves as a leader in the enterprise software market. We have successfully created and launched a comprehensive suite of applications designed to help businesses across various industries manage their complex operations more effectively and efficiently.\n",
    "\n",
    "        The software portfolio includes advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is built with scalability in mind, allowing businesses to grow without worrying about system limitations.\n",
    "\n",
    "        Our development team consists of highly experienced software engineers, architects, and designers who utilize cutting-edge programming languages, frameworks, and development tools. We follow industry best practices including agile development methodologies, continuous integration, and deployment practices to ensure rapid delivery of high-quality software products that meet the evolving needs of our clients.\n",
    "\n",
    "        Our comprehensive testing procedures include automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing to identify and resolve any issues before deployment. We maintain strict quality assurance standards throughout the development lifecycle.\n",
    "\n",
    "        Customer feedback has been overwhelmingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set. We continuously gather user feedback and market intelligence to guide our product development roadmap.\n",
    "\n",
    "        We maintain a dedicated customer support team that provides exceptional service through multiple channels including phone, email, chat, and an extensive knowledge base. Our support team helps users maximize the benefits of our software solutions and ensures smooth implementation and adoption.\n",
    "        \"\"\"\n",
    "    \n",
    "    return f\"Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n{content}\\n\\nImproved version:\"\n",
    "\n",
    "def generate_code_prompt(target_tokens=200):\n",
    "    \"\"\"Generate code generation prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"Create a Python function that implements a binary search algorithm with error handling, type hints, and comprehensive documentation. Include unit tests.\",\n",
    "        \"Write a JavaScript React component for a responsive navigation bar with dropdown menus, mobile hamburger menu, and smooth animations.\",\n",
    "        \"Develop a SQL query to analyze customer purchase patterns including total spend, frequency, and product categories with performance optimization.\",\n",
    "        \"Create a Python class for managing database connections with connection pooling, error handling, and transaction management.\",\n",
    "        \"Write a REST API endpoint in Python Flask for user authentication with JWT tokens, rate limiting, and input validation.\",\n",
    "        \"Implement a sorting algorithm visualization in JavaScript with HTML5 Canvas showing step-by-step execution and performance metrics.\",\n",
    "        \"Create a data validation function in TypeScript for form inputs with custom error messages and real-time validation feedback.\",\n",
    "        \"Write a Python script for automated testing of API endpoints with comprehensive test cases and detailed reporting.\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "def generate_conversation_prompt(target_tokens=150):\n",
    "    \"\"\"Generate conversational prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"I'm planning a career change from marketing to data science. Can you help me understand the key skills I need to develop and create a learning roadmap?\",\n",
    "        \"I'm having trouble with my team's productivity. We're missing deadlines and communication seems poor. What strategies would you recommend?\",\n",
    "        \"I want to start a small business selling handmade crafts online. Can you guide me through the essential steps and considerations?\",\n",
    "        \"I'm preparing for a job interview for a senior management position. What questions should I expect and how should I prepare?\",\n",
    "        \"I need to improve my public speaking skills for upcoming presentations. Can you provide practical tips and practice exercises?\",\n",
    "        \"I'm considering investing in renewable energy stocks. What factors should I consider and what are the current market trends?\",\n",
    "        \"I want to learn a new programming language to advance my career. Which language would you recommend and why?\",\n",
    "        \"I'm struggling with work-life balance as a remote worker. Can you suggest strategies to maintain productivity and well-being?\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "def generate_qa_long_context_prompt(target_tokens=1500):\n",
    "    \"\"\"Generate Q&A prompts with long context\"\"\"\n",
    "    context = \"\"\"\n",
    "    The history of artificial intelligence dates back to ancient times, with myths and stories of artificial beings endowed with intelligence or consciousness by master craftsmen. The formal field of AI research was founded at a conference at Dartmouth College in 1956, where the term \"artificial intelligence\" was coined.\n",
    "\n",
    "    Early AI research focused on problem-solving and symbolic methods. In the 1960s, the US Department of Defense took interest in this type of work and began training computers to mimic basic human reasoning. This early work paved the way for the automation and formal reasoning that we see in computers today.\n",
    "\n",
    "    The field experienced several boom and bust cycles, known as \"AI winters,\" when funding and interest waned due to overinflated expectations and limited practical applications. However, the field has experienced a renaissance since the 2000s, driven by advances in machine learning, particularly deep learning, and the availability of large datasets and powerful computing resources.\n",
    "\n",
    "    Machine learning, a subset of AI, involves training algorithms on data to make predictions or decisions without being explicitly programmed for every scenario. Deep learning, a subset of machine learning, uses neural networks with multiple layers to model and understand complex patterns in data.\n",
    "\n",
    "    The current wave of AI advancement is characterized by breakthrough applications in computer vision, natural language processing, speech recognition, and game playing. Notable achievements include IBM's Deep Blue defeating world chess champion Garry Kasparov in 1997, IBM's Watson winning at Jeopardy! in 2011, and Google's AlphaGo defeating the world champion Go player in 2016.\n",
    "\n",
    "    Modern AI applications are ubiquitous in our daily lives, from recommendation systems on streaming platforms and e-commerce sites to virtual assistants like Siri and Alexa, autonomous vehicles, and medical diagnosis tools. The technology continues to advance rapidly, with new breakthroughs in generative AI, large language models, and multimodal AI systems.\n",
    "\n",
    "    The development of AI raises important ethical and societal questions about privacy, job displacement, bias in algorithms, and the concentration of power among tech companies. There are ongoing debates about AI governance, safety, and the need for regulation to ensure AI benefits humanity while minimizing risks.\n",
    "\n",
    "    Looking to the future, AI is expected to continue advancing rapidly, with potential developments in artificial general intelligence (AGI) that could match or exceed human intelligence across all domains. This prospect brings both tremendous opportunities and significant challenges that society must address.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extend context to reach target tokens\n",
    "    extended_context = context\n",
    "    while len(extended_context.split()) < target_tokens * 0.75:\n",
    "        extended_context += \" \" + context\n",
    "    \n",
    "    questions = [\n",
    "        \"Based on the provided context, what were the key factors that led to the AI winters, and how did the field recover?\",\n",
    "        \"Explain the relationship between artificial intelligence, machine learning, and deep learning as described in the context.\",\n",
    "        \"What are the major ethical and societal concerns raised by AI development according to the passage?\",\n",
    "        \"Describe the evolution of AI from its early days to modern applications, highlighting major milestones.\",\n",
    "        \"What role did computing power and data availability play in the recent AI renaissance?\"\n",
    "    ]\n",
    "    \n",
    "    question = questions[hash(str(target_tokens)) % len(questions)]\n",
    "    return f\"Context:\\n{extended_context[:int(target_tokens*4)]}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "def generate_creative_writing_prompt(target_tokens=120):\n",
    "    \"\"\"Generate creative writing prompts\"\"\"\n",
    "    prompts = [\n",
    "        \"Write a short story about a time traveler who discovers that changing the past has unexpected consequences in the present.\",\n",
    "        \"Create a dramatic monologue from the perspective of the last tree in a deforested world speaking to humanity.\",\n",
    "        \"Write a humorous story about a superhero whose power is the ability to make anyone laugh uncontrollably at inappropriate times.\",\n",
    "        \"Compose a mystery story where the detective realizes they are actually the criminal they've been hunting.\",\n",
    "        \"Write a science fiction story about first contact between humans and an alien species that communicates through colors.\",\n",
    "        \"Create a fantasy tale about a young wizard who discovers their magic only works when they're telling the truth.\",\n",
    "        \"Write a story about a librarian who discovers that books in their library can transport readers into the stories.\",\n",
    "        \"Compose a thriller about a person who receives messages from their future self warning about upcoming dangers.\"\n",
    "    ]\n",
    "    return prompts[hash(str(target_tokens)) % len(prompts)]\n",
    "\n",
    "# Prompt generator mapping\n",
    "PROMPT_GENERATORS = {\n",
    "    'email_generation': generate_email_prompt,\n",
    "    'summarization': generate_summarization_prompt,\n",
    "    'rewrite_small': generate_rewrite_prompt,\n",
    "    'rewrite_large': generate_rewrite_prompt,\n",
    "    'code_generation': generate_code_prompt,\n",
    "    'conversation': generate_conversation_prompt,\n",
    "    'qa_long_context': generate_qa_long_context_prompt,\n",
    "    'creative_writing': generate_creative_writing_prompt\n",
    "}\n",
    "\n",
    "class TPUBenchmarkSuite:\n",
    "    def __init__(self, endpoint, use_dedicated_endpoint=True):\n",
    "        self.endpoint = endpoint  # Single endpoint object, not a dictionary\n",
    "        self.use_dedicated_endpoint = use_dedicated_endpoint\n",
    "        self.metrics = {}\n",
    "        self.metrics_lock = threading.Lock()\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset metrics for new test\"\"\"\n",
    "        self.metrics = {\n",
    "            'ttft_times': [],\n",
    "            'inter_token_latencies': [],\n",
    "            'end_to_end_times': [],\n",
    "            'input_tokens': [],\n",
    "            'output_tokens': [],\n",
    "            'request_errors': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "    \n",
    "    def make_request(self, request_id, prompt, config):\n",
    "        \"\"\"Single request function with detailed timing\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Prepare request exactly like your working code\n",
    "            instances = [{\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": config['max_tokens'],\n",
    "                \"temperature\": config['temperature'],\n",
    "                \"raw_response\": config.get('raw_response', True)  # Match your structure\n",
    "            }]\n",
    "            \n",
    "            # Debug: Print first few requests to see what's being sent\n",
    "            if request_id < 3:\n",
    "                print(f\"DEBUG Request {request_id}: {instances[0]}\")\n",
    "            \n",
    "            # Record request start\n",
    "            request_start = time.time()\n",
    "            \n",
    "            # Make prediction using your exact method\n",
    "            response = self.endpoint.predict(\n",
    "                instances=instances, \n",
    "                use_dedicated_endpoint=self.use_dedicated_endpoint\n",
    "            )\n",
    "            \n",
    "            request_end = time.time()\n",
    "            \n",
    "            # Debug: Print first few responses\n",
    "            if request_id < 3:\n",
    "                print(f\"DEBUG Response {request_id}: {response}\")\n",
    "                print(f\"DEBUG Predictions: {response.predictions if hasattr(response, 'predictions') else 'No predictions attr'}\")\n",
    "            \n",
    "            # Parse response like your code\n",
    "            if hasattr(response, 'predictions') and response.predictions:\n",
    "                # Handle both string and dict responses\n",
    "                prediction = response.predictions[0]\n",
    "                if isinstance(prediction, dict):\n",
    "                    output_text = prediction.get('generated_text', '') or prediction.get('output', '') or str(prediction)\n",
    "                else:\n",
    "                    output_text = str(prediction)\n",
    "            else:\n",
    "                output_text = \"\"\n",
    "                if request_id < 3:\n",
    "                    print(f\"DEBUG: No predictions in response for request {request_id}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            end_to_end_time = request_end - request_start\n",
    "            \n",
    "            # Estimate tokens (rough approximation - you can adjust this)\n",
    "            input_tokens = len(prompt.split()) * 1.3\n",
    "            output_tokens = len(output_text.split()) * 1.3\n",
    "            \n",
    "            # Simulate TTFT and inter-token timing (estimates)\n",
    "            estimated_ttft = min(0.5, end_to_end_time * 0.02)\n",
    "            estimated_inter_token = (end_to_end_time - estimated_ttft) / max(1, output_tokens)\n",
    "            \n",
    "            # Store metrics\n",
    "            with self.metrics_lock:\n",
    "                self.metrics['ttft_times'].append(estimated_ttft)\n",
    "                self.metrics['inter_token_latencies'].append(estimated_inter_token)\n",
    "                self.metrics['end_to_end_times'].append(end_to_end_time)\n",
    "                self.metrics['input_tokens'].append(input_tokens)\n",
    "                self.metrics['output_tokens'].append(output_tokens)\n",
    "                self.metrics['timestamps'].append(request_start)\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'success': True,\n",
    "                'timestamp': request_start,\n",
    "                'end_to_end_time': end_to_end_time,\n",
    "                'ttft': estimated_ttft,\n",
    "                'inter_token_latency': estimated_inter_token,\n",
    "                'input_tokens': input_tokens,\n",
    "                'output_tokens': output_tokens,\n",
    "                'output_length': len(output_text),\n",
    "                'prompt_length': len(prompt),\n",
    "                'output_text': output_text[:200] + \"...\" if len(output_text) > 200 else output_text,\n",
    "                'raw_prediction': str(prediction)[:100] + \"...\" if len(str(prediction)) > 100 else str(prediction)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            \n",
    "            # Debug: Print first few errors\n",
    "            if request_id < 5:\n",
    "                print(f\"DEBUG Error {request_id}: {str(e)}\")\n",
    "                import traceback\n",
    "                print(f\"DEBUG Traceback: {traceback.format_exc()}\")\n",
    "            \n",
    "            with self.metrics_lock:\n",
    "                self.metrics['request_errors'].append({\n",
    "                    'request_id': request_id,\n",
    "                    'error': str(e),\n",
    "                    'time': error_time\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'success': False,\n",
    "                'timestamp': start_time,\n",
    "                'error': str(e),\n",
    "                'time': error_time,\n",
    "                'end_to_end_time': error_time,\n",
    "                'ttft': 0,\n",
    "                'inter_token_latency': 0,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'output_length': 0,\n",
    "                'prompt_length': len(prompt),\n",
    "                'output_text': \"\",\n",
    "                'raw_prediction': f\"Error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def run_scenario_test(self, scenario_name):\n",
    "        \"\"\"Run a specific test scenario\"\"\"\n",
    "        if scenario_name not in TEST_SCENARIOS:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario_name}\")\n",
    "        \n",
    "        config = TEST_SCENARIOS[scenario_name]\n",
    "        prompt_generator = PROMPT_GENERATORS[scenario_name]\n",
    "        \n",
    "        # Reset metrics\n",
    "        self.reset_metrics()\n",
    "        \n",
    "        # Create timestamped filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = f\"tpu_benchmark_results_{scenario_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        csv_filename = f\"{output_dir}/{scenario_name}_summary_{timestamp}.csv\"\n",
    "        detailed_csv_filename = f\"{output_dir}/{scenario_name}_detailed_{timestamp}.csv\"\n",
    "        md_filename = f\"{output_dir}/{scenario_name}_report_{timestamp}.md\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🚀 STARTING TEST: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Description: {config['description']}\")\n",
    "        print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # Generate test prompts\n",
    "        print(f\"\\n📝 Generating test prompts...\")\n",
    "        test_prompts = []\n",
    "        for i in range(config['total_requests']):\n",
    "            prompt = prompt_generator(config['input_token_length'])\n",
    "            test_prompts.append(prompt)\n",
    "        \n",
    "        print(f\"✅ Generated {len(test_prompts)} test prompts\")\n",
    "        print(f\"📊 Sample prompt length: {len(test_prompts[0].split())} words\")\n",
    "        print(f\"🎯 Target input tokens: {config['input_token_length']}\")\n",
    "        print(f\"🎯 Target output tokens: {config['output_tokens']}\")\n",
    "        \n",
    "        # Run load test\n",
    "        print(f\"\\n🔥 Starting load test:\")\n",
    "        print(f\"   - Concurrent users: {config['concurrent_users']}\")\n",
    "        print(f\"   - Total requests: {config['total_requests']}\")\n",
    "        print(f\"   - Temperature: {config['temperature']}\")\n",
    "        print(f\"   - Max tokens: {config['max_tokens']}\")\n",
    "        \n",
    "        # Execute test\n",
    "        test_start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=config['concurrent_users']) as executor:\n",
    "            # Submit all requests\n",
    "            future_to_id = {\n",
    "                executor.submit(self.make_request, i, test_prompts[i % len(test_prompts)], config): i \n",
    "                for i in range(config['total_requests'])\n",
    "            }\n",
    "            \n",
    "            # Collect results with progress tracking\n",
    "            completed = 0\n",
    "            for future in as_completed(future_to_id):\n",
    "                request_id = future_to_id[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        'request_id': request_id,\n",
    "                        'success': False,\n",
    "                        'timestamp': time.time(),\n",
    "                        'error': str(e),\n",
    "                        'end_to_end_time': 0,\n",
    "                        'ttft': 0,\n",
    "                        'inter_token_latency': 0,\n",
    "                        'input_tokens': 0,\n",
    "                        'output_tokens': 0,\n",
    "                        'output_length': 0,\n",
    "                        'prompt_length': 0,\n",
    "                        'output_text': \"\"\n",
    "                    })\n",
    "                \n",
    "                completed += 1\n",
    "                if completed % 100 == 0:\n",
    "                    print(f\"   ⚡ Completed {completed}/{config['total_requests']} requests...\")\n",
    "        \n",
    "        test_end_time = time.time()\n",
    "        total_test_time = test_end_time - test_start_time\n",
    "        \n",
    "        # Calculate and display results\n",
    "        self.calculate_and_display_results(results, total_test_time, config, scenario_name)\n",
    "        \n",
    "        # Save results\n",
    "        self.save_results(results, config, scenario_name, timestamp, total_test_time,\n",
    "                         csv_filename, detailed_csv_filename, md_filename)\n",
    "        \n",
    "        print(f\"\\n✅ Test completed successfully!\")\n",
    "        print(f\"📁 Files saved in: {output_dir}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_and_display_results(self, results, total_test_time, config, scenario_name):\n",
    "        \"\"\"Calculate and display test results\"\"\"\n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        failed_requests = [r for r in results if not r.get('success', False)]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 TEST RESULTS: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\n📈 Test Summary:\")\n",
    "        print(f\"   - Total requests: {len(results)}\")\n",
    "        print(f\"   - Successful requests: {len(successful_requests)}\")\n",
    "        print(f\"   - Failed requests: {len(failed_requests)}\")\n",
    "        print(f\"   - Success rate: {len(successful_requests)/len(results)*100:.1f}%\")\n",
    "        print(f\"   - Total test time: {total_test_time:.1f} seconds\")\n",
    "        \n",
    "        if successful_requests:\n",
    "            # Calculate metrics\n",
    "            ttft_times = [r['ttft'] for r in successful_requests]\n",
    "            inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "            e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "            input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "            output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "            \n",
    "            def percentile(data, p):\n",
    "                return np.percentile(data, p)\n",
    "            \n",
    "            # Latency metrics\n",
    "            ttft_p50 = percentile(ttft_times, 50)\n",
    "            ttft_p95 = percentile(ttft_times, 95)\n",
    "            ttft_p99 = percentile(ttft_times, 99)\n",
    "            inter_token_p50 = percentile(inter_token_times, 50)\n",
    "            inter_token_p95 = percentile(inter_token_times, 95)\n",
    "            e2e_p50 = percentile(e2e_times, 50)\n",
    "            e2e_p95 = percentile(e2e_times, 95)\n",
    "            e2e_p99 = percentile(e2e_times, 99)\n",
    "            \n",
    "            # Throughput calculations\n",
    "            total_output_tokens = sum(output_tokens)\n",
    "            total_input_tokens = sum(input_tokens)\n",
    "            total_tokens = total_output_tokens + total_input_tokens\n",
    "            \n",
    "            token_output_throughput = total_output_tokens / total_test_time\n",
    "            overall_token_throughput = total_tokens / total_test_time\n",
    "            requests_per_second = len(successful_requests) / total_test_time\n",
    "            \n",
    "            print(f\"\\n⚡ Latency Metrics:\")\n",
    "            print(f\"   - TTFT (p50): {ttft_p50:.3f}s\")\n",
    "            print(f\"   - TTFT (p95): {ttft_p95:.3f}s\")\n",
    "            print(f\"   - TTFT (p99): {ttft_p99:.3f}s\")\n",
    "            print(f\"   - Inter-token (p50): {inter_token_p50:.3f}s\")\n",
    "            print(f\"   - Inter-token (p95): {inter_token_p95:.3f}s\")\n",
    "            print(f\"   - End-to-End (p50): {e2e_p50:.1f}s\")\n",
    "            print(f\"   - End-to-End (p95): {e2e_p95:.1f}s\")\n",
    "            print(f\"   - End-to-End (p99): {e2e_p99:.1f}s\")\n",
    "            \n",
    "            print(f\"\\n🚀 Throughput Metrics:\")\n",
    "            print(f\"   - Token Output Throughput: {token_output_throughput:.2f} tok/sec\")\n",
    "            print(f\"   - Overall Token Throughput: {overall_token_throughput:.2f} tok/sec\")\n",
    "            print(f\"   - Requests per second: {requests_per_second:.2f} req/sec\")\n",
    "            \n",
    "            print(f\"\\n📊 Token Statistics:\")\n",
    "            print(f\"   - Average input tokens: {statistics.mean(input_tokens):.1f}\")\n",
    "            print(f\"   - Average output tokens: {statistics.mean(output_tokens):.1f}\")\n",
    "            print(f\"   - Total input tokens: {int(total_input_tokens):,}\")\n",
    "            print(f\"   - Total output tokens: {int(total_output_tokens):,}\")\n",
    "    \n",
    "    def save_results(self, results, config, scenario_name, timestamp, total_test_time,\n",
    "                    csv_filename, detailed_csv_filename, md_filename):\n",
    "        \"\"\"Save test results to files\"\"\"\n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        failed_requests = [r for r in results if not r.get('success', False)]\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(detailed_csv_filename, index=False)\n",
    "        \n",
    "        # Create summary data\n",
    "        summary_data = {\n",
    "            'scenario': [scenario_name],\n",
    "            'timestamp': [timestamp],\n",
    "            'test_duration_seconds': [total_test_time],\n",
    "            'total_requests': [len(results)],\n",
    "            'successful_requests': [len(successful_requests)],\n",
    "            'failed_requests': [len(failed_requests)],\n",
    "            'success_rate_percent': [len(successful_requests)/len(results)*100],\n",
    "            'concurrent_users': [config['concurrent_users']],\n",
    "            'target_input_tokens': [config['input_token_length']],\n",
    "            'target_output_tokens': [config['output_tokens']],\n",
    "            'temperature': [config['temperature']],\n",
    "            'max_tokens': [config['max_tokens']]\n",
    "        }\n",
    "        \n",
    "        if successful_requests:\n",
    "            # Calculate metrics for summary\n",
    "            ttft_times = [r['ttft'] for r in successful_requests]\n",
    "            inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "            e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "            input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "            output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "            \n",
    "            summary_data.update({\n",
    "                'ttft_p50_seconds': [np.percentile(ttft_times, 50)],\n",
    "                'ttft_p95_seconds': [np.percentile(ttft_times, 95)],\n",
    "                'ttft_p99_seconds': [np.percentile(ttft_times, 99)],\n",
    "                'inter_token_p50_seconds': [np.percentile(inter_token_times, 50)],\n",
    "                'inter_token_p95_seconds': [np.percentile(inter_token_times, 95)],\n",
    "                'e2e_p50_seconds': [np.percentile(e2e_times, 50)],\n",
    "                'e2e_p95_seconds': [np.percentile(e2e_times, 95)],\n",
    "                'e2e_p99_seconds': [np.percentile(e2e_times, 99)],\n",
    "                'token_output_throughput': [sum(output_tokens) / total_test_time],\n",
    "                'overall_token_throughput': [(sum(output_tokens) + sum(input_tokens)) / total_test_time],\n",
    "                'requests_per_second': [len(successful_requests) / total_test_time],\n",
    "                'avg_input_tokens': [statistics.mean(input_tokens)],\n",
    "                'avg_output_tokens': [statistics.mean(output_tokens)],\n",
    "                'total_input_tokens': [sum(input_tokens)],\n",
    "                'total_output_tokens': [sum(output_tokens)]\n",
    "            })\n",
    "        else:\n",
    "            # Fill with zeros if no successful requests\n",
    "            for key in ['ttft_p50_seconds', 'ttft_p95_seconds', 'ttft_p99_seconds', \n",
    "                        'inter_token_p50_seconds', 'inter_token_p95_seconds',\n",
    "                        'e2e_p50_seconds', 'e2e_p95_seconds', 'e2e_p99_seconds',\n",
    "                        'token_output_throughput', 'overall_token_throughput', \n",
    "                        'requests_per_second', 'avg_input_tokens', 'avg_output_tokens',\n",
    "                        'total_input_tokens', 'total_output_tokens']:\n",
    "                summary_data[key] = [0]\n",
    "        \n",
    "        # Save summary CSV\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        # Generate markdown report\n",
    "        self.generate_markdown_report(results, config, scenario_name, timestamp, \n",
    "                                    total_test_time, md_filename)\n",
    "    \n",
    "    def generate_markdown_report(self, results, config, scenario_name, timestamp, \n",
    "                               total_test_time, md_filename):\n",
    "        \"\"\"Generate detailed markdown report\"\"\"\n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        failed_requests = [r for r in results if not r.get('success', False)]\n",
    "        \n",
    "        md_content = f\"\"\"# TPU Benchmark Report: {config['name']}\n",
    "\n",
    "**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Scenario:** {scenario_name}  \n",
    "**Test Duration:** {total_test_time:.1f} seconds  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## 📋 Test Scenario Description\n",
    "\n",
    "{config['description']}\n",
    "\n",
    "## ⚙️ Test Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Scenario | {config['name']} |\n",
    "| Concurrent Users | {config['concurrent_users']} |\n",
    "| Total Requests | {config['total_requests']} |\n",
    "| Target Input Tokens | {config['input_token_length']} |\n",
    "| Target Output Tokens | {config['output_tokens']} |\n",
    "| Temperature | {config['temperature']} |\n",
    "| Max Tokens | {config['max_tokens']} |\n",
    "| Raw Response | {config['raw_response']} |\n",
    "\n",
    "## 📊 Test Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Requests | {len(results)} |\n",
    "| Successful Requests | {len(successful_requests)} |\n",
    "| Failed Requests | {len(failed_requests)} |\n",
    "| Success Rate | {len(successful_requests)/len(results)*100:.1f}% |\n",
    "| Test Duration | {total_test_time:.1f} seconds |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        if successful_requests:\n",
    "            # Calculate detailed metrics\n",
    "            ttft_times = [r['ttft'] for r in successful_requests]\n",
    "            inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "            e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "            input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "            output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "            \n",
    "            ttft_p50 = np.percentile(ttft_times, 50)\n",
    "            ttft_p95 = np.percentile(ttft_times, 95)\n",
    "            ttft_p99 = np.percentile(ttft_times, 99)\n",
    "            inter_token_p50 = np.percentile(inter_token_times, 50)\n",
    "            inter_token_p95 = np.percentile(inter_token_times, 95)\n",
    "            e2e_p50 = np.percentile(e2e_times, 50)\n",
    "            e2e_p95 = np.percentile(e2e_times, 95)\n",
    "            e2e_p99 = np.percentile(e2e_times, 99)\n",
    "            \n",
    "            total_output_tokens = sum(output_tokens)\n",
    "            total_input_tokens = sum(input_tokens)\n",
    "            token_output_throughput = total_output_tokens / total_test_time\n",
    "            overall_token_throughput = (total_output_tokens + total_input_tokens) / total_test_time\n",
    "            requests_per_second = len(successful_requests) / total_test_time\n",
    "            \n",
    "            md_content += f\"\"\"\n",
    "## ⚡ Latency Metrics\n",
    "\n",
    "| Metric | p50 | p95 | p99 |\n",
    "|--------|-----|-----|-----|\n",
    "| Time to First Token (TTFT) | {ttft_p50:.3f}s | {ttft_p95:.3f}s | {ttft_p99:.3f}s |\n",
    "| Inter-token Latency | {inter_token_p50:.3f}s | {inter_token_p95:.3f}s | - |\n",
    "| End-to-End Latency | {e2e_p50:.1f}s | {e2e_p95:.1f}s | {e2e_p99:.1f}s |\n",
    "\n",
    "## 🚀 Throughput Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Token Output Throughput | {token_output_throughput:.2f} tok/sec |\n",
    "| Overall Token Throughput | {overall_token_throughput:.2f} tok/sec |\n",
    "| Requests per Second | {requests_per_second:.2f} req/sec |\n",
    "\n",
    "## 📊 Token Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Average Input Tokens | {statistics.mean(input_tokens):.1f} |\n",
    "| Average Output Tokens | {statistics.mean(output_tokens):.1f} |\n",
    "| Total Input Tokens | {int(total_input_tokens):,} |\n",
    "| Total Output Tokens | {int(total_output_tokens):,} |\n",
    "| Input/Output Ratio | {statistics.mean(input_tokens)/statistics.mean(output_tokens):.2f} |\n",
    "\n",
    "## 🎯 Target vs Actual Comparison\n",
    "\n",
    "| Metric | Target | Actual | Difference |\n",
    "|--------|--------|--------|------------|\n",
    "| Input Token Length | {config['input_token_length']} | {statistics.mean(input_tokens):.1f} | {((statistics.mean(input_tokens) - config['input_token_length']) / config['input_token_length'] * 100):+.1f}% |\n",
    "| Output Tokens | {config['output_tokens']} | {statistics.mean(output_tokens):.1f} | {((statistics.mean(output_tokens) - config['output_tokens']) / config['output_tokens'] * 100):+.1f}% |\n",
    "\n",
    "## 📈 Performance Analysis\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "            # Performance analysis based on scenario\n",
    "            if scenario_name == 'email_generation':\n",
    "                md_content += f\"\"\"\n",
    "### Email Generation Performance Analysis\n",
    "\n",
    "This scenario tests the model's ability to generate long-form emails from short prompts, simulating common business use cases.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Efficiency Ratio:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.1f}:1 (output:input tokens)\n",
    "- **Content Generation Speed:** {token_output_throughput:.1f} tokens/sec for email content\n",
    "- **Practical Throughput:** Can generate ~{requests_per_second*60:.0f} emails per minute\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if token_output_throughput > 50 else \"⚠️ Moderate\" if token_output_throughput > 20 else \"❌ Poor\"} for real-time email generation\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'summarization':\n",
    "                md_content += f\"\"\"\n",
    "### Summarization Performance Analysis\n",
    "\n",
    "This scenario tests the model's ability to condense large amounts of text into concise summaries.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Compression Ratio:** {statistics.mean(input_tokens)/statistics.mean(output_tokens):.1f}:1 (input:output tokens)\n",
    "- **Processing Speed:** {overall_token_throughput:.1f} total tokens/sec\n",
    "- **Summarization Efficiency:** {requests_per_second:.1f} documents/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if requests_per_second > 2 else \"⚠️ Moderate\" if requests_per_second > 1 else \"❌ Poor\"} for batch document processing\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'rewrite_small':\n",
    "                md_content += f\"\"\"\n",
    "### Small Content Rewriting Performance Analysis\n",
    "\n",
    "This scenario tests balanced input/output rewriting for content improvement.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Balanced Processing:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.2f}:1 ratio\n",
    "- **Rewriting Speed:** {token_output_throughput:.1f} tokens/sec output generation\n",
    "- **Content Throughput:** {requests_per_second:.1f} documents/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if requests_per_second > 1.5 else \"⚠️ Moderate\" if requests_per_second > 0.8 else \"❌ Poor\"} for content editing workflows\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'rewrite_large':\n",
    "                md_content += f\"\"\"\n",
    "### Large Content Rewriting Performance Analysis\n",
    "\n",
    "This scenario tests the model's ability to handle substantial document rewriting tasks.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Large Document Handling:** {statistics.mean(input_tokens):.0f} avg input tokens processed\n",
    "- **Comprehensive Rewriting:** {statistics.mean(output_tokens):.0f} avg output tokens generated\n",
    "- **Processing Efficiency:** {overall_token_throughput:.1f} total tokens/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if overall_token_throughput > 800 else \"⚠️ Moderate\" if overall_token_throughput > 400 else \"❌ Poor\"} for enterprise document processing\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'code_generation':\n",
    "                md_content += f\"\"\"\n",
    "### Code Generation Performance Analysis\n",
    "\n",
    "This scenario tests programming assistance and code generation capabilities.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Code Generation Ratio:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.1f}:1\n",
    "- **Code Output Speed:** {token_output_throughput:.1f} tokens/sec\n",
    "- **Developer Assistance Rate:** {requests_per_second:.1f} code requests/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if token_output_throughput > 40 else \"⚠️ Moderate\" if token_output_throughput > 20 else \"❌ Poor\"} for IDE integration\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'conversation':\n",
    "                md_content += f\"\"\"\n",
    "### Conversational AI Performance Analysis\n",
    "\n",
    "This scenario tests interactive conversation capabilities for chatbot applications.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Response Generation:** {statistics.mean(output_tokens):.0f} avg tokens per response\n",
    "- **Conversation Speed:** {token_output_throughput:.1f} tokens/sec\n",
    "- **User Interaction Rate:** {requests_per_second:.1f} conversations/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if requests_per_second > 3 else \"⚠️ Moderate\" if requests_per_second > 1.5 else \"❌ Poor\"} for real-time chat applications\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'qa_long_context':\n",
    "                md_content += f\"\"\"\n",
    "### Long Context Q&A Performance Analysis\n",
    "\n",
    "This scenario tests question answering with extensive context processing.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Context Processing:** {statistics.mean(input_tokens):.0f} avg context tokens\n",
    "- **Answer Generation:** {statistics.mean(output_tokens):.0f} avg answer tokens\n",
    "- **Knowledge Processing:** {overall_token_throughput:.1f} total tokens/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if overall_token_throughput > 600 else \"⚠️ Moderate\" if overall_token_throughput > 300 else \"❌ Poor\"} for knowledge base applications\n",
    "\"\"\"\n",
    "            \n",
    "            elif scenario_name == 'creative_writing':\n",
    "                md_content += f\"\"\"\n",
    "### Creative Writing Performance Analysis\n",
    "\n",
    "This scenario tests creative content generation for storytelling and narrative creation.\n",
    "\n",
    "**Key Observations:**\n",
    "- **Creative Expansion:** {statistics.mean(output_tokens)/statistics.mean(input_tokens):.1f}:1 expansion ratio\n",
    "- **Story Generation Speed:** {token_output_throughput:.1f} tokens/sec\n",
    "- **Creative Throughput:** {requests_per_second:.1f} stories/sec\n",
    "\n",
    "**Use Case Suitability:** {\"✅ Excellent\" if token_output_throughput > 60 else \"⚠️ Moderate\" if token_output_throughput > 30 else \"❌ Poor\"} for content creation platforms\n",
    "\"\"\"\n",
    "\n",
    "        # Error analysis\n",
    "        if failed_requests:\n",
    "            md_content += f\"\"\"\n",
    "## ❌ Error Analysis\n",
    "\n",
    "**Total Failed Requests:** {len(failed_requests)}\n",
    "\n",
    "\"\"\"\n",
    "            error_types = defaultdict(int)\n",
    "            for req in failed_requests:\n",
    "                error_msg = req.get('error', 'Unknown error')\n",
    "                error_types[error_msg] += 1\n",
    "            \n",
    "            md_content += \"| Error Type | Count | Percentage |\\n|------------|-------|------------|\\n\"\n",
    "            for error, count in error_types.items():\n",
    "                percentage = count / len(results) * 100\n",
    "                md_content += f\"| {error} | {count} | {percentage:.1f}% |\\n\"\n",
    "\n",
    "        md_content += f\"\"\"\n",
    "\n",
    "## 🔧 Technical Environment\n",
    "\n",
    "- **Model Hosting:** TPU-based vLLM deployment\n",
    "- **Test Framework:** Multi-scenario benchmarking suite\n",
    "- **Concurrency Model:** ThreadPoolExecutor\n",
    "- **Metrics Collection:** Real-time latency and throughput tracking\n",
    "\n",
    "## 📁 Generated Files\n",
    "\n",
    "- **Summary CSV:** `{os.path.basename(csv_filename)}`\n",
    "- **Detailed CSV:** `{os.path.basename(detailed_csv_filename)}`\n",
    "- **This Report:** `{os.path.basename(md_filename)}`\n",
    "\n",
    "---\n",
    "*Report generated automatically by TPU Benchmark Suite v2.0*\n",
    "*Test scenario: {scenario_name} | Timestamp: {timestamp}*\n",
    "\"\"\"\n",
    "\n",
    "        # Save markdown report\n",
    "        with open(md_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_content)\n",
    "\n",
    "def run_all_scenarios(endpoint, use_dedicated_endpoint=True, scenarios=None):\n",
    "    \"\"\"Run all test scenarios or specified scenarios\"\"\"\n",
    "    if scenarios is None:\n",
    "        scenarios = list(TEST_SCENARIOS.keys())\n",
    "    \n",
    "    suite = TPUBenchmarkSuite(endpoint, use_dedicated_endpoint)\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"\\n🚀 STARTING COMPREHENSIVE TPU BENCHMARK SUITE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Scenarios to run: {', '.join(scenarios)}\")\n",
    "    print(f\"Total scenarios: {len(scenarios)}\")\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"\\n🔄 Running scenario {i}/{len(scenarios)}: {scenario}\")\n",
    "        try:\n",
    "            results = suite.run_scenario_test(scenario)\n",
    "            all_results[scenario] = results\n",
    "            print(f\"✅ Scenario {scenario} completed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Scenario {scenario} failed: {e}\")\n",
    "            all_results[scenario] = None\n",
    "    \n",
    "    # Generate comparative report\n",
    "    generate_comparative_report(all_results)\n",
    "    \n",
    "    print(f\"\\n🎉 ALL BENCHMARKS COMPLETED!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_comparative_report(all_results):\n",
    "    \"\"\"Generate a comparative report across all scenarios\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"tpu_benchmark_comparative\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    comparative_csv = f\"{output_dir}/comparative_summary_{timestamp}.csv\"\n",
    "    comparative_md = f\"{output_dir}/comparative_report_{timestamp}.md\"\n",
    "    \n",
    "    # Collect summary data from all scenarios\n",
    "    comparative_data = []\n",
    "    \n",
    "    for scenario_name, results in all_results.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "            \n",
    "        successful_requests = [r for r in results if r.get('success', False)]\n",
    "        if not successful_requests:\n",
    "            continue\n",
    "            \n",
    "        config = TEST_SCENARIOS[scenario_name]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ttft_times = [r['ttft'] for r in successful_requests]\n",
    "        inter_token_times = [r['inter_token_latency'] for r in successful_requests]\n",
    "        e2e_times = [r['end_to_end_time'] for r in successful_requests]\n",
    "        input_tokens = [r['input_tokens'] for r in successful_requests]\n",
    "        output_tokens = [r['output_tokens'] for r in successful_requests]\n",
    "        \n",
    "        total_test_time = max([r['timestamp'] for r in results]) - min([r['timestamp'] for r in results])\n",
    "        if total_test_time == 0:\n",
    "            total_test_time = 1  # Avoid division by zero\n",
    "        \n",
    "        comparative_data.append({\n",
    "            'scenario': scenario_name,\n",
    "            'scenario_name': config['name'],\n",
    "            'description': config['description'],\n",
    "            'concurrent_users': config['concurrent_users'],\n",
    "            'total_requests': len(results),\n",
    "            'successful_requests': len(successful_requests),\n",
    "            'success_rate': len(successful_requests) / len(results) * 100,\n",
    "            'ttft_p95': np.percentile(ttft_times, 95),\n",
    "            'inter_token_p95': np.percentile(inter_token_times, 95),\n",
    "            'e2e_p95': np.percentile(e2e_times, 95),\n",
    "            'token_output_throughput': sum(output_tokens) / total_test_time,\n",
    "            'overall_token_throughput': (sum(output_tokens) + sum(input_tokens)) / total_test_time,\n",
    "            'requests_per_second': len(successful_requests) / total_test_time,\n",
    "            'avg_input_tokens': statistics.mean(input_tokens),\n",
    "            'avg_output_tokens': statistics.mean(output_tokens),\n",
    "            'target_input_tokens': config['input_token_length'],\n",
    "            'target_output_tokens': config['output_tokens']\n",
    "        })\n",
    "    \n",
    "    # Save comparative CSV\n",
    "    if comparative_data:\n",
    "        comp_df = pd.DataFrame(comparative_data)\n",
    "        comp_df.to_csv(comparative_csv, index=False)\n",
    "        \n",
    "        # Generate comparative markdown report\n",
    "        md_content = f\"\"\"# TPU Benchmark Comparative Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Scenarios Tested:** {len(comparative_data)}  \n",
    "**Timestamp:** {timestamp}\n",
    "\n",
    "## 🎯 Executive Summary\n",
    "\n",
    "This report compares performance across different use case scenarios for TPU-hosted vLLM deployment.\n",
    "\n",
    "## 📊 Scenario Comparison Overview\n",
    "\n",
    "| Scenario | Use Case | Success Rate | Throughput (tok/sec) | TTFT p95 (s) | Requests/sec |\n",
    "|----------|----------|--------------|---------------------|---------------|--------------|\n",
    "\"\"\"\n",
    "        \n",
    "        for data in comparative_data:\n",
    "            md_content += f\"| {data['scenario_name']} | {data['description'][:30]}... | {data['success_rate']:.1f}% | {data['token_output_throughput']:.1f} | {data['ttft_p95']:.3f} | {data['requests_per_second']:.1f} |\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "## 🏆 Performance Rankings\n",
    "\n",
    "### Best Token Output Throughput\n",
    "\"\"\"\n",
    "        sorted_by_throughput = sorted(comparative_data, key=lambda x: x['token_output_throughput'], reverse=True)\n",
    "        for i, data in enumerate(sorted_by_throughput[:3], 1):\n",
    "            md_content += f\"{i}. **{data['scenario_name']}**: {data['token_output_throughput']:.1f} tok/sec\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "### Lowest Latency (TTFT p95)\n",
    "\"\"\"\n",
    "        sorted_by_ttft = sorted(comparative_data, key=lambda x: x['ttft_p95'])\n",
    "        for i, data in enumerate(sorted_by_ttft[:3], 1):\n",
    "            md_content += f\"{i}. **{data['scenario_name']}**: {data['ttft_p95']:.3f}s\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "### Highest Request Rate\n",
    "\"\"\"\n",
    "        sorted_by_rps = sorted(comparative_data, key=lambda x: x['requests_per_second'], reverse=True)\n",
    "        for i, data in enumerate(sorted_by_rps[:3], 1):\n",
    "            md_content += f\"{i}. **{data['scenario_name']}**: {data['requests_per_second']:.1f} req/sec\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "## 📈 Detailed Analysis by Use Case\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for data in comparative_data:\n",
    "            efficiency_ratio = data['avg_output_tokens'] / data['avg_input_tokens']\n",
    "            md_content += f\"\"\"\n",
    "### {data['scenario_name']}\n",
    "\n",
    "**Description:** {data['description']}\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Success Rate | {data['success_rate']:.1f}% |\n",
    "| Token Output Throughput | {data['token_output_throughput']:.1f} tok/sec |\n",
    "| Overall Token Throughput | {data['overall_token_throughput']:.1f} tok/sec |\n",
    "| Requests per Second | {data['requests_per_second']:.1f} req/sec |\n",
    "| TTFT p95 | {data['ttft_p95']:.3f}s |\n",
    "| Inter-token Latency p95 | {data['inter_token_p95']:.3f}s |\n",
    "| End-to-End p95 | {data['e2e_p95']:.1f}s |\n",
    "| Average Input Tokens | {data['avg_input_tokens']:.0f} |\n",
    "| Average Output Tokens | {data['avg_output_tokens']:.0f} |\n",
    "| Efficiency Ratio | {efficiency_ratio:.2f}:1 |\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "## 🎯 Recommendations\n",
    "\n",
    "### Production Deployment Suitability\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for data in comparative_data:\n",
    "            suitability = \"🟢 Excellent\" if data['success_rate'] > 95 and data['token_output_throughput'] > 30 else \"🟡 Good\" if data['success_rate'] > 90 and data['token_output_throughput'] > 15 else \"🔴 Needs Optimization\"\n",
    "            md_content += f\"- **{data['scenario_name']}**: {suitability}\\n\"\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "\n",
    "### Optimization Priorities\n",
    "\n",
    "1. **Latency Optimization**: Focus on scenarios with TTFT p95 > 1.0s\n",
    "2. **Throughput Scaling**: Improve scenarios with < 20 tok/sec output throughput\n",
    "3. **Reliability**: Address scenarios with < 95% success rate\n",
    "\n",
    "## 📁 Files Generated\n",
    "\n",
    "- **Comparative CSV**: `{os.path.basename(comparative_csv)}`\n",
    "- **This Report**: `{os.path.basename(comparative_md)}`\n",
    "\n",
    "---\n",
    "*Comparative analysis generated by TPU Benchmark Suite v2.0*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(comparative_md, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_content)\n",
    "        \n",
    "        print(f\"\\n📊 Comparative report generated:\")\n",
    "        print(f\"   - CSV: {comparative_csv}\")\n",
    "        print(f\"   - Report: {comparative_md}\")\n",
    "\n",
    "def list_available_scenarios():\n",
    "    \"\"\"List all available test scenarios\"\"\"\n",
    "    print(\"\\n🎯 Available Test Scenarios:\")\n",
    "    print(\"=\"*60)\n",
    "    for key, config in TEST_SCENARIOS.items():\n",
    "        print(f\"📌 {key}: {config['name']}\")\n",
    "        print(f\"   Description: {config['description']}\")\n",
    "        print(f\"   Input: {config['input_token_length']} tokens → Output: {config['output_tokens']} tokens\")\n",
    "        print(f\"   Concurrency: {config['concurrent_users']} users, Requests: {config['total_requests']}\")\n",
    "        print()\n",
    "\n",
    "def test_single_request(endpoint, use_dedicated_endpoint):\n",
    "    \"\"\"Test a single request to verify endpoint compatibility\"\"\"\n",
    "    print(\"🔍 Testing single request with your endpoint...\")\n",
    "    \n",
    "    # Test with a simple prompt like your working example\n",
    "    test_prompt = \"What is a car that can run on the wall?\"\n",
    "    \n",
    "    instances = [{\n",
    "        \"prompt\": test_prompt,\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 1.0,\n",
    "        \"raw_response\": True\n",
    "    }]\n",
    "    \n",
    "    try:\n",
    "        print(f\"📤 Sending request: {instances[0]}\")\n",
    "        \n",
    "        response = endpoint.predict(\n",
    "            instances=instances, \n",
    "            use_dedicated_endpoint=use_dedicated_endpoint\n",
    "        )\n",
    "        \n",
    "        print(f\"📥 Response type: {type(response)}\")\n",
    "        print(f\"📥 Response attributes: {dir(response)}\")\n",
    "        \n",
    "        if hasattr(response, 'predictions'):\n",
    "            print(f\"📥 Predictions: {response.predictions}\")\n",
    "            if response.predictions:\n",
    "                prediction = response.predictions[0]\n",
    "                print(f\"📥 First prediction type: {type(prediction)}\")\n",
    "                print(f\"📥 First prediction: {prediction}\")\n",
    "                return True\n",
    "        else:\n",
    "            print(\"❌ No 'predictions' attribute found\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        print(f\"❌ Traceback: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def run_benchmark_example():\n",
    "    \"\"\"Example function showing how to use the benchmark suite\"\"\"\n",
    "    print(\"🚀 TPU vLLM Multi-Scenario Benchmark Suite\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n📋 To use this benchmark suite, follow these steps:\")\n",
    "    print(\"\\n1. First, test your endpoint compatibility:\")\n",
    "    print(\"   test_single_request(endpoint, use_dedicated_endpoint)\")\n",
    "    print(\"\\n2. If that works, initialize the benchmark suite:\")\n",
    "    print(\"   suite = TPUBenchmarkSuite(endpoint, use_dedicated_endpoint)\")\n",
    "    print(\"\\n3. Run individual scenarios:\")\n",
    "    print(\"   results = suite.run_scenario_test('email_generation')\")\n",
    "    print(\"   results = suite.run_scenario_test('summarization')\")\n",
    "    print(\"   results = suite.run_scenario_test('rewrite_small')\")\n",
    "    print(\"   results = suite.run_scenario_test('rewrite_large')\")\n",
    "    print(\"\\n4. Or run all scenarios at once:\")\n",
    "    print(\"   all_results = run_all_scenarios(endpoint, use_dedicated_endpoint)\")\n",
    "    print(\"\\n5. Or run specific scenarios:\")\n",
    "    print(\"   selected = run_all_scenarios(endpoint, use_dedicated_endpoint,\")\n",
    "    print(\"                              scenarios=['email_generation', 'summarization'])\")\n",
    "    print(\"\\n💡 Available scenarios:\")\n",
    "    list_available_scenarios()\n",
    "\n",
    "# Main execution - notebook friendly\n",
    "def main():\n",
    "    \"\"\"Main function - notebook friendly version\"\"\"\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If we're in a notebook, just show the example\n",
    "        run_benchmark_example()\n",
    "    except NameError:\n",
    "        # We're in a regular Python script, use argparse\n",
    "        import sys\n",
    "        parser = argparse.ArgumentParser(description='TPU vLLM Multi-Scenario Benchmark Suite')\n",
    "        parser.add_argument('--scenarios', nargs='+', choices=list(TEST_SCENARIOS.keys()) + ['all'], \n",
    "                           default=['all'], help='Scenarios to run')\n",
    "        parser.add_argument('--list-scenarios', action='store_true', help='List available scenarios')\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        if args.list_scenarios:\n",
    "            list_available_scenarios()\n",
    "            return\n",
    "        \n",
    "        run_benchmark_example()\n",
    "\n",
    "# Auto-run example when imported in notebook\n",
    "try:\n",
    "    get_ipython()\n",
    "    print(\"📚 TPU Benchmark Suite loaded successfully!\")\n",
    "    print(\"💡 Run list_available_scenarios() to see all test scenarios\")\n",
    "    print(\"🚀 Run run_benchmark_example() to see usage instructions\")\n",
    "except NameError:\n",
    "    # Not in notebook\n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing single request with your endpoint...\n",
      "📤 Sending request: {'prompt': 'What is a car that can run on the wall?', 'max_tokens': 50, 'temperature': 1.0, 'raw_response': True}\n",
      "📥 Response type: <class 'google.cloud.aiplatform.models.Prediction'>\n",
      "📥 Response attributes: ['__add__', '__annotations__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__mul__', '__ne__', '__new__', '__orig_bases__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_field_defaults', '_fields', '_make', '_replace', 'count', 'deployed_model_id', 'explanations', 'index', 'metadata', 'model_resource_name', 'model_version_id', 'predictions']\n",
      "📥 Predictions: [' Car Park\\nA car that can run on the wall is called a Car Park.\\nThe car and the wall share a common letter \"A\", which humorously fulfills the expectation of a car running \"on\" the wall, being the same element']\n",
      "📥 First prediction type: <class 'str'>\n",
      "📥 First prediction:  Car Park\n",
      "A car that can run on the wall is called a Car Park.\n",
      "The car and the wall share a common letter \"A\", which humorously fulfills the expectation of a car running \"on\" the wall, being the same element\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your existing working setup\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "# use_dedicated_endpoint is already defined in your environment\n",
    "\n",
    "# Test the endpoint first\n",
    "test_single_request(endpoint, use_dedicated_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 STARTING TEST: Email Generation\n",
      "================================================================================\n",
      "Description: Short prompts generating long emails\n",
      "Test started at: 2025-07-08 01:34:40\n",
      "Output directory: tpu_benchmark_results_email_generation\n",
      "\n",
      "📝 Generating test prompts...\n",
      "✅ Generated 2000 test prompts\n",
      "📊 Sample prompt length: 15 words\n",
      "🎯 Target input tokens: 80\n",
      "🎯 Target output tokens: 350\n",
      "\n",
      "🔥 Starting load test:\n",
      "   - Concurrent users: 100\n",
      "   - Total requests: 2000\n",
      "   - Temperature: 0.7\n",
      "   - Max tokens: 400\n",
      "DEBUG Request 0: {'prompt': 'Create a re-engagement email for inactive users offering special incentives to return to our service.', 'max_tokens': 400, 'temperature': 0.7, 'raw_response': True}\n",
      "DEBUG Request 1: {'prompt': 'Create a re-engagement email for inactive users offering special incentives to return to our service.', 'max_tokens': 400, 'temperature': 0.7, 'raw_response': True}\n",
      "DEBUG Request 2: {'prompt': 'Create a re-engagement email for inactive users offering special incentives to return to our service.', 'max_tokens': 400, 'temperature': 0.7, 'raw_response': True}\n",
      "DEBUG Response 0: Prediction(predictions=[\" Use a persuasive and friendly tone to encourage them to try our new features and content.\\nSubject: We Miss You! Come Back and Discover What's New\\n\\nDear [Username],\\n\\nIt's been a while since we've seen you around here, and we're starting to feel a little lonely without you! As one of our valued users, we wanted to reach out and invite you to come back and experience all the exciting new features and content we've added since your last visit.\\n\\nWe've been busy behind the scenes, and we think you'll love what we've done. Here are just a few of the new things you can expect to find:\\n\\n* **New content**: We've added hundreds of new [articles/videos/etc.] to our library, covering the latest topics and trends in [industry/field].\\n* **Improved search**: Our new search function makes it easier than ever to find exactly what you're looking for, with advanced filters and sorting options.\\n* **Enhanced community features**: You can now connect with other users, join discussions, and share your own experiences and expertise.\\n\\nBut that's not all - we're also offering a special incentive to welcome you back. For a limited time, we're providing:\\n\\n* **20% off your next purchase**: Use the code COMEBACK20 at checkout to receive your discount.\\n* **Exclusive access to premium content**: Try out our premium features, including [list specific features, e.g. ad-free browsing, exclusive videos, etc.].\\n\\nWe value your loyalty and want to make sure you feel welcome and supported. Our team is always here to help, and we're committed to providing the best possible experience for our users.\\n\\nSo what are you waiting for? Click the link below to log back in and start exploring all the new and exciting things we have to offer.\\n\\n[Insert CTA button: Log Back In]\\n\\nWe can't wait to see you again and hear your thoughts on our new features and content. If you have any questions\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" Use a persuasive and friendly tone to encourage them to try our new features and content.\\nSubject: We Miss You! Come Back and Discover What's New\\n\\nDear [Username],\\n\\nIt's been a while since we've seen you around here, and we're starting to feel a little lonely without you! As one of our valued users, we wanted to reach out and invite you to come back and experience all the exciting new features and content we've added since your last visit.\\n\\nWe've been busy behind the scenes, and we think you'll love what we've done. Here are just a few of the new things you can expect to find:\\n\\n* **New content**: We've added hundreds of new [articles/videos/etc.] to our library, covering the latest topics and trends in [industry/field].\\n* **Improved search**: Our new search function makes it easier than ever to find exactly what you're looking for, with advanced filters and sorting options.\\n* **Enhanced community features**: You can now connect with other users, join discussions, and share your own experiences and expertise.\\n\\nBut that's not all - we're also offering a special incentive to welcome you back. For a limited time, we're providing:\\n\\n* **20% off your next purchase**: Use the code COMEBACK20 at checkout to receive your discount.\\n* **Exclusive access to premium content**: Try out our premium features, including [list specific features, e.g. ad-free browsing, exclusive videos, etc.].\\n\\nWe value your loyalty and want to make sure you feel welcome and supported. Our team is always here to help, and we're committed to providing the best possible experience for our users.\\n\\nSo what are you waiting for? Click the link below to log back in and start exploring all the new and exciting things we have to offer.\\n\\n[Insert CTA button: Log Back In]\\n\\nWe can't wait to see you again and hear your thoughts on our new features and content. If you have any questions\"]\n",
      "DEBUG Response 1: Prediction(predictions=[\" (Cats)\\n**Subject: We've Missed You! Come Back to the Feline Fun!**\\n\\nDear [User's Name],\\n\\nIt's been a while since you last visited us, and we've missed you! As a valued member of our feline-loving community, we want to welcome you back with open arms (and paws).\\n\\nWe've been busy improving our service, and we think you'll love the new features and content we've added. Our community is still buzzing with cat lovers, and we'd love for you to rejoin the conversation!\\n\\n**Exclusive Offers for You:**\\n\\n1. **20% discount on your next purchase**: Use code WELCOME BACK20 at checkout to redeem your discount.\\n2. **Free access to premium content**: Enjoy exclusive articles, videos, and photos featuring adorable cats from around the world.\\n3. **Entry into our monthly cat-themed giveaway**: You'll be automatically entered to win a purr-fect prize package, including cat toys, treats, and more!\\n\\n**What's New:**\\n\\n* New cat breeds and profiles added to our database\\n* Improved search functionality to help you find your favorite felines\\n* Enhanced community features, including discussion forums and photo sharing\\n\\n**Ready to Return?**\\n\\nClick the link below to log in and start exploring our updated service:\\n\\n[Insert CTA button: Log In Now]\\n\\nIf you have any questions or need help getting started again, our support team is always here to assist you. Just reply to this email or visit our help center.\\n\\nWe're excited to have you back and look forward to sharing more feline fun with you!\\n\\nBest regards,\\n\\nThe [Your Company Name] Team\\n\\nP.S. Don't forget to follow us on social media to stay up-to-date on the latest cat news and adorable photos! [Insert social media links]\\n\\nThis re-engagement email aims to:\\n\\n1. Personalize the message with the user's name\\n2. Offer exclusive incentives to return to\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" (Cats)\\n**Subject: We've Missed You! Come Back to the Feline Fun!**\\n\\nDear [User's Name],\\n\\nIt's been a while since you last visited us, and we've missed you! As a valued member of our feline-loving community, we want to welcome you back with open arms (and paws).\\n\\nWe've been busy improving our service, and we think you'll love the new features and content we've added. Our community is still buzzing with cat lovers, and we'd love for you to rejoin the conversation!\\n\\n**Exclusive Offers for You:**\\n\\n1. **20% discount on your next purchase**: Use code WELCOME BACK20 at checkout to redeem your discount.\\n2. **Free access to premium content**: Enjoy exclusive articles, videos, and photos featuring adorable cats from around the world.\\n3. **Entry into our monthly cat-themed giveaway**: You'll be automatically entered to win a purr-fect prize package, including cat toys, treats, and more!\\n\\n**What's New:**\\n\\n* New cat breeds and profiles added to our database\\n* Improved search functionality to help you find your favorite felines\\n* Enhanced community features, including discussion forums and photo sharing\\n\\n**Ready to Return?**\\n\\nClick the link below to log in and start exploring our updated service:\\n\\n[Insert CTA button: Log In Now]\\n\\nIf you have any questions or need help getting started again, our support team is always here to assist you. Just reply to this email or visit our help center.\\n\\nWe're excited to have you back and look forward to sharing more feline fun with you!\\n\\nBest regards,\\n\\nThe [Your Company Name] Team\\n\\nP.S. Don't forget to follow us on social media to stay up-to-date on the latest cat news and adorable photos! [Insert social media links]\\n\\nThis re-engagement email aims to:\\n\\n1. Personalize the message with the user's name\\n2. Offer exclusive incentives to return to\"]\n",
      "DEBUG Response 2: Prediction(predictions=[\" Focus on addressing common pain points and highlighting our improvements.\\nSubject: We Miss You!\\n\\nDear [Username],\\n\\nIt's been a while since you last logged in to our service, and we're starting to feel your absence. We want to welcome you back and show you what you've been missing. Our team has been working hard to address the issues you may have faced in the past, and we're excited to share the improvements we've made.\\n\\n**We've listened to your feedback:**\\n\\n* **Simplified Navigation:** We've revamped our interface to make it easier to find what you need, faster.\\n* **Enhanced Performance:** Our platform is now more responsive and efficient, reducing wait times and errors.\\n* **New Features:** We've added exciting new tools and functionalities to help you achieve your goals.\\n\\n**Exclusive Offer:**\\n\\nAs a valued user, we're offering you a special incentive to return:\\n\\n* **20% discount** on your next purchase or subscription\\n* **Free access** to our premium content and resources for a limited time\\n* **Priority support** to ensure a seamless experience\\n\\n**What's new and exciting:**\\n\\n* **New Content:** Our library has grown with fresh and relevant materials to help you stay up-to-date.\\n* **Community Engagement:** Join our revamped forums and connect with like-minded individuals.\\n* **Success Stories:** Hear from our satisfied users who have achieved remarkable results with our service.\\n\\n**Ready to come back?**\\n\\nClick the link below to log in and rediscover the benefits of our service:\\n\\n[Insert CTA button: Log In Now]\\n\\nIf you have any questions or need assistance, our support team is here to help. Simply reply to this email or visit our support center.\\n\\nWe're looking forward to welcoming you back and helping you achieve your goals.\\n\\nBest regards,\\n\\n[Your Name]\\n\\nP.S. Don't forget to follow us on social media to stay updated on the latest news, tips, and promotions! [Insert social media links\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" Focus on addressing common pain points and highlighting our improvements.\\nSubject: We Miss You!\\n\\nDear [Username],\\n\\nIt's been a while since you last logged in to our service, and we're starting to feel your absence. We want to welcome you back and show you what you've been missing. Our team has been working hard to address the issues you may have faced in the past, and we're excited to share the improvements we've made.\\n\\n**We've listened to your feedback:**\\n\\n* **Simplified Navigation:** We've revamped our interface to make it easier to find what you need, faster.\\n* **Enhanced Performance:** Our platform is now more responsive and efficient, reducing wait times and errors.\\n* **New Features:** We've added exciting new tools and functionalities to help you achieve your goals.\\n\\n**Exclusive Offer:**\\n\\nAs a valued user, we're offering you a special incentive to return:\\n\\n* **20% discount** on your next purchase or subscription\\n* **Free access** to our premium content and resources for a limited time\\n* **Priority support** to ensure a seamless experience\\n\\n**What's new and exciting:**\\n\\n* **New Content:** Our library has grown with fresh and relevant materials to help you stay up-to-date.\\n* **Community Engagement:** Join our revamped forums and connect with like-minded individuals.\\n* **Success Stories:** Hear from our satisfied users who have achieved remarkable results with our service.\\n\\n**Ready to come back?**\\n\\nClick the link below to log in and rediscover the benefits of our service:\\n\\n[Insert CTA button: Log In Now]\\n\\nIf you have any questions or need assistance, our support team is here to help. Simply reply to this email or visit our support center.\\n\\nWe're looking forward to welcoming you back and helping you achieve your goals.\\n\\nBest regards,\\n\\n[Your Name]\\n\\nP.S. Don't forget to follow us on social media to stay updated on the latest news, tips, and promotions! [Insert social media links\"]\n",
      "   ⚡ Completed 100/2000 requests...\n",
      "   ⚡ Completed 200/2000 requests...\n",
      "   ⚡ Completed 300/2000 requests...\n",
      "   ⚡ Completed 400/2000 requests...\n",
      "   ⚡ Completed 500/2000 requests...\n",
      "   ⚡ Completed 600/2000 requests...\n",
      "   ⚡ Completed 700/2000 requests...\n",
      "   ⚡ Completed 800/2000 requests...\n",
      "   ⚡ Completed 900/2000 requests...\n",
      "   ⚡ Completed 1000/2000 requests...\n",
      "   ⚡ Completed 1100/2000 requests...\n",
      "   ⚡ Completed 1200/2000 requests...\n",
      "   ⚡ Completed 1300/2000 requests...\n",
      "   ⚡ Completed 1400/2000 requests...\n",
      "   ⚡ Completed 1500/2000 requests...\n",
      "   ⚡ Completed 1600/2000 requests...\n",
      "   ⚡ Completed 1700/2000 requests...\n",
      "   ⚡ Completed 1800/2000 requests...\n",
      "   ⚡ Completed 1900/2000 requests...\n",
      "   ⚡ Completed 2000/2000 requests...\n",
      "\n",
      "================================================================================\n",
      "📊 TEST RESULTS: Email Generation\n",
      "================================================================================\n",
      "\n",
      "📈 Test Summary:\n",
      "   - Total requests: 2000\n",
      "   - Successful requests: 2000\n",
      "   - Failed requests: 0\n",
      "   - Success rate: 100.0%\n",
      "   - Total test time: 545.7 seconds\n",
      "\n",
      "⚡ Latency Metrics:\n",
      "   - TTFT (p50): 0.500s\n",
      "   - TTFT (p95): 0.500s\n",
      "   - TTFT (p99): 0.500s\n",
      "   - Inter-token (p50): 0.066s\n",
      "   - Inter-token (p95): 0.072s\n",
      "   - End-to-End (p50): 27.4s\n",
      "   - End-to-End (p95): 27.5s\n",
      "   - End-to-End (p99): 27.7s\n",
      "\n",
      "🚀 Throughput Metrics:\n",
      "   - Token Output Throughput: 1470.62 tok/sec\n",
      "   - Overall Token Throughput: 1542.09 tok/sec\n",
      "   - Requests per second: 3.66 req/sec\n",
      "\n",
      "📊 Token Statistics:\n",
      "   - Average input tokens: 19.5\n",
      "   - Average output tokens: 401.3\n",
      "   - Total input tokens: 39,000\n",
      "   - Total output tokens: 802,525\n",
      "\n",
      "✅ Test completed successfully!\n",
      "📁 Files saved in: tpu_benchmark_results_email_generation\n",
      "\n",
      "================================================================================\n",
      "🚀 STARTING TEST: Text Summarization\n",
      "================================================================================\n",
      "Description: Long text summarization to concise output\n",
      "Test started at: 2025-07-08 01:43:45\n",
      "Output directory: tpu_benchmark_results_summarization\n",
      "\n",
      "📝 Generating test prompts...\n",
      "✅ Generated 3000 test prompts\n",
      "📊 Sample prompt length: 265 words\n",
      "🎯 Target input tokens: 500\n",
      "🎯 Target output tokens: 80\n",
      "\n",
      "🔥 Starting load test:\n",
      "   - Concurrent users: 150\n",
      "   - Total requests: 3000\n",
      "   - Temperature: 0.3\n",
      "   - Max tokens: 120\n",
      "DEBUG Request 0: {'prompt': \"Please provide a concise summary of the following business analysis:\\n\\n\\n    In today's rapidly evolving business landscape, companies are increasingly turning to artificial intelligence and machine learning technologies to gain competitive advantages and streamline their operations. The implementation of AI systems has become a critical factor in determining organizational success across various industries, from healthcare and finance to retail and manufacturing.\\n\\n    The adoption of AI technologies brings numerous benefits including improved efficiency, enhanced decision-making capabilities, cost reduction, and the ability to process vast amounts of data in real-time. However, organizations also face significant challenges when implementing these systems, including data privacy concerns, integration complexities, workforce adaptation requirements, and substantial initial investment costs.\\n\\n    Recent studies indicate that companies successfully implementing AI solutions report an average increase in productivity of 40% and cost savings of up to 30% within the first two years of deployment. These improvements are primarily attributed to automation of repetitive tasks, enhanced predictive analytics, and improved customer service through chatbots and virtual assistants.\\n\\n    The key to successful AI implementation lies in developing a comprehensive strategy that addresses technical requirements, organizational readiness, and change management processes. Companies must invest in employee training, establish clear governance frameworks, and ensure compliance with relevant regulations and ethical guidelines.\\n\\n    Looking ahead, the future of AI in business appears promising, with emerging technologies such as generative AI, computer vision, and natural language processing offering new opportunities for innovation and growth. Organizations that proactively embrace these technologies while addressing associated challenges will be better positioned to thrive in the digital economy.\\n\\n    The integration of AI systems also requires careful consi\\n\\nSummary:\", 'max_tokens': 120, 'temperature': 0.3, 'raw_response': True}\n",
      "DEBUG Request 1: {'prompt': \"Please provide a concise summary of the following business analysis:\\n\\n\\n    In today's rapidly evolving business landscape, companies are increasingly turning to artificial intelligence and machine learning technologies to gain competitive advantages and streamline their operations. The implementation of AI systems has become a critical factor in determining organizational success across various industries, from healthcare and finance to retail and manufacturing.\\n\\n    The adoption of AI technologies brings numerous benefits including improved efficiency, enhanced decision-making capabilities, cost reduction, and the ability to process vast amounts of data in real-time. However, organizations also face significant challenges when implementing these systems, including data privacy concerns, integration complexities, workforce adaptation requirements, and substantial initial investment costs.\\n\\n    Recent studies indicate that companies successfully implementing AI solutions report an average increase in productivity of 40% and cost savings of up to 30% within the first two years of deployment. These improvements are primarily attributed to automation of repetitive tasks, enhanced predictive analytics, and improved customer service through chatbots and virtual assistants.\\n\\n    The key to successful AI implementation lies in developing a comprehensive strategy that addresses technical requirements, organizational readiness, and change management processes. Companies must invest in employee training, establish clear governance frameworks, and ensure compliance with relevant regulations and ethical guidelines.\\n\\n    Looking ahead, the future of AI in business appears promising, with emerging technologies such as generative AI, computer vision, and natural language processing offering new opportunities for innovation and growth. Organizations that proactively embrace these technologies while addressing associated challenges will be better positioned to thrive in the digital economy.\\n\\n    The integration of AI systems also requires careful consi\\n\\nSummary:\", 'max_tokens': 120, 'temperature': 0.3, 'raw_response': True}\n",
      "DEBUG Request 2: {'prompt': \"Please provide a concise summary of the following business analysis:\\n\\n\\n    In today's rapidly evolving business landscape, companies are increasingly turning to artificial intelligence and machine learning technologies to gain competitive advantages and streamline their operations. The implementation of AI systems has become a critical factor in determining organizational success across various industries, from healthcare and finance to retail and manufacturing.\\n\\n    The adoption of AI technologies brings numerous benefits including improved efficiency, enhanced decision-making capabilities, cost reduction, and the ability to process vast amounts of data in real-time. However, organizations also face significant challenges when implementing these systems, including data privacy concerns, integration complexities, workforce adaptation requirements, and substantial initial investment costs.\\n\\n    Recent studies indicate that companies successfully implementing AI solutions report an average increase in productivity of 40% and cost savings of up to 30% within the first two years of deployment. These improvements are primarily attributed to automation of repetitive tasks, enhanced predictive analytics, and improved customer service through chatbots and virtual assistants.\\n\\n    The key to successful AI implementation lies in developing a comprehensive strategy that addresses technical requirements, organizational readiness, and change management processes. Companies must invest in employee training, establish clear governance frameworks, and ensure compliance with relevant regulations and ethical guidelines.\\n\\n    Looking ahead, the future of AI in business appears promising, with emerging technologies such as generative AI, computer vision, and natural language processing offering new opportunities for innovation and growth. Organizations that proactively embrace these technologies while addressing associated challenges will be better positioned to thrive in the digital economy.\\n\\n    The integration of AI systems also requires careful consi\\n\\nSummary:\", 'max_tokens': 120, 'temperature': 0.3, 'raw_response': True}\n",
      "DEBUG Response 1: Prediction(predictions=[' \\nThe adoption of artificial intelligence (AI) and machine learning technologies is crucial for businesses to gain a competitive edge and improve operations. While AI implementation offers benefits like increased efficiency and cost savings, it also poses challenges such as data privacy concerns and integration complexities. Successful AI adoption requires a comprehensive strategy, employee training, and compliance with regulations. As AI technologies continue to evolve, businesses that proactively adapt will be better positioned to succeed in the digital economy.'], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [' \\nThe adoption of artificial intelligence (AI) and machine learning technologies is crucial for businesses to gain a competitive edge and improve operations. While AI implementation offers benefits like increased efficiency and cost savings, it also poses challenges such as data privacy concerns and integration complexities. Successful AI adoption requires a comprehensive strategy, employee training, and compliance with regulations. As AI technologies continue to evolve, businesses that proactively adapt will be better positioned to succeed in the digital economy.']\n",
      "DEBUG Response 0: Prediction(predictions=[' \\nThe adoption of artificial intelligence (AI) and machine learning technologies is crucial for businesses to gain a competitive edge and improve operations. While AI implementation presents challenges such as data privacy concerns and initial investment costs, it also offers numerous benefits including improved efficiency, enhanced decision-making, and cost reduction. Successful implementation requires a comprehensive strategy addressing technical, organizational, and change management aspects. With emerging AI technologies offering new opportunities, businesses that proactively adopt AI while addressing challenges will thrive in the digital economy.'], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [' \\nThe adoption of artificial intelligence (AI) and machine learning technologies is crucial for businesses to gain a competitive edge and improve operations. While AI implementation presents challenges such as data privacy concerns and initial investment costs, it also offers numerous benefits including improved efficiency, enhanced decision-making, and cost reduction. Successful implementation requires a comprehensive strategy addressing technical, organizational, and change management aspects. With emerging AI technologies offering new opportunities, businesses that proactively adopt AI while addressing challenges will thrive in the digital economy.']\n",
      "DEBUG Response 2: Prediction(predictions=[' \\nThe adoption of artificial intelligence (AI) and machine learning technologies is crucial for businesses to gain a competitive edge and improve operations. While AI implementation brings benefits such as improved efficiency and cost reduction, it also poses challenges like data privacy concerns and integration complexities. Successful AI implementation requires a comprehensive strategy, employee training, and compliance with regulations. Companies that effectively implement AI can expect significant productivity gains and cost savings, and those that proactively embrace emerging AI technologies will be well-positioned for future success.'], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [' \\nThe adoption of artificial intelligence (AI) and machine learning technologies is crucial for businesses to gain a competitive edge and improve operations. While AI implementation brings benefits such as improved efficiency and cost reduction, it also poses challenges like data privacy concerns and integration complexities. Successful AI implementation requires a comprehensive strategy, employee training, and compliance with regulations. Companies that effectively implement AI can expect significant productivity gains and cost savings, and those that proactively embrace emerging AI technologies will be well-positioned for future success.']\n",
      "   ⚡ Completed 100/3000 requests...\n",
      "   ⚡ Completed 200/3000 requests...\n",
      "   ⚡ Completed 300/3000 requests...\n",
      "   ⚡ Completed 400/3000 requests...\n",
      "   ⚡ Completed 500/3000 requests...\n",
      "   ⚡ Completed 600/3000 requests...\n",
      "   ⚡ Completed 700/3000 requests...\n",
      "   ⚡ Completed 800/3000 requests...\n",
      "   ⚡ Completed 900/3000 requests...\n",
      "   ⚡ Completed 1000/3000 requests...\n",
      "   ⚡ Completed 1100/3000 requests...\n",
      "   ⚡ Completed 1200/3000 requests...\n",
      "   ⚡ Completed 1300/3000 requests...\n",
      "   ⚡ Completed 1400/3000 requests...\n",
      "   ⚡ Completed 1500/3000 requests...\n",
      "   ⚡ Completed 1600/3000 requests...\n",
      "   ⚡ Completed 1700/3000 requests...\n",
      "   ⚡ Completed 1800/3000 requests...\n",
      "   ⚡ Completed 1900/3000 requests...\n",
      "   ⚡ Completed 2000/3000 requests...\n",
      "   ⚡ Completed 2100/3000 requests...\n",
      "   ⚡ Completed 2200/3000 requests...\n",
      "   ⚡ Completed 2300/3000 requests...\n",
      "   ⚡ Completed 2400/3000 requests...\n",
      "   ⚡ Completed 2500/3000 requests...\n",
      "   ⚡ Completed 2600/3000 requests...\n",
      "   ⚡ Completed 2700/3000 requests...\n",
      "   ⚡ Completed 2800/3000 requests...\n",
      "   ⚡ Completed 2900/3000 requests...\n",
      "   ⚡ Completed 3000/3000 requests...\n",
      "\n",
      "================================================================================\n",
      "📊 TEST RESULTS: Text Summarization\n",
      "================================================================================\n",
      "\n",
      "📈 Test Summary:\n",
      "   - Total requests: 3000\n",
      "   - Successful requests: 3000\n",
      "   - Failed requests: 0\n",
      "   - Success rate: 100.0%\n",
      "   - Total test time: 211.0 seconds\n",
      "\n",
      "⚡ Latency Metrics:\n",
      "   - TTFT (p50): 0.206s\n",
      "   - TTFT (p95): 0.250s\n",
      "   - TTFT (p99): 0.250s\n",
      "   - Inter-token (p50): 0.093s\n",
      "   - Inter-token (p95): 0.098s\n",
      "   - End-to-End (p50): 10.3s\n",
      "   - End-to-End (p95): 12.5s\n",
      "   - End-to-End (p99): 12.5s\n",
      "\n",
      "🚀 Throughput Metrics:\n",
      "   - Token Output Throughput: 1563.27 tok/sec\n",
      "   - Overall Token Throughput: 6462.20 tok/sec\n",
      "   - Requests per second: 14.22 req/sec\n",
      "\n",
      "📊 Token Statistics:\n",
      "   - Average input tokens: 344.5\n",
      "   - Average output tokens: 109.9\n",
      "   - Total input tokens: 1,033,500\n",
      "   - Total output tokens: 329,794\n",
      "\n",
      "✅ Test completed successfully!\n",
      "📁 Files saved in: tpu_benchmark_results_summarization\n",
      "\n",
      "================================================================================\n",
      "🚀 STARTING TEST: Content Rewriting (Small)\n",
      "================================================================================\n",
      "Description: Small content rewriting and improvement\n",
      "Test started at: 2025-07-08 01:47:16\n",
      "Output directory: tpu_benchmark_results_rewrite_small\n",
      "\n",
      "📝 Generating test prompts...\n",
      "✅ Generated 4000 test prompts\n",
      "📊 Sample prompt length: 148 words\n",
      "🎯 Target input tokens: 250\n",
      "🎯 Target output tokens: 250\n",
      "\n",
      "🔥 Starting load test:\n",
      "   - Concurrent users: 200\n",
      "   - Total requests: 4000\n",
      "   - Temperature: 0.5\n",
      "   - Max tokens: 300\n",
      "DEBUG Request 0: {'prompt': 'Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n\\n        Our company has been working on developing new software solutions for the past several years. We have created various applications that help businesses manage their operations more effectively. The software includes features for inventory management, customer relationship management, and financial reporting.\\n\\n        The development team consists of experienced programmers who use modern programming languages and frameworks. We follow agile development methodologies to ensure quick delivery of high-quality software products. Our testing procedures include both automated and manual testing to identify and fix any issues before deployment.\\n\\n        Customer feedback has been very positive, with many users praising the user-friendly interface and robust functionality. We continue to add new features based on user requests and market demands. Our support team provides excellent customer service to help users maximize the benefits of our software solutions.\\n        \\n\\nImproved version:', 'max_tokens': 300, 'temperature': 0.5, 'raw_response': True}\n",
      "DEBUG Request 1: {'prompt': 'Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n\\n        Our company has been working on developing new software solutions for the past several years. We have created various applications that help businesses manage their operations more effectively. The software includes features for inventory management, customer relationship management, and financial reporting.\\n\\n        The development team consists of experienced programmers who use modern programming languages and frameworks. We follow agile development methodologies to ensure quick delivery of high-quality software products. Our testing procedures include both automated and manual testing to identify and fix any issues before deployment.\\n\\n        Customer feedback has been very positive, with many users praising the user-friendly interface and robust functionality. We continue to add new features based on user requests and market demands. Our support team provides excellent customer service to help users maximize the benefits of our software solutions.\\n        \\n\\nImproved version:', 'max_tokens': 300, 'temperature': 0.5, 'raw_response': True}\n",
      "DEBUG Request 2: {'prompt': 'Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n\\n        Our company has been working on developing new software solutions for the past several years. We have created various applications that help businesses manage their operations more effectively. The software includes features for inventory management, customer relationship management, and financial reporting.\\n\\n        The development team consists of experienced programmers who use modern programming languages and frameworks. We follow agile development methodologies to ensure quick delivery of high-quality software products. Our testing procedures include both automated and manual testing to identify and fix any issues before deployment.\\n\\n        Customer feedback has been very positive, with many users praising the user-friendly interface and robust functionality. We continue to add new features based on user requests and market demands. Our support team provides excellent customer service to help users maximize the benefits of our software solutions.\\n        \\n\\nImproved version:', 'max_tokens': 300, 'temperature': 0.5, 'raw_response': True}\n",
      "DEBUG Response 0: Prediction(predictions=[' \\n\\n\\nOur organization has been at the forefront of developing innovative software solutions for several years, with a focus on empowering businesses to optimize their operations. Our suite of applications offers a comprehensive range of features, including inventory management, customer relationship management, and financial reporting, all designed to streamline processes and enhance productivity.\\n\\nOur development team comprises seasoned programmers who leverage cutting-edge programming languages and frameworks to craft high-quality software products. We adhere to agile development methodologies, which enable us to deliver solutions rapidly while maintaining exceptional standards. Our rigorous testing protocols combine automated and manual testing to ensure that our software is thoroughly vetted and issue-free before deployment.\\n\\nThe response from our customers has been overwhelmingly positive, with many praising the intuitive user interface and robust functionality of our software. We prioritize continuous improvement, incorporating user feedback and market trends into our development roadmap to ensure our solutions remain relevant and effective. Our dedicated support team provides exceptional customer service, empowering users to unlock the full potential of our software and drive business success.\\n\\n\\nChanges made:\\n\\n\\n*   Reorganized the structure to improve flow and clarity\\n*   Enhanced vocabulary and sentence structure to increase professionalism and engagement\\n*   Added more descriptive language to help paint a vivid picture of the company\\'s capabilities and values\\n*   Emphasized the benefits of the software solutions, such as streamlining processes and enhancing productivity\\n*   Highlighted the company\\'s commitment to continuous improvement and customer satisfaction\\n*   Used more active and descriptive verbs, such as \"empowering,\" \"streamline,\" and'], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [' \\n\\n\\nOur organization has been at the forefront of developing innovative software solutions for several years, with a focus on empowering businesses to optimize their operations. Our suite of applications offers a comprehensive range of features, including inventory management, customer relationship management, and financial reporting, all designed to streamline processes and enhance productivity.\\n\\nOur development team comprises seasoned programmers who leverage cutting-edge programming languages and frameworks to craft high-quality software products. We adhere to agile development methodologies, which enable us to deliver solutions rapidly while maintaining exceptional standards. Our rigorous testing protocols combine automated and manual testing to ensure that our software is thoroughly vetted and issue-free before deployment.\\n\\nThe response from our customers has been overwhelmingly positive, with many praising the intuitive user interface and robust functionality of our software. We prioritize continuous improvement, incorporating user feedback and market trends into our development roadmap to ensure our solutions remain relevant and effective. Our dedicated support team provides exceptional customer service, empowering users to unlock the full potential of our software and drive business success.\\n\\n\\nChanges made:\\n\\n\\n*   Reorganized the structure to improve flow and clarity\\n*   Enhanced vocabulary and sentence structure to increase professionalism and engagement\\n*   Added more descriptive language to help paint a vivid picture of the company\\'s capabilities and values\\n*   Emphasized the benefits of the software solutions, such as streamlining processes and enhancing productivity\\n*   Highlighted the company\\'s commitment to continuous improvement and customer satisfaction\\n*   Used more active and descriptive verbs, such as \"empowering,\" \"streamline,\" and']\n",
      "DEBUG Response 1: Prediction(predictions=[\" \\n\\nOur company has been at the forefront of software development for several years, designing and delivering innovative solutions that empower businesses to streamline their operations and drive growth. Our comprehensive software suite encompasses a range of applications that cater to diverse business needs, including inventory management, customer relationship management, and financial reporting.\\n\\nOur development team comprises seasoned programmers who leverage cutting-edge programming languages and frameworks to craft high-performance software products. We adhere to agile development methodologies, which enable us to respond swiftly to changing market demands and deliver top-notch software solutions with rapid time-to-market. Our rigorous testing protocols combine automated and manual testing to ensure that our software products meet the highest standards of quality and reliability.\\n\\nThe feedback from our customers has been overwhelmingly positive, with many users praising the intuitive user interface and robust functionality of our software. We prioritize customer-centricity, continuously gathering feedback and incorporating new features that address emerging trends and user needs. Our dedicated support team provides exceptional customer service, empowering users to unlock the full potential of our software solutions and drive business success.\\n\\n\\nChanges made:\\n\\n- More formal and professional tone\\n- Added more descriptive language to paint a vivid picture\\n- Emphasized the company's expertise and innovative approach\\n- Highlighted the benefits of the software solutions for businesses\\n- Used more active and engaging verbs\\n- Added a few words to make the text more concise and clear\\n- Changed some sentence structures to improve flow and readability\\n- Emphasized customer-centricity and the importance of feedback\\n- Used more precise language\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" \\n\\nOur company has been at the forefront of software development for several years, designing and delivering innovative solutions that empower businesses to streamline their operations and drive growth. Our comprehensive software suite encompasses a range of applications that cater to diverse business needs, including inventory management, customer relationship management, and financial reporting.\\n\\nOur development team comprises seasoned programmers who leverage cutting-edge programming languages and frameworks to craft high-performance software products. We adhere to agile development methodologies, which enable us to respond swiftly to changing market demands and deliver top-notch software solutions with rapid time-to-market. Our rigorous testing protocols combine automated and manual testing to ensure that our software products meet the highest standards of quality and reliability.\\n\\nThe feedback from our customers has been overwhelmingly positive, with many users praising the intuitive user interface and robust functionality of our software. We prioritize customer-centricity, continuously gathering feedback and incorporating new features that address emerging trends and user needs. Our dedicated support team provides exceptional customer service, empowering users to unlock the full potential of our software solutions and drive business success.\\n\\n\\nChanges made:\\n\\n- More formal and professional tone\\n- Added more descriptive language to paint a vivid picture\\n- Emphasized the company's expertise and innovative approach\\n- Highlighted the benefits of the software solutions for businesses\\n- Used more active and engaging verbs\\n- Added a few words to make the text more concise and clear\\n- Changed some sentence structures to improve flow and readability\\n- Emphasized customer-centricity and the importance of feedback\\n- Used more precise language\"]\n",
      "DEBUG Response 2: Prediction(predictions=[\" \\n\\nOur company has been at the forefront of developing innovative software solutions for several years, with a focus on empowering businesses to optimize their operations. Our comprehensive suite of applications offers a range of features, including inventory management, customer relationship management, and financial reporting, all designed to streamline business processes and drive growth.\\n\\nOur development team comprises seasoned programmers who leverage cutting-edge programming languages and frameworks to craft high-quality software products. We adhere to agile development methodologies, which enable us to deliver solutions quickly and efficiently while maintaining the highest standards of quality. Our rigorous testing protocols combine automated and manual testing to identify and rectify any issues, ensuring seamless deployment and minimizing downtime.\\n\\nThe feedback from our customers has been overwhelmingly positive, with many praising the intuitive user interface and robust functionality of our software. We prioritize customer feedback and continuously update our solutions to meet evolving user needs and market trends. Our dedicated support team provides exceptional customer service, ensuring that users can fully leverage the benefits of our software and achieve their business objectives. By fostering a culture of innovation and customer-centricity, we remain committed to delivering software solutions that make a meaningful impact on our clients' businesses.\\n\\nNote: I made the following changes:\\n- Improved sentence structure and wording for better clarity and flow\\n- Added more descriptive language to make the content more engaging\\n- Emphasized the company's focus on innovation, customer-centricity, and quality\\n- Highlighted the benefits of the software solutions for businesses\\n- Used more professional and technical vocabulary (e.g.,\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" \\n\\nOur company has been at the forefront of developing innovative software solutions for several years, with a focus on empowering businesses to optimize their operations. Our comprehensive suite of applications offers a range of features, including inventory management, customer relationship management, and financial reporting, all designed to streamline business processes and drive growth.\\n\\nOur development team comprises seasoned programmers who leverage cutting-edge programming languages and frameworks to craft high-quality software products. We adhere to agile development methodologies, which enable us to deliver solutions quickly and efficiently while maintaining the highest standards of quality. Our rigorous testing protocols combine automated and manual testing to identify and rectify any issues, ensuring seamless deployment and minimizing downtime.\\n\\nThe feedback from our customers has been overwhelmingly positive, with many praising the intuitive user interface and robust functionality of our software. We prioritize customer feedback and continuously update our solutions to meet evolving user needs and market trends. Our dedicated support team provides exceptional customer service, ensuring that users can fully leverage the benefits of our software and achieve their business objectives. By fostering a culture of innovation and customer-centricity, we remain committed to delivering software solutions that make a meaningful impact on our clients' businesses.\\n\\nNote: I made the following changes:\\n- Improved sentence structure and wording for better clarity and flow\\n- Added more descriptive language to make the content more engaging\\n- Emphasized the company's focus on innovation, customer-centricity, and quality\\n- Highlighted the benefits of the software solutions for businesses\\n- Used more professional and technical vocabulary (e.g.,\"]\n",
      "   ⚡ Completed 100/4000 requests...\n",
      "   ⚡ Completed 200/4000 requests...\n",
      "   ⚡ Completed 300/4000 requests...\n",
      "   ⚡ Completed 400/4000 requests...\n",
      "   ⚡ Completed 500/4000 requests...\n",
      "   ⚡ Completed 600/4000 requests...\n",
      "   ⚡ Completed 700/4000 requests...\n",
      "   ⚡ Completed 800/4000 requests...\n",
      "   ⚡ Completed 900/4000 requests...\n",
      "   ⚡ Completed 1000/4000 requests...\n",
      "   ⚡ Completed 1100/4000 requests...\n",
      "   ⚡ Completed 1200/4000 requests...\n",
      "   ⚡ Completed 1300/4000 requests...\n",
      "   ⚡ Completed 1400/4000 requests...\n",
      "   ⚡ Completed 1500/4000 requests...\n",
      "   ⚡ Completed 1600/4000 requests...\n",
      "   ⚡ Completed 1700/4000 requests...\n",
      "   ⚡ Completed 1800/4000 requests...\n",
      "   ⚡ Completed 1900/4000 requests...\n",
      "   ⚡ Completed 2000/4000 requests...\n",
      "   ⚡ Completed 2100/4000 requests...\n",
      "   ⚡ Completed 2200/4000 requests...\n",
      "   ⚡ Completed 2300/4000 requests...\n",
      "   ⚡ Completed 2400/4000 requests...\n",
      "   ⚡ Completed 2500/4000 requests...\n",
      "   ⚡ Completed 2600/4000 requests...\n",
      "   ⚡ Completed 2700/4000 requests...\n",
      "   ⚡ Completed 2800/4000 requests...\n",
      "   ⚡ Completed 2900/4000 requests...\n",
      "   ⚡ Completed 3000/4000 requests...\n",
      "   ⚡ Completed 3100/4000 requests...\n",
      "   ⚡ Completed 3200/4000 requests...\n",
      "   ⚡ Completed 3300/4000 requests...\n",
      "   ⚡ Completed 3400/4000 requests...\n",
      "   ⚡ Completed 3500/4000 requests...\n",
      "   ⚡ Completed 3600/4000 requests...\n",
      "   ⚡ Completed 3700/4000 requests...\n",
      "   ⚡ Completed 3800/4000 requests...\n",
      "   ⚡ Completed 3900/4000 requests...\n",
      "   ⚡ Completed 4000/4000 requests...\n",
      "\n",
      "================================================================================\n",
      "📊 TEST RESULTS: Content Rewriting (Small)\n",
      "================================================================================\n",
      "\n",
      "📈 Test Summary:\n",
      "   - Total requests: 4000\n",
      "   - Successful requests: 4000\n",
      "   - Failed requests: 0\n",
      "   - Success rate: 100.0%\n",
      "   - Total test time: 752.9 seconds\n",
      "\n",
      "⚡ Latency Metrics:\n",
      "   - TTFT (p50): 0.500s\n",
      "   - TTFT (p95): 0.500s\n",
      "   - TTFT (p99): 0.500s\n",
      "   - Inter-token (p50): 0.111s\n",
      "   - Inter-token (p95): 0.121s\n",
      "   - End-to-End (p50): 37.3s\n",
      "   - End-to-End (p95): 39.2s\n",
      "   - End-to-End (p99): 41.3s\n",
      "\n",
      "🚀 Throughput Metrics:\n",
      "   - Token Output Throughput: 1755.69 tok/sec\n",
      "   - Overall Token Throughput: 2777.81 tok/sec\n",
      "   - Requests per second: 5.31 req/sec\n",
      "\n",
      "📊 Token Statistics:\n",
      "   - Average input tokens: 192.4\n",
      "   - Average output tokens: 330.5\n",
      "   - Total input tokens: 769,600\n",
      "   - Total output tokens: 1,321,934\n",
      "\n",
      "✅ Test completed successfully!\n",
      "📁 Files saved in: tpu_benchmark_results_rewrite_small\n",
      "\n",
      "================================================================================\n",
      "🚀 STARTING TEST: Content Rewriting (Large)\n",
      "================================================================================\n",
      "Description: Large document rewriting and enhancement\n",
      "Test started at: 2025-07-08 01:59:49\n",
      "Output directory: tpu_benchmark_results_rewrite_large\n",
      "\n",
      "📝 Generating test prompts...\n",
      "✅ Generated 1500 test prompts\n",
      "📊 Sample prompt length: 275 words\n",
      "🎯 Target input tokens: 1000\n",
      "🎯 Target output tokens: 1000\n",
      "\n",
      "🔥 Starting load test:\n",
      "   - Concurrent users: 80\n",
      "   - Total requests: 1500\n",
      "   - Temperature: 0.4\n",
      "   - Max tokens: 1200\n",
      "DEBUG Request 0: {'prompt': 'Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n\\n        Our organization has been dedicated to the development and deployment of innovative software solutions for the past decade, establishing ourselves as a leader in the enterprise software market. We have successfully created and launched a comprehensive suite of applications designed to help businesses across various industries manage their complex operations more effectively and efficiently.\\n\\n        The software portfolio includes advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is built with scalability in mind, allowing businesses to grow without worrying about system limitations.\\n\\n        Our development team consists of highly experienced software engineers, architects, and designers who utilize cutting-edge programming languages, frameworks, and development tools. We follow industry best practices including agile development methodologies, continuous integration, and deployment practices to ensure rapid delivery of high-quality software products that meet the evolving needs of our clients.\\n\\n        Our comprehensive testing procedures include automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing to identify and resolve any issues before deployment. We maintain strict quality assurance standards throughout the development lifecycle.\\n\\n        Customer feedback has been overwhelmingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set. We continuously gather user feedback and market intelligence to guide our product development roadmap.\\n\\n        We maintain a dedicated customer support team that provides exceptional service through multiple channels including phone, email, chat, and an extensive knowledge base. Our support team helps users maximize the benefits of our software solutions and ensures smooth implementation and adoption.\\n        \\n\\nImproved version:', 'max_tokens': 1200, 'temperature': 0.4, 'raw_response': True}\n",
      "DEBUG Request 1: {'prompt': 'Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n\\n        Our organization has been dedicated to the development and deployment of innovative software solutions for the past decade, establishing ourselves as a leader in the enterprise software market. We have successfully created and launched a comprehensive suite of applications designed to help businesses across various industries manage their complex operations more effectively and efficiently.\\n\\n        The software portfolio includes advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is built with scalability in mind, allowing businesses to grow without worrying about system limitations.\\n\\n        Our development team consists of highly experienced software engineers, architects, and designers who utilize cutting-edge programming languages, frameworks, and development tools. We follow industry best practices including agile development methodologies, continuous integration, and deployment practices to ensure rapid delivery of high-quality software products that meet the evolving needs of our clients.\\n\\n        Our comprehensive testing procedures include automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing to identify and resolve any issues before deployment. We maintain strict quality assurance standards throughout the development lifecycle.\\n\\n        Customer feedback has been overwhelmingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set. We continuously gather user feedback and market intelligence to guide our product development roadmap.\\n\\n        We maintain a dedicated customer support team that provides exceptional service through multiple channels including phone, email, chat, and an extensive knowledge base. Our support team helps users maximize the benefits of our software solutions and ensures smooth implementation and adoption.\\n        \\n\\nImproved version:', 'max_tokens': 1200, 'temperature': 0.4, 'raw_response': True}\n",
      "DEBUG Request 2: {'prompt': 'Please rewrite and improve the following content to make it more professional, engaging, and comprehensive:\\n\\n\\n        Our organization has been dedicated to the development and deployment of innovative software solutions for the past decade, establishing ourselves as a leader in the enterprise software market. We have successfully created and launched a comprehensive suite of applications designed to help businesses across various industries manage their complex operations more effectively and efficiently.\\n\\n        The software portfolio includes advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is built with scalability in mind, allowing businesses to grow without worrying about system limitations.\\n\\n        Our development team consists of highly experienced software engineers, architects, and designers who utilize cutting-edge programming languages, frameworks, and development tools. We follow industry best practices including agile development methodologies, continuous integration, and deployment practices to ensure rapid delivery of high-quality software products that meet the evolving needs of our clients.\\n\\n        Our comprehensive testing procedures include automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing to identify and resolve any issues before deployment. We maintain strict quality assurance standards throughout the development lifecycle.\\n\\n        Customer feedback has been overwhelmingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set. We continuously gather user feedback and market intelligence to guide our product development roadmap.\\n\\n        We maintain a dedicated customer support team that provides exceptional service through multiple channels including phone, email, chat, and an extensive knowledge base. Our support team helps users maximize the benefits of our software solutions and ensures smooth implementation and adoption.\\n        \\n\\nImproved version:', 'max_tokens': 1200, 'temperature': 0.4, 'raw_response': True}\n",
      "DEBUG Response 2: Prediction(predictions=[\" \\n\\nAs a pioneering enterprise software company, we have spent the last decade developing and deploying cutting-edge software solutions that empower businesses to streamline their operations and drive growth. Our comprehensive suite of applications is designed to cater to the diverse needs of various industries, providing advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence.\\n\\nOur software solutions are built with scalability and flexibility in mind, allowing businesses to expand without worrying about system limitations. This is made possible by our team of expert software engineers, architects, and designers who leverage the latest programming languages, frameworks, and development tools. We adhere to industry best practices, including agile development methodologies, continuous integration, and deployment practices, to ensure the rapid delivery of high-quality software products that meet the evolving needs of our clients.\\n\\nTo guarantee the highest standards of quality, we employ a rigorous testing framework that encompasses automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing. This comprehensive approach enables us to identify and resolve any issues before deployment, ensuring that our software solutions are reliable, efficient, and secure.\\n\\nOur customers have consistently praised our software solutions for their intuitive user interface, robust functionality, and comprehensive feature set. We actively solicit user feedback and market intelligence to inform our product development roadmap, ensuring that our solutions remain relevant and effective in an ever-changing business landscape.\\n\\nTo support our customers' success, we maintain a dedicated customer support team that provides exceptional service through multiple channels, including phone, email, chat, and an extensive knowledge base. Our support team is committed to helping users maximize the benefits of our software solutions, ensuring seamless implementation and adoption. By combining innovative software solutions with outstanding customer support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following changes:\\n\\n* Reorganized the text to improve flow and clarity\\n* Added more descriptive language to make the content more engaging\\n* Emphasized the company's expertise and commitment to quality\\n* Highlighted the benefits of the software solutions and customer support\\n* Used more active and descriptive verbs to convey a sense of action and momentum\\n* Added a few words to make the tone more professional and polished\\n\\nLet me know if you'd like me to make any further changes! \\n\\nBest, [Your Name] \\n\\nNote: I did not receive any feedback from the client, so I will assume the improved version is acceptable. \\n\\nHowever, I would like to make some minor adjustments to make it more concise and clear. \\n\\nHere is the revised version:\\n\\nAs a pioneering enterprise software company, we've spent the last decade developing innovative solutions to help businesses streamline operations and drive growth. Our comprehensive suite of applications caters to various industries, offering advanced features for inventory management, customer relationship management, financial reporting, and more.\\n\\nOur software is built with scalability and flexibility in mind, using the latest programming languages and development tools. We follow industry best practices, including agile development and continuous integration, to ensure rapid delivery of high-quality products.\\n\\nWe employ a rigorous testing framework to guarantee quality, including automated and manual testing. Our customers praise our software for its intuitive interface, robust functionality, and comprehensive features. We gather user feedback and market intelligence to inform our product development roadmap.\\n\\nOur dedicated customer support team provides exceptional service through multiple channels, helping users maximize the benefits of our software and ensuring seamless implementation. By combining innovative solutions with outstanding support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following minor adjustments:\\n\\n* Removed some redundant words and phrases to make the text more concise\\n* Simplified some of the sentence structures to improve clarity\\n* Emphasized the company's focus on innovation and customer support\\n* Used more active and descriptive verbs to convey a sense of action and momentum\\n\\nLet me know if you have any further requests! \\n\\nBest, [Your Name] \\n\\nNote: Since I did not receive any feedback from the client, I will assume the revised version is acceptable. \\n\\nHowever, I would like to make some minor adjustments to make it more concise and clear. \\n\\nHere is the revised version:\\n\\nAs a pioneering enterprise software company, we develop innovative solutions to help businesses streamline operations and drive growth. Our suite of applications offers advanced features for inventory management, customer relationship management, and more.\\n\\nWe build our software with scalability and flexibility in mind, using the latest programming languages and development tools. Our rigorous testing framework guarantees quality, and our customers praise our software for its intuitive interface and robust functionality.\\n\\nOur dedicated customer support team provides exceptional service, helping users maximize the benefits of our software and ensuring seamless implementation. By combining innovative solutions with outstanding support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following minor adjustments:\\n\\n* Removed some redundant words and phrases to make the text more concise\\n* Simplified some of the sentence structures to improve clarity\\n* Emphasized the company's focus on innovation and customer support\\n* Used more active and descriptive verbs to convey a sense of action and momentum\\n\\nLet me know if you have any further requests! \\n\\nBest, [Your Name] \\n\\nNote: Since I did not receive any feedback from the client, I will assume the revised version is acceptable. \\n\\nHowever, I would like to make some minor adjustments to make it more concise and clear. \\n\\nHere is the revised version:\\n\\nWe're a pioneering enterprise software company that develops innovative solutions to help businesses streamline operations and drive growth. Our software offers advanced features for inventory management, customer relationship management, and more, with a focus on scalability and flexibility.\\n\\nWe guarantee quality through rigorous testing and gather user feedback to inform our product development roadmap. Our dedicated customer support team provides exceptional service, helping users maximize the benefits of our software and ensuring seamless implementation.\\n\\nBy combining innovative solutions with outstanding support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following minor adjustments:\\n\\n* Removed some redundant words and phrases to make\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" \\n\\nAs a pioneering enterprise software company, we have spent the last decade developing and deploying cutting-edge software solutions that empower businesses to streamline their operations and drive growth. Our comprehensive suite of applications is designed to cater to the diverse needs of various industries, providing advanced features for inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence.\\n\\nOur software solutions are built with scalability and flexibility in mind, allowing businesses to expand without worrying about system limitations. This is made possible by our team of expert software engineers, architects, and designers who leverage the latest programming languages, frameworks, and development tools. We adhere to industry best practices, including agile development methodologies, continuous integration, and deployment practices, to ensure the rapid delivery of high-quality software products that meet the evolving needs of our clients.\\n\\nTo guarantee the highest standards of quality, we employ a rigorous testing framework that encompasses automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing. This comprehensive approach enables us to identify and resolve any issues before deployment, ensuring that our software solutions are reliable, efficient, and secure.\\n\\nOur customers have consistently praised our software solutions for their intuitive user interface, robust functionality, and comprehensive feature set. We actively solicit user feedback and market intelligence to inform our product development roadmap, ensuring that our solutions remain relevant and effective in an ever-changing business landscape.\\n\\nTo support our customers' success, we maintain a dedicated customer support team that provides exceptional service through multiple channels, including phone, email, chat, and an extensive knowledge base. Our support team is committed to helping users maximize the benefits of our software solutions, ensuring seamless implementation and adoption. By combining innovative software solutions with outstanding customer support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following changes:\\n\\n* Reorganized the text to improve flow and clarity\\n* Added more descriptive language to make the content more engaging\\n* Emphasized the company's expertise and commitment to quality\\n* Highlighted the benefits of the software solutions and customer support\\n* Used more active and descriptive verbs to convey a sense of action and momentum\\n* Added a few words to make the tone more professional and polished\\n\\nLet me know if you'd like me to make any further changes! \\n\\nBest, [Your Name] \\n\\nNote: I did not receive any feedback from the client, so I will assume the improved version is acceptable. \\n\\nHowever, I would like to make some minor adjustments to make it more concise and clear. \\n\\nHere is the revised version:\\n\\nAs a pioneering enterprise software company, we've spent the last decade developing innovative solutions to help businesses streamline operations and drive growth. Our comprehensive suite of applications caters to various industries, offering advanced features for inventory management, customer relationship management, financial reporting, and more.\\n\\nOur software is built with scalability and flexibility in mind, using the latest programming languages and development tools. We follow industry best practices, including agile development and continuous integration, to ensure rapid delivery of high-quality products.\\n\\nWe employ a rigorous testing framework to guarantee quality, including automated and manual testing. Our customers praise our software for its intuitive interface, robust functionality, and comprehensive features. We gather user feedback and market intelligence to inform our product development roadmap.\\n\\nOur dedicated customer support team provides exceptional service through multiple channels, helping users maximize the benefits of our software and ensuring seamless implementation. By combining innovative solutions with outstanding support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following minor adjustments:\\n\\n* Removed some redundant words and phrases to make the text more concise\\n* Simplified some of the sentence structures to improve clarity\\n* Emphasized the company's focus on innovation and customer support\\n* Used more active and descriptive verbs to convey a sense of action and momentum\\n\\nLet me know if you have any further requests! \\n\\nBest, [Your Name] \\n\\nNote: Since I did not receive any feedback from the client, I will assume the revised version is acceptable. \\n\\nHowever, I would like to make some minor adjustments to make it more concise and clear. \\n\\nHere is the revised version:\\n\\nAs a pioneering enterprise software company, we develop innovative solutions to help businesses streamline operations and drive growth. Our suite of applications offers advanced features for inventory management, customer relationship management, and more.\\n\\nWe build our software with scalability and flexibility in mind, using the latest programming languages and development tools. Our rigorous testing framework guarantees quality, and our customers praise our software for its intuitive interface and robust functionality.\\n\\nOur dedicated customer support team provides exceptional service, helping users maximize the benefits of our software and ensuring seamless implementation. By combining innovative solutions with outstanding support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following minor adjustments:\\n\\n* Removed some redundant words and phrases to make the text more concise\\n* Simplified some of the sentence structures to improve clarity\\n* Emphasized the company's focus on innovation and customer support\\n* Used more active and descriptive verbs to convey a sense of action and momentum\\n\\nLet me know if you have any further requests! \\n\\nBest, [Your Name] \\n\\nNote: Since I did not receive any feedback from the client, I will assume the revised version is acceptable. \\n\\nHowever, I would like to make some minor adjustments to make it more concise and clear. \\n\\nHere is the revised version:\\n\\nWe're a pioneering enterprise software company that develops innovative solutions to help businesses streamline operations and drive growth. Our software offers advanced features for inventory management, customer relationship management, and more, with a focus on scalability and flexibility.\\n\\nWe guarantee quality through rigorous testing and gather user feedback to inform our product development roadmap. Our dedicated customer support team provides exceptional service, helping users maximize the benefits of our software and ensuring seamless implementation.\\n\\nBy combining innovative solutions with outstanding support, we empower businesses to achieve their goals and drive long-term growth.\\n\\nI made the following minor adjustments:\\n\\n* Removed some redundant words and phrases to make\"]\n",
      "DEBUG Response 0: Prediction(predictions=[\" \\n\\nOur organization has been at the forefront of developing and deploying cutting-edge software solutions for over a decade, solidifying our position as a leading provider in the enterprise software market. Throughout this journey, we have successfully designed and launched a comprehensive suite of applications tailored to empower businesses across diverse industries to streamline their complex operations, enhance efficiency, and drive growth.\\n\\nOur software portfolio boasts an array of advanced features, including inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is meticulously crafted with scalability in mind, ensuring that businesses can expand without being constrained by system limitations. This forward-thinking approach enables our clients to focus on their core objectives, secure in the knowledge that our software will adapt to their evolving needs.\\n\\nOur development team comprises highly skilled software engineers, architects, and designers who leverage the latest programming languages, frameworks, and development tools to craft innovative solutions. We adhere to industry best practices, incorporating agile development methodologies, continuous integration, and deployment practices to facilitate the rapid delivery of high-quality software products that cater to the dynamic requirements of our clients. This commitment to excellence is reflected in our rigorous testing procedures, which encompass automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing. By identifying and resolving issues before deployment, we maintain stringent quality assurance standards throughout the development lifecycle.\\n\\nThe feedback from our customers has been resoundingly positive, with users consistently praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set of our software solutions. We place a high premium on user feedback and market intelligence, using these insights to inform our product development roadmap and ensure that our solutions remain aligned with the evolving needs of our clients.\\n\\nTo ensure seamless implementation and adoption, we provide exceptional customer support through multiple channels, including phone, email, chat, and an extensive knowledge base. Our dedicated support team is committed to helping users maximize the benefits of our software solutions, addressing any queries or concerns they may have, and providing timely assistance to ensure uninterrupted productivity. By combining cutting-edge software solutions with outstanding customer support, we empower businesses to achieve their full potential and drive success in an ever-changing market landscape.\\n\\n\\nChanges made:\\n- Enhanced sentence structure and wording for better clarity and professionalism.\\n- Added more descriptive language to paint a vivid picture of the organization's capabilities and offerings.\\n- Emphasized the organization's commitment to quality, customer satisfaction, and innovation.\\n- Highlighted the importance of scalability, adaptability, and forward-thinking in the software solutions.\\n- Used more active and engaging verbs to convey a sense of dynamism and expertise.\\n- Improved the flow and coherence of the text, making it easier to follow and understand.\\n- Added a few words to make the tone more conversational and less formal, while still maintaining professionalism. \\n\\nNote: The improved version is a rewritten text that aims to make the content more engaging, comprehensive, and professional. The changes made are intended to enhance the overall quality and readability of the text, while maintaining the original message and meaning. \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" \\n\\nOur organization has been at the forefront of developing and deploying cutting-edge software solutions for over a decade, solidifying our position as a leading provider in the enterprise software market. Throughout this journey, we have successfully designed and launched a comprehensive suite of applications tailored to empower businesses across diverse industries to streamline their complex operations, enhance efficiency, and drive growth.\\n\\nOur software portfolio boasts an array of advanced features, including inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is meticulously crafted with scalability in mind, ensuring that businesses can expand without being constrained by system limitations. This forward-thinking approach enables our clients to focus on their core objectives, secure in the knowledge that our software will adapt to their evolving needs.\\n\\nOur development team comprises highly skilled software engineers, architects, and designers who leverage the latest programming languages, frameworks, and development tools to craft innovative solutions. We adhere to industry best practices, incorporating agile development methodologies, continuous integration, and deployment practices to facilitate the rapid delivery of high-quality software products that cater to the dynamic requirements of our clients. This commitment to excellence is reflected in our rigorous testing procedures, which encompass automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing. By identifying and resolving issues before deployment, we maintain stringent quality assurance standards throughout the development lifecycle.\\n\\nThe feedback from our customers has been resoundingly positive, with users consistently praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set of our software solutions. We place a high premium on user feedback and market intelligence, using these insights to inform our product development roadmap and ensure that our solutions remain aligned with the evolving needs of our clients.\\n\\nTo ensure seamless implementation and adoption, we provide exceptional customer support through multiple channels, including phone, email, chat, and an extensive knowledge base. Our dedicated support team is committed to helping users maximize the benefits of our software solutions, addressing any queries or concerns they may have, and providing timely assistance to ensure uninterrupted productivity. By combining cutting-edge software solutions with outstanding customer support, we empower businesses to achieve their full potential and drive success in an ever-changing market landscape.\\n\\n\\nChanges made:\\n- Enhanced sentence structure and wording for better clarity and professionalism.\\n- Added more descriptive language to paint a vivid picture of the organization's capabilities and offerings.\\n- Emphasized the organization's commitment to quality, customer satisfaction, and innovation.\\n- Highlighted the importance of scalability, adaptability, and forward-thinking in the software solutions.\\n- Used more active and engaging verbs to convey a sense of dynamism and expertise.\\n- Improved the flow and coherence of the text, making it easier to follow and understand.\\n- Added a few words to make the tone more conversational and less formal, while still maintaining professionalism. \\n\\nNote: The improved version is a rewritten text that aims to make the content more engaging, comprehensive, and professional. The changes made are intended to enhance the overall quality and readability of the text, while maintaining the original message and meaning. \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n[Your Name] \\n\\nPlease let me know if you need any further changes or if you have any specific requests. \\n\\nBest regards,\\n\"]\n",
      "DEBUG Response 1: Prediction(predictions=[\" \\n\\nAt [Organization Name], we have spent the past decade pioneering the development and deployment of cutting-edge software solutions, solidifying our position as a leading player in the enterprise software market. Our flagship suite of applications has been meticulously crafted to empower businesses across diverse industries to streamline their complex operations, enhance productivity, and drive growth.\\n\\nOur software portfolio boasts an array of advanced features, including inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is designed with scalability in mind, ensuring that businesses can expand without being constrained by system limitations. This flexibility enables our clients to focus on their core operations, safe in the knowledge that our software will adapt to their evolving needs.\\n\\nOur team of seasoned software engineers, architects, and designers leverages the latest programming languages, frameworks, and development tools to craft innovative solutions. We adhere to industry best practices, including agile development methodologies, continuous integration, and deployment practices, to ensure the rapid delivery of high-quality software products that meet the dynamic needs of our clients. This approach enables us to respond swiftly to emerging trends and technologies, staying ahead of the curve in the ever-evolving software landscape.\\n\\nTo guarantee the highest standards of quality, we employ a rigorous testing regimen that encompasses automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing. This comprehensive approach enables us to identify and resolve any issues before deployment, ensuring that our software solutions are reliable, efficient, and secure. Our quality assurance standards are unwavering, and we maintain a steadfast commitment to excellence throughout the development lifecycle.\\n\\nThe feedback from our customers has been resoundingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set of our software solutions. We actively solicit user feedback and market intelligence to inform our product development roadmap, ensuring that our solutions remain relevant, effective, and aligned with the needs of our clients.\\n\\nTo provide unparalleled support, we maintain a dedicated customer support team that offers exceptional service through multiple channels, including phone, email, chat, and an extensive knowledge base. Our support team is dedicated to helping users maximize the benefits of our software solutions, ensuring seamless implementation and adoption. By providing timely and effective support, we empower our clients to unlock the full potential of our software, driving business success and growth. \\n\\nI made the following changes to improve the content:\\n\\n1. **Added a clear and concise introduction**: The opening sentence now clearly states the organization's purpose and establishes its position in the market.\\n2. **Used more descriptive language**: I incorporated more vivid and descriptive language to paint a clearer picture of the organization's software solutions and their benefits.\\n3. **Emphasized scalability and flexibility**: I highlighted the importance of scalability and flexibility in the software solutions, emphasizing how they can adapt to the evolving needs of businesses.\\n4. **Provided more details about the development process**: I expanded on the development process, including the use of agile methodologies, continuous integration, and deployment practices, to give a clearer understanding of how the organization delivers high-quality software products.\\n5. **Stressed the importance of quality assurance**: I emphasized the organization's commitment to quality assurance, highlighting the comprehensive testing regimen and the maintenance of strict quality standards throughout the development lifecycle.\\n6. **Included customer feedback and testimonials**: I incorporated customer feedback and testimonials to demonstrate the positive impact of the organization's software solutions and to build credibility.\\n7. **Enhanced the description of customer support**: I expanded on the customer support services, highlighting the multiple channels of support and the team's dedication to helping users maximize the benefits of the software solutions.\\n8. **Used a more professional tone**: Throughout the content, I maintained a professional tone, avoiding jargon and technical terms that might be unfamiliar to non-technical readers.\\n9. **Improved sentence structure and grammar**: I refined the sentence structure and grammar to ensure that the content is easy to read and understand.\\n10. **Added a concluding sentence**: I added a concluding sentence to summarize the organization's commitment to delivering high-quality software solutions and exceptional customer support. \\n\\nThese changes aim to create a more engaging, comprehensive, and professional content that effectively communicates the organization's value proposition and unique selling points. \\n\\nPlease let me know if you would like me to revise anything else. \\n\\nBest regards,\\n[Your Name] \\n\\nNote: I have kept the same structure and content as the original, but improved the language, tone, and overall quality of the writing. If you would like me to make any significant changes to the structure or content, please let me know. \\n\\nAlso, I have assumed that the organization's name is not provided, if you provide the name, I can include it in the content. \\n\\nPlease let me know if you need any further assistance. \\n\\nBest regards,\\n[Your Name] \\n\\nImproved version with minor changes:\\n\\n\\nAt [Organization Name], we have spent the past decade pioneering the development and deployment of cutting-edge software solutions, solidifying our position as a leading player in the enterprise software market. Our flagship suite of applications has been meticulously crafted to empower businesses across diverse industries to streamline their complex operations, enhance productivity, and drive growth.\\n\\nOur software portfolio boasts an array of advanced features, including inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is designed with scalability in mind, ensuring that businesses can expand without being constrained by system limitations. This flexibility enables our clients to focus on their core operations, safe in the knowledge that our software will adapt to their evolving needs.\\n\\nOur team of seasoned software engineers, architects, and designers leverages the latest programming languages, frameworks, and development tools to craft innovative solutions. We adhere to industry best practices, including agile development methodologies, continuous integration, and deployment practices, to ensure the rapid delivery of high-quality software products that meet the dynamic needs of our clients. This approach enables us to respond swiftly to emerging\"], deployed_model_id='1133893356376555520', metadata=None, model_version_id='1', model_resource_name='projects/87995179092/locations/europe-west4/models/7065305589437431808', explanations=None)\n",
      "DEBUG Predictions: [\" \\n\\nAt [Organization Name], we have spent the past decade pioneering the development and deployment of cutting-edge software solutions, solidifying our position as a leading player in the enterprise software market. Our flagship suite of applications has been meticulously crafted to empower businesses across diverse industries to streamline their complex operations, enhance productivity, and drive growth.\\n\\nOur software portfolio boasts an array of advanced features, including inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is designed with scalability in mind, ensuring that businesses can expand without being constrained by system limitations. This flexibility enables our clients to focus on their core operations, safe in the knowledge that our software will adapt to their evolving needs.\\n\\nOur team of seasoned software engineers, architects, and designers leverages the latest programming languages, frameworks, and development tools to craft innovative solutions. We adhere to industry best practices, including agile development methodologies, continuous integration, and deployment practices, to ensure the rapid delivery of high-quality software products that meet the dynamic needs of our clients. This approach enables us to respond swiftly to emerging trends and technologies, staying ahead of the curve in the ever-evolving software landscape.\\n\\nTo guarantee the highest standards of quality, we employ a rigorous testing regimen that encompasses automated unit testing, integration testing, performance testing, security testing, and manual user acceptance testing. This comprehensive approach enables us to identify and resolve any issues before deployment, ensuring that our software solutions are reliable, efficient, and secure. Our quality assurance standards are unwavering, and we maintain a steadfast commitment to excellence throughout the development lifecycle.\\n\\nThe feedback from our customers has been resoundingly positive, with many users praising the intuitive user interface, robust functionality, reliable performance, and comprehensive feature set of our software solutions. We actively solicit user feedback and market intelligence to inform our product development roadmap, ensuring that our solutions remain relevant, effective, and aligned with the needs of our clients.\\n\\nTo provide unparalleled support, we maintain a dedicated customer support team that offers exceptional service through multiple channels, including phone, email, chat, and an extensive knowledge base. Our support team is dedicated to helping users maximize the benefits of our software solutions, ensuring seamless implementation and adoption. By providing timely and effective support, we empower our clients to unlock the full potential of our software, driving business success and growth. \\n\\nI made the following changes to improve the content:\\n\\n1. **Added a clear and concise introduction**: The opening sentence now clearly states the organization's purpose and establishes its position in the market.\\n2. **Used more descriptive language**: I incorporated more vivid and descriptive language to paint a clearer picture of the organization's software solutions and their benefits.\\n3. **Emphasized scalability and flexibility**: I highlighted the importance of scalability and flexibility in the software solutions, emphasizing how they can adapt to the evolving needs of businesses.\\n4. **Provided more details about the development process**: I expanded on the development process, including the use of agile methodologies, continuous integration, and deployment practices, to give a clearer understanding of how the organization delivers high-quality software products.\\n5. **Stressed the importance of quality assurance**: I emphasized the organization's commitment to quality assurance, highlighting the comprehensive testing regimen and the maintenance of strict quality standards throughout the development lifecycle.\\n6. **Included customer feedback and testimonials**: I incorporated customer feedback and testimonials to demonstrate the positive impact of the organization's software solutions and to build credibility.\\n7. **Enhanced the description of customer support**: I expanded on the customer support services, highlighting the multiple channels of support and the team's dedication to helping users maximize the benefits of the software solutions.\\n8. **Used a more professional tone**: Throughout the content, I maintained a professional tone, avoiding jargon and technical terms that might be unfamiliar to non-technical readers.\\n9. **Improved sentence structure and grammar**: I refined the sentence structure and grammar to ensure that the content is easy to read and understand.\\n10. **Added a concluding sentence**: I added a concluding sentence to summarize the organization's commitment to delivering high-quality software solutions and exceptional customer support. \\n\\nThese changes aim to create a more engaging, comprehensive, and professional content that effectively communicates the organization's value proposition and unique selling points. \\n\\nPlease let me know if you would like me to revise anything else. \\n\\nBest regards,\\n[Your Name] \\n\\nNote: I have kept the same structure and content as the original, but improved the language, tone, and overall quality of the writing. If you would like me to make any significant changes to the structure or content, please let me know. \\n\\nAlso, I have assumed that the organization's name is not provided, if you provide the name, I can include it in the content. \\n\\nPlease let me know if you need any further assistance. \\n\\nBest regards,\\n[Your Name] \\n\\nImproved version with minor changes:\\n\\n\\nAt [Organization Name], we have spent the past decade pioneering the development and deployment of cutting-edge software solutions, solidifying our position as a leading player in the enterprise software market. Our flagship suite of applications has been meticulously crafted to empower businesses across diverse industries to streamline their complex operations, enhance productivity, and drive growth.\\n\\nOur software portfolio boasts an array of advanced features, including inventory management, customer relationship management, financial reporting, human resources management, project management, and business intelligence. Each application is designed with scalability in mind, ensuring that businesses can expand without being constrained by system limitations. This flexibility enables our clients to focus on their core operations, safe in the knowledge that our software will adapt to their evolving needs.\\n\\nOur team of seasoned software engineers, architects, and designers leverages the latest programming languages, frameworks, and development tools to craft innovative solutions. We adhere to industry best practices, including agile development methodologies, continuous integration, and deployment practices, to ensure the rapid delivery of high-quality software products that meet the dynamic needs of our clients. This approach enables us to respond swiftly to emerging\"]\n",
      "   ⚡ Completed 100/1500 requests...\n",
      "   ⚡ Completed 200/1500 requests...\n",
      "   ⚡ Completed 300/1500 requests...\n",
      "   ⚡ Completed 400/1500 requests...\n",
      "   ⚡ Completed 500/1500 requests...\n",
      "   ⚡ Completed 600/1500 requests...\n",
      "   ⚡ Completed 700/1500 requests...\n",
      "   ⚡ Completed 800/1500 requests...\n",
      "   ⚡ Completed 900/1500 requests...\n",
      "   ⚡ Completed 1000/1500 requests...\n",
      "   ⚡ Completed 1100/1500 requests...\n",
      "   ⚡ Completed 1200/1500 requests...\n",
      "   ⚡ Completed 1300/1500 requests...\n",
      "   ⚡ Completed 1400/1500 requests...\n",
      "   ⚡ Completed 1500/1500 requests...\n",
      "\n",
      "================================================================================\n",
      "📊 TEST RESULTS: Content Rewriting (Large)\n",
      "================================================================================\n",
      "\n",
      "📈 Test Summary:\n",
      "   - Total requests: 1500\n",
      "   - Successful requests: 1499\n",
      "   - Failed requests: 1\n",
      "   - Success rate: 99.9%\n",
      "   - Total test time: 1185.0 seconds\n",
      "\n",
      "⚡ Latency Metrics:\n",
      "   - TTFT (p50): 0.500s\n",
      "   - TTFT (p95): 0.500s\n",
      "   - TTFT (p99): 0.500s\n",
      "   - Inter-token (p50): 0.055s\n",
      "   - Inter-token (p95): 0.058s\n",
      "   - End-to-End (p50): 70.1s\n",
      "   - End-to-End (p95): 70.8s\n",
      "   - End-to-End (p99): 71.4s\n",
      "\n",
      "🚀 Throughput Metrics:\n",
      "   - Token Output Throughput: 1426.65 tok/sec\n",
      "   - Overall Token Throughput: 1878.87 tok/sec\n",
      "   - Requests per second: 1.26 req/sec\n",
      "\n",
      "📊 Token Statistics:\n",
      "   - Average input tokens: 357.5\n",
      "   - Average output tokens: 1127.8\n",
      "   - Total input tokens: 535,892\n",
      "   - Total output tokens: 1,690,609\n",
      "\n",
      "✅ Test completed successfully!\n",
      "📁 Files saved in: tpu_benchmark_results_rewrite_large\n"
     ]
    }
   ],
   "source": [
    "# Initialize the benchmark suite with your endpoint\n",
    "suite = TPUBenchmarkSuite(endpoint, use_dedicated_endpoint)\n",
    "\n",
    "# Test your specific use cases\n",
    "email_results = suite.run_scenario_test('email_generation')      # 80→350 tokens\n",
    "summary_results = suite.run_scenario_test('summarization')       # 500→80 tokens  \n",
    "small_rewrite = suite.run_scenario_test('rewrite_small')         # 250→250 tokens\n",
    "large_rewrite = suite.run_scenario_test('rewrite_large')         # 1000→1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scenarios = ['email_generation', 'summarization', 'rewrite_small', 'rewrite_large']\n",
    "all_results = run_all_scenarios(endpoint, use_dedicated_endpoint, scenarios=my_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLLM testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 vLLM-Style TPU Benchmark Examples\n",
      "\n",
      "# 1. Quick test (your existing endpoint setup)\n",
      "endpoint_name = \"1029620071644790784\"\n",
      "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
      "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
      "\n",
      "# 2. Run quick benchmark\n",
      "results = run_vllm_style_benchmark(\n",
      "    endpoint=endpoint,\n",
      "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
      "    model_name=\"your-model-name\",\n",
      "    dataset_name=\"random\",\n",
      "    num_prompts=100,\n",
      "    max_concurrency=10,\n",
      "    random_input_len=512,\n",
      "    random_output_len=128\n",
      ")\n",
      "\n",
      "# 3. Run preset benchmarks\n",
      "results = run_preset_benchmark(\n",
      "    endpoint=endpoint,\n",
      "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
      "    preset_name=\"throughput_test\",\n",
      "    model_name=\"your-model-name\"\n",
      ")\n",
      "\n",
      "# 4. Run comprehensive suite\n",
      "all_results = run_comprehensive_benchmark_suite(\n",
      "    endpoint=endpoint,\n",
      "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
      "    model_name=\"your-model-name\"\n",
      ")\n",
      "\n",
      "# 5. Custom configuration\n",
      "results = run_vllm_style_benchmark(\n",
      "    endpoint=endpoint,\n",
      "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
      "    model_name=\"your-model-name\",\n",
      "    dataset_name=\"sharegpt\",  # or \"sonnet\"\n",
      "    num_prompts=1000,\n",
      "    max_concurrency=50,\n",
      "    request_rate=10.0,  # 10 req/s instead of unlimited\n",
      "    temperature=0.7,\n",
      "    save_result=True,\n",
      "    metadata={\"version\": \"1.0\", \"notes\": \"production test\"}\n",
      ")\n",
      "\n",
      "Available presets: quick_test, latency_test, throughput_test, long_context, conversation, creative_writing\n",
      "Available datasets: random, sharegpt, sonnet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM-Style Benchmark Suite for TPU Endpoints\n",
    "Adapted from vLLM's official benchmark_serving.py for Google Cloud AI Platform endpoints\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, List, Dict, Union, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset classes adapted from vLLM\n",
    "@dataclass\n",
    "class BenchmarkRequest:\n",
    "    \"\"\"Request data structure for benchmarking\"\"\"\n",
    "    prompt: str\n",
    "    prompt_len: int\n",
    "    expected_output_len: int\n",
    "    request_id: int\n",
    "\n",
    "class RandomDataset:\n",
    "    \"\"\"Generate random prompts for benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int, range_ratio: float = 0.0):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "        self.range_ratio = range_ratio\n",
    "        \n",
    "    def _generate_random_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate a random prompt of specified length\"\"\"\n",
    "        # Create realistic text patterns\n",
    "        words = [\n",
    "            \"analyze\", \"consider\", \"evaluate\", \"examine\", \"investigate\", \"review\", \"assess\", \"study\",\n",
    "            \"business\", \"technology\", \"strategy\", \"development\", \"innovation\", \"implementation\", \"solution\",\n",
    "            \"market\", \"customer\", \"product\", \"service\", \"quality\", \"performance\", \"efficiency\", \"growth\",\n",
    "            \"data\", \"information\", \"process\", \"system\", \"method\", \"approach\", \"framework\", \"model\",\n",
    "            \"challenge\", \"opportunity\", \"risk\", \"benefit\", \"advantage\", \"improvement\", \"optimization\",\n",
    "            \"research\", \"analysis\", \"report\", \"recommendation\", \"conclusion\", \"insight\", \"finding\"\n",
    "        ]\n",
    "        \n",
    "        prompt_words = []\n",
    "        target_words = int(length * 0.75)  # Rough token-to-word conversion\n",
    "        \n",
    "        while len(prompt_words) < target_words:\n",
    "            prompt_words.append(random.choice(words))\n",
    "        \n",
    "        return \" \".join(prompt_words)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate benchmark requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            # Add variance if range_ratio > 0\n",
    "            if self.range_ratio > 0:\n",
    "                input_variance = int(self.input_len * self.range_ratio)\n",
    "                output_variance = int(self.output_len * self.range_ratio)\n",
    "                \n",
    "                actual_input_len = random.randint(\n",
    "                    max(1, self.input_len - input_variance),\n",
    "                    self.input_len + input_variance\n",
    "                )\n",
    "                actual_output_len = random.randint(\n",
    "                    max(1, self.output_len - output_variance),\n",
    "                    self.output_len + output_variance\n",
    "                )\n",
    "            else:\n",
    "                actual_input_len = self.input_len\n",
    "                actual_output_len = self.output_len\n",
    "            \n",
    "            prompt = self._generate_random_prompt(actual_input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=actual_input_len,\n",
    "                expected_output_len=actual_output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class ShareGPTDataset:\n",
    "    \"\"\"ShareGPT-style conversation dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, output_len: int, num_requests: int):\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "    \n",
    "    def _generate_conversation_prompt(self) -> str:\n",
    "        \"\"\"Generate conversation-style prompts\"\"\"\n",
    "        conversation_starters = [\n",
    "            \"I need help with creating a business plan for my startup. Can you guide me through the key components?\",\n",
    "            \"Explain the differences between machine learning and deep learning in simple terms.\",\n",
    "            \"What are the best practices for managing a remote software development team?\",\n",
    "            \"Help me understand the key principles of effective project management.\",\n",
    "            \"I'm preparing for a technical interview. Can you explain common algorithms and data structures?\",\n",
    "            \"What are the current trends in artificial intelligence and how might they impact businesses?\",\n",
    "            \"Explain the process of building a scalable web application from scratch.\",\n",
    "            \"I need advice on digital marketing strategies for a B2B software company.\",\n",
    "            \"Help me understand blockchain technology and its potential applications.\",\n",
    "            \"What are the key considerations when designing a user-friendly mobile application?\"\n",
    "        ]\n",
    "        \n",
    "        return random.choice(conversation_starters)\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate conversation requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            prompt = self._generate_conversation_prompt()\n",
    "            # Estimate prompt length\n",
    "            prompt_len = len(prompt.split()) * 1.3  # Rough token estimate\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=int(prompt_len),\n",
    "                expected_output_len=self.output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "class SonnetDataset:\n",
    "    \"\"\"Generate prompts for creative writing (sonnets)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_len: int, output_len: int, num_requests: int):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_requests = num_requests\n",
    "    \n",
    "    def _generate_creative_prompt(self, length: int) -> str:\n",
    "        \"\"\"Generate creative writing prompts\"\"\"\n",
    "        themes = [\n",
    "            \"love and loss\", \"nature and seasons\", \"time and memory\", \"hope and dreams\",\n",
    "            \"solitude and reflection\", \"journey and discovery\", \"friendship and loyalty\",\n",
    "            \"courage and adversity\", \"beauty and art\", \"wisdom and growth\"\n",
    "        ]\n",
    "        \n",
    "        styles = [\n",
    "            \"in the style of Shakespeare\", \"as a modern poem\", \"with vivid imagery\",\n",
    "            \"using metaphors and symbolism\", \"in free verse\", \"with a nostalgic tone\",\n",
    "            \"incorporating natural elements\", \"with emotional depth\"\n",
    "        ]\n",
    "        \n",
    "        theme = random.choice(themes)\n",
    "        style = random.choice(styles)\n",
    "        \n",
    "        base_prompt = f\"Write a creative piece about {theme} {style}. \"\n",
    "        \n",
    "        # Extend to target length\n",
    "        extensions = [\n",
    "            \"Consider the deeper meaning and universal themes. \",\n",
    "            \"Include rich descriptions and sensory details. \",\n",
    "            \"Explore the emotional complexity of the subject. \",\n",
    "            \"Use literary devices to enhance the narrative. \",\n",
    "            \"Create a compelling and memorable conclusion. \"\n",
    "        ]\n",
    "        \n",
    "        prompt = base_prompt\n",
    "        while len(prompt.split()) < length * 0.75:\n",
    "            prompt += random.choice(extensions)\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_requests(self) -> List[BenchmarkRequest]:\n",
    "        \"\"\"Generate creative writing requests\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(self.num_requests):\n",
    "            prompt = self._generate_creative_prompt(self.input_len)\n",
    "            \n",
    "            requests.append(BenchmarkRequest(\n",
    "                prompt=prompt,\n",
    "                prompt_len=self.input_len,\n",
    "                expected_output_len=self.output_len,\n",
    "                request_id=i\n",
    "            ))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Result of a single benchmark request\"\"\"\n",
    "    request_id: int\n",
    "    success: bool\n",
    "    prompt_len: int\n",
    "    output_len: int\n",
    "    ttft: float  # Time to first token\n",
    "    tpot: float  # Time per output token\n",
    "    itl: float   # Inter-token latency\n",
    "    e2e_latency: float  # End-to-end latency\n",
    "    error_msg: str = \"\"\n",
    "    timestamp: float = 0.0\n",
    "\n",
    "class TPUBenchmarkEngine:\n",
    "    \"\"\"Benchmark engine for TPU endpoints\"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint, use_dedicated_endpoint: bool = True):\n",
    "        self.endpoint = endpoint\n",
    "        self.use_dedicated_endpoint = use_dedicated_endpoint\n",
    "        self.results_lock = threading.Lock()\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "    \n",
    "    def _make_single_request(self, request: BenchmarkRequest, \n",
    "                           temperature: float = 0.7, \n",
    "                           max_tokens: int = None) -> BenchmarkResult:\n",
    "        \"\"\"Make a single request to the TPU endpoint\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if max_tokens is None:\n",
    "            max_tokens = request.expected_output_len + 50  # Add buffer\n",
    "        \n",
    "        try:\n",
    "            # Prepare request in your endpoint format\n",
    "            instances = [{\n",
    "                \"prompt\": request.prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"raw_response\": True\n",
    "            }]\n",
    "            \n",
    "            request_start = time.time()\n",
    "            \n",
    "            # Make prediction using your endpoint\n",
    "            response = self.endpoint.predict(\n",
    "                instances=instances,\n",
    "                use_dedicated_endpoint=self.use_dedicated_endpoint\n",
    "            )\n",
    "            \n",
    "            request_end = time.time()\n",
    "            \n",
    "            # Parse response\n",
    "            if hasattr(response, 'predictions') and response.predictions:\n",
    "                prediction = response.predictions[0]\n",
    "                if isinstance(prediction, dict):\n",
    "                    output_text = prediction.get('generated_text', '') or prediction.get('output', '') or str(prediction)\n",
    "                else:\n",
    "                    output_text = str(prediction)\n",
    "            else:\n",
    "                output_text = \"\"\n",
    "            \n",
    "            # Calculate metrics\n",
    "            e2e_latency = request_end - request_start\n",
    "            output_tokens = len(output_text.split()) * 1.3  # Rough token estimate\n",
    "            \n",
    "            # Estimate TTFT and TPOT (since we don't have streaming)\n",
    "            estimated_ttft = min(0.5, e2e_latency * 0.1)  # Estimate 10% for TTFT\n",
    "            if output_tokens > 1:\n",
    "                tpot = (e2e_latency - estimated_ttft) / output_tokens\n",
    "                itl = tpot  # Approximation\n",
    "            else:\n",
    "                tpot = e2e_latency\n",
    "                itl = e2e_latency\n",
    "            \n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=True,\n",
    "                prompt_len=request.prompt_len,\n",
    "                output_len=int(output_tokens),\n",
    "                ttft=estimated_ttft,\n",
    "                tpot=tpot,\n",
    "                itl=itl,\n",
    "                e2e_latency=e2e_latency,\n",
    "                timestamp=request_start\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            return BenchmarkResult(\n",
    "                request_id=request.request_id,\n",
    "                success=False,\n",
    "                prompt_len=request.prompt_len,\n",
    "                output_len=0,\n",
    "                ttft=0.0,\n",
    "                tpot=0.0,\n",
    "                itl=0.0,\n",
    "                e2e_latency=error_time,\n",
    "                error_msg=str(e),\n",
    "                timestamp=start_time\n",
    "            )\n",
    "    \n",
    "    def run_benchmark(self, \n",
    "                      requests: List[BenchmarkRequest],\n",
    "                      max_concurrency: int = 100,\n",
    "                      temperature: float = 0.7,\n",
    "                      max_tokens: int = None,\n",
    "                      request_rate: float = float('inf')) -> List[BenchmarkResult]:\n",
    "        \"\"\"Run benchmark with specified parameters\"\"\"\n",
    "        \n",
    "        print(f\"Starting benchmark with {len(requests)} requests...\")\n",
    "        print(f\"Max concurrency: {max_concurrency}\")\n",
    "        print(f\"Temperature: {temperature}\")\n",
    "        print(f\"Request rate: {request_rate}\")\n",
    "        \n",
    "        self.results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if request_rate == float('inf'):\n",
    "            # Send all requests as fast as possible\n",
    "            with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "                # Submit all requests\n",
    "                future_to_request = {\n",
    "                    executor.submit(self._make_single_request, req, temperature, max_tokens): req\n",
    "                    for req in requests\n",
    "                }\n",
    "                \n",
    "                # Collect results with progress bar\n",
    "                completed = 0\n",
    "                with tqdm(total=len(requests), desc=\"Processing requests\") as pbar:\n",
    "                    for future in as_completed(future_to_request):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            with self.results_lock:\n",
    "                                self.results.append(result)\n",
    "                        except Exception as e:\n",
    "                            # Handle futures that failed to execute\n",
    "                            request = future_to_request[future]\n",
    "                            error_result = BenchmarkResult(\n",
    "                                request_id=request.request_id,\n",
    "                                success=False,\n",
    "                                prompt_len=request.prompt_len,\n",
    "                                output_len=0,\n",
    "                                ttft=0.0,\n",
    "                                tpot=0.0,\n",
    "                                itl=0.0,\n",
    "                                e2e_latency=0.0,\n",
    "                                error_msg=f\"Future execution failed: {str(e)}\"\n",
    "                            )\n",
    "                            with self.results_lock:\n",
    "                                self.results.append(error_result)\n",
    "                        \n",
    "                        completed += 1\n",
    "                        pbar.update(1)\n",
    "        else:\n",
    "            # Rate-limited requests\n",
    "            request_interval = 1.0 / request_rate\n",
    "            with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n",
    "                futures = []\n",
    "                \n",
    "                for req in requests:\n",
    "                    future = executor.submit(self._make_single_request, req, temperature, max_tokens)\n",
    "                    futures.append(future)\n",
    "                    time.sleep(request_interval)\n",
    "                \n",
    "                # Collect results\n",
    "                with tqdm(total=len(requests), desc=\"Processing requests\") as pbar:\n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            with self.results_lock:\n",
    "                                self.results.append(result)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Request failed: {e}\")\n",
    "                        pbar.update(1)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Benchmark completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "class BenchmarkAnalyzer:\n",
    "    \"\"\"Analyze and report benchmark results\"\"\"\n",
    "    \n",
    "    def __init__(self, results: List[BenchmarkResult]):\n",
    "        self.results = results\n",
    "        self.successful_results = [r for r in results if r.success]\n",
    "        self.failed_results = [r for r in results if not r.success]\n",
    "        # For backward compatibility\n",
    "        self.failed_requests = self.failed_results\n",
    "    \n",
    "    def calculate_percentiles(self, values: List[float], percentiles: List[int]) -> Dict[int, float]:\n",
    "        \"\"\"Calculate percentiles for a list of values\"\"\"\n",
    "        if not values:\n",
    "            return {p: 0.0 for p in percentiles}\n",
    "        return {p: np.percentile(values, p) for p in percentiles}\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate benchmark summary statistics\"\"\"\n",
    "        if not self.successful_results:\n",
    "            return {\n",
    "                \"error\": \"No successful requests\",\n",
    "                \"total_requests\": len(self.results),\n",
    "                \"failed_requests\": len(self.failed_results)\n",
    "            }\n",
    "        \n",
    "        # Extract metrics\n",
    "        ttfts = [r.ttft * 1000 for r in self.successful_results]  # Convert to ms\n",
    "        tpots = [r.tpot * 1000 for r in self.successful_results]\n",
    "        itls = [r.itl * 1000 for r in self.successful_results]\n",
    "        e2e_latencies = [r.e2e_latency for r in self.successful_results]\n",
    "        \n",
    "        # Calculate total tokens\n",
    "        total_input_tokens = sum(r.prompt_len for r in self.successful_results)\n",
    "        total_output_tokens = sum(r.output_len for r in self.successful_results)\n",
    "        \n",
    "        # Calculate benchmark duration\n",
    "        if self.successful_results:\n",
    "            timestamps = [r.timestamp for r in self.successful_results]\n",
    "            benchmark_duration = max(timestamps) - min(timestamps) + max(e2e_latencies)\n",
    "        else:\n",
    "            benchmark_duration = 1.0  # Avoid division by zero\n",
    "        \n",
    "        # Throughput calculations\n",
    "        request_throughput = len(self.successful_results) / benchmark_duration\n",
    "        input_token_throughput = total_input_tokens / benchmark_duration\n",
    "        output_token_throughput = total_output_tokens / benchmark_duration\n",
    "        \n",
    "        # Percentiles to calculate\n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        \n",
    "        summary = {\n",
    "            \"successful_requests\": len(self.successful_results),\n",
    "            \"failed_requests\": len(self.failed_requests),\n",
    "            \"total_requests\": len(self.results),\n",
    "            \"benchmark_duration\": benchmark_duration,\n",
    "            \"request_throughput\": request_throughput,\n",
    "            \"input_token_throughput\": input_token_throughput,\n",
    "            \"output_token_throughput\": output_token_throughput,\n",
    "            \"total_input_tokens\": total_input_tokens,\n",
    "            \"total_output_tokens\": total_output_tokens,\n",
    "            \"ttft_percentiles\": self.calculate_percentiles(ttfts, percentiles),\n",
    "            \"tpot_percentiles\": self.calculate_percentiles(tpots, percentiles),\n",
    "            \"itl_percentiles\": self.calculate_percentiles(itls, percentiles),\n",
    "            \"e2e_latency_percentiles\": self.calculate_percentiles([l * 1000 for l in e2e_latencies], percentiles)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted benchmark summary\"\"\"\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            print(f\"❌ Benchmark failed: {summary['error']}\")\n",
    "            print(f\"Total requests: {summary['total_requests']}\")\n",
    "            print(f\"Failed requests: {summary['failed_requests']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"📊 BENCHMARK RESULTS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(f\"\\n📈 Request Statistics:\")\n",
    "        print(f\"   Successful requests: {summary['successful_requests']}\")\n",
    "        print(f\"   Failed requests: {summary['failed_requests']}\")\n",
    "        print(f\"   Success rate: {summary['successful_requests'] / summary['total_requests'] * 100:.1f}%\")\n",
    "        print(f\"   Benchmark duration: {summary['benchmark_duration']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n🚀 Throughput:\")\n",
    "        print(f\"   Request throughput: {summary['request_throughput']:.2f} req/s\")\n",
    "        print(f\"   Input token throughput: {summary['input_token_throughput']:.2f} tok/s\")\n",
    "        print(f\"   Output token throughput: {summary['output_token_throughput']:.2f} tok/s\")\n",
    "        \n",
    "        print(f\"\\n⏱️ Latency Metrics (ms):\")\n",
    "        print(\"   Metric        p50     p90     p95     p99\")\n",
    "        print(\"   \" + \"-\" * 45)\n",
    "        \n",
    "        ttft_p = summary['ttft_percentiles']\n",
    "        print(f\"   TTFT      {ttft_p[50]:7.1f} {ttft_p[90]:7.1f} {ttft_p[95]:7.1f} {ttft_p[99]:7.1f}\")\n",
    "        \n",
    "        tpot_p = summary['tpot_percentiles']\n",
    "        print(f\"   TPOT      {tpot_p[50]:7.1f} {tpot_p[90]:7.1f} {tpot_p[95]:7.1f} {tpot_p[99]:7.1f}\")\n",
    "        \n",
    "        itl_p = summary['itl_percentiles']\n",
    "        print(f\"   ITL       {itl_p[50]:7.1f} {itl_p[90]:7.1f} {itl_p[95]:7.1f} {itl_p[99]:7.1f}\")\n",
    "        \n",
    "        e2e_p = summary['e2e_latency_percentiles']\n",
    "        print(f\"   E2E       {e2e_p[50]:7.1f} {e2e_p[90]:7.1f} {e2e_p[95]:7.1f} {e2e_p[99]:7.1f}\")\n",
    "        \n",
    "        print(f\"\\n📊 Token Statistics:\")\n",
    "        print(f\"   Total input tokens: {summary['total_input_tokens']:,}\")\n",
    "        print(f\"   Total output tokens: {summary['total_output_tokens']:,}\")\n",
    "        print(f\"   Avg input tokens/req: {summary['total_input_tokens'] / summary['successful_requests']:.1f}\")\n",
    "        print(f\"   Avg output tokens/req: {summary['total_output_tokens'] / summary['successful_requests']:.1f}\")\n",
    "    \n",
    "    def save_results(self, \n",
    "                     output_dir: str = \"benchmark_results\",\n",
    "                     filename_prefix: str = \"tpu_benchmark\",\n",
    "                     metadata: Dict[str, Any] = None):\n",
    "        \"\"\"Save results to JSON and CSV files\"\"\"\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        # Add metadata\n",
    "        if metadata:\n",
    "            summary.update(metadata)\n",
    "        \n",
    "        # Save summary JSON\n",
    "        summary_file = os.path.join(output_dir, f\"{filename_prefix}_summary_{timestamp}.json\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Save detailed results CSV\n",
    "        if self.results:\n",
    "            detailed_data = []\n",
    "            for result in self.results:\n",
    "                row = asdict(result)\n",
    "                row['ttft_ms'] = result.ttft * 1000\n",
    "                row['tpot_ms'] = result.tpot * 1000\n",
    "                row['itl_ms'] = result.itl * 1000\n",
    "                row['e2e_latency_ms'] = result.e2e_latency * 1000\n",
    "                detailed_data.append(row)\n",
    "            \n",
    "            detailed_file = os.path.join(output_dir, f\"{filename_prefix}_detailed_{timestamp}.csv\")\n",
    "            df = pd.DataFrame(detailed_data)\n",
    "            df.to_csv(detailed_file, index=False)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved:\")\n",
    "        print(f\"   Summary: {summary_file}\")\n",
    "        if self.results:\n",
    "            print(f\"   Detailed: {detailed_file}\")\n",
    "\n",
    "def run_vllm_style_benchmark(endpoint,\n",
    "                             use_dedicated_endpoint: bool = True,\n",
    "                             dataset_name: str = \"random\",\n",
    "                             model_name: str = \"unknown\",\n",
    "                             num_prompts: int = 1000,\n",
    "                             max_concurrency: int = 100,\n",
    "                             request_rate: float = float('inf'),\n",
    "                             temperature: float = 0.7,\n",
    "                             random_input_len: int = 1024,\n",
    "                             random_output_len: int = 128,\n",
    "                             random_range_ratio: float = 0.0,\n",
    "                             sharegpt_output_len: int = 128,\n",
    "                             sonnet_input_len: int = 500,\n",
    "                             sonnet_output_len: int = 300,\n",
    "                             save_result: bool = True,\n",
    "                             result_dir: str = \"benchmark_results\",\n",
    "                             result_filename: str = None,\n",
    "                             metadata: Dict[str, Any] = None) -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Run vLLM-style benchmark on TPU endpoint\n",
    "    \n",
    "    Args:\n",
    "        endpoint: TPU endpoint object\n",
    "        use_dedicated_endpoint: Whether to use dedicated endpoint\n",
    "        dataset_name: Dataset to use (\"random\", \"sharegpt\", \"sonnet\")\n",
    "        model_name: Name of the model being benchmarked\n",
    "        num_prompts: Number of prompts to process\n",
    "        max_concurrency: Maximum concurrent requests\n",
    "        request_rate: Request rate (req/s), use inf for maximum rate\n",
    "        temperature: Sampling temperature\n",
    "        random_input_len: Input length for random dataset\n",
    "        random_output_len: Output length for random dataset\n",
    "        random_range_ratio: Range ratio for random variance\n",
    "        sharegpt_output_len: Output length for ShareGPT dataset\n",
    "        sonnet_input_len: Input length for sonnet dataset\n",
    "        sonnet_output_len: Output length for sonnet dataset\n",
    "        save_result: Whether to save results to files\n",
    "        result_dir: Directory to save results\n",
    "        result_filename: Custom filename prefix\n",
    "        metadata: Additional metadata to save\n",
    "    \n",
    "    Returns:\n",
    "        List of benchmark results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting vLLM-style TPU Benchmark\")\n",
    "    print(f\"📋 Configuration:\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Dataset: {dataset_name}\")\n",
    "    print(f\"   Requests: {num_prompts}\")\n",
    "    print(f\"   Concurrency: {max_concurrency}\")\n",
    "    print(f\"   Temperature: {temperature}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    if dataset_name == \"random\":\n",
    "        print(f\"   Input length: {random_input_len}\")\n",
    "        print(f\"   Output length: {random_output_len}\")\n",
    "        print(f\"   Range ratio: {random_range_ratio}\")\n",
    "        \n",
    "        dataset = RandomDataset(\n",
    "            input_len=random_input_len,\n",
    "            output_len=random_output_len,\n",
    "            num_requests=num_prompts,\n",
    "            range_ratio=random_range_ratio\n",
    "        )\n",
    "    elif dataset_name == \"sharegpt\":\n",
    "        print(f\"   Output length: {sharegpt_output_len}\")\n",
    "        \n",
    "        dataset = ShareGPTDataset(\n",
    "            output_len=sharegpt_output_len,\n",
    "            num_requests=num_prompts\n",
    "        )\n",
    "    elif dataset_name == \"sonnet\":\n",
    "        print(f\"   Input length: {sonnet_input_len}\")\n",
    "        print(f\"   Output length: {sonnet_output_len}\")\n",
    "        \n",
    "        dataset = SonnetDataset(\n",
    "            input_len=sonnet_input_len,\n",
    "            output_len=sonnet_output_len,\n",
    "            num_requests=num_prompts\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    # Generate requests\n",
    "    print(f\"\\n📝 Generating {num_prompts} requests...\")\n",
    "    requests = dataset.generate_requests()\n",
    "    \n",
    "    # Run benchmark\n",
    "    engine = TPUBenchmarkEngine(endpoint, use_dedicated_endpoint)\n",
    "    results = engine.run_benchmark(\n",
    "        requests=requests,\n",
    "        max_concurrency=max_concurrency,\n",
    "        temperature=temperature,\n",
    "        request_rate=request_rate\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analyzer = BenchmarkAnalyzer(results)\n",
    "    analyzer.print_summary()\n",
    "    \n",
    "    # Save results\n",
    "    if save_result:\n",
    "        filename_prefix = result_filename or f\"vllm_tpu_{dataset_name}_{model_name.replace('/', '_')}\"\n",
    "        \n",
    "        # Prepare metadata\n",
    "        benchmark_metadata = {\n",
    "            \"model\": model_name,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"num_prompts\": num_prompts,\n",
    "            \"max_concurrency\": max_concurrency,\n",
    "            \"request_rate\": request_rate,\n",
    "            \"temperature\": temperature,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        \n",
    "        if dataset_name == \"random\":\n",
    "            benchmark_metadata.update({\n",
    "                \"random_input_len\": random_input_len,\n",
    "                \"random_output_len\": random_output_len,\n",
    "                \"random_range_ratio\": random_range_ratio\n",
    "            })\n",
    "        elif dataset_name == \"sharegpt\":\n",
    "            benchmark_metadata[\"sharegpt_output_len\"] = sharegpt_output_len\n",
    "        elif dataset_name == \"sonnet\":\n",
    "            benchmark_metadata.update({\n",
    "                \"sonnet_input_len\": sonnet_input_len,\n",
    "                \"sonnet_output_len\": sonnet_output_len\n",
    "            })\n",
    "        \n",
    "        if metadata:\n",
    "            benchmark_metadata.update(metadata)\n",
    "        \n",
    "        analyzer.save_results(\n",
    "            output_dir=result_dir,\n",
    "            filename_prefix=filename_prefix,\n",
    "            metadata=benchmark_metadata\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with command line interface\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"vLLM-style benchmark for TPU endpoints\")\n",
    "    \n",
    "    # Dataset options\n",
    "    parser.add_argument(\"--dataset-name\", choices=[\"random\", \"sharegpt\", \"sonnet\"], \n",
    "                       default=\"random\", help=\"Dataset to benchmark on\")\n",
    "    parser.add_argument(\"--model\", required=True, help=\"Model name for identification\")\n",
    "    parser.add_argument(\"--num-prompts\", type=int, default=1000, \n",
    "                       help=\"Number of prompts to process\")\n",
    "    \n",
    "    # Request options\n",
    "    parser.add_argument(\"--max-concurrency\", type=int, default=100,\n",
    "                       help=\"Maximum number of concurrent requests\")\n",
    "    parser.add_argument(\"--request-rate\", type=float, default=float('inf'),\n",
    "                       help=\"Request rate in requests per second\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.7,\n",
    "                       help=\"Sampling temperature\")\n",
    "    \n",
    "    # Dataset-specific options\n",
    "    parser.add_argument(\"--random-input-len\", type=int, default=1024,\n",
    "                       help=\"Input length for random dataset\")\n",
    "    parser.add_argument(\"--random-output-len\", type=int, default=128,\n",
    "                       help=\"Output length for random dataset\")\n",
    "    parser.add_argument(\"--random-range-ratio\", type=float, default=0.0,\n",
    "                       help=\"Range ratio for random variance\")\n",
    "    parser.add_argument(\"--sharegpt-output-len\", type=int, default=128,\n",
    "                       help=\"Output length for ShareGPT dataset\")\n",
    "    parser.add_argument(\"--sonnet-input-len\", type=int, default=500,\n",
    "                       help=\"Input length for sonnet dataset\")\n",
    "    parser.add_argument(\"--sonnet-output-len\", type=int, default=300,\n",
    "                       help=\"Output length for sonnet dataset\")\n",
    "    \n",
    "    # Output options\n",
    "    parser.add_argument(\"--save-result\", action=\"store_true\",\n",
    "                       help=\"Save benchmark results\")\n",
    "    parser.add_argument(\"--result-dir\", default=\"benchmark_results\",\n",
    "                       help=\"Directory to save results\")\n",
    "    parser.add_argument(\"--result-filename\", default=None,\n",
    "                       help=\"Custom filename prefix for results\")\n",
    "    parser.add_argument(\"--metadata\", nargs=\"*\", metavar=\"KEY=VALUE\",\n",
    "                       help=\"Additional metadata (e.g., --metadata version=0.3.3 tp=1)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Parse metadata\n",
    "    metadata = {}\n",
    "    if args.metadata:\n",
    "        for item in args.metadata:\n",
    "            if \"=\" in item:\n",
    "                key, value = item.split(\"=\", 1)\n",
    "                metadata[key] = value\n",
    "    \n",
    "    print(\"⚠️  This benchmark requires your TPU endpoint to be configured.\")\n",
    "    print(\"   Please ensure you have:\")\n",
    "    print(\"   1. endpoint = aiplatform.Endpoint(aip_endpoint_name)\")\n",
    "    print(\"   2. use_dedicated_endpoint = True/False\")\n",
    "    print(\"   3. Then call: run_vllm_style_benchmark(endpoint, use_dedicated_endpoint, ...)\")\n",
    "    \n",
    "    return args\n",
    "\n",
    "# Preset benchmark configurations\n",
    "BENCHMARK_PRESETS = {\n",
    "    \"quick_test\": {\n",
    "        \"num_prompts\": 100,\n",
    "        \"max_concurrency\": 10,\n",
    "        \"random_input_len\": 512,\n",
    "        \"random_output_len\": 64,\n",
    "        \"description\": \"Quick test with small load\"\n",
    "    },\n",
    "    \"latency_test\": {\n",
    "        \"num_prompts\": 500,\n",
    "        \"max_concurrency\": 1,\n",
    "        \"random_input_len\": 1024,\n",
    "        \"random_output_len\": 128,\n",
    "        \"description\": \"Single-request latency measurement\"\n",
    "    },\n",
    "    \"throughput_test\": {\n",
    "        \"num_prompts\": 2000,\n",
    "        \"max_concurrency\": 200,\n",
    "        \"random_input_len\": 512,\n",
    "        \"random_output_len\": 128,\n",
    "        \"description\": \"High-throughput test\"\n",
    "    },\n",
    "    \"long_context\": {\n",
    "        \"num_prompts\": 100,\n",
    "        \"max_concurrency\": 20,\n",
    "        \"random_input_len\": 4096,\n",
    "        \"random_output_len\": 256,\n",
    "        \"description\": \"Long context processing test\"\n",
    "    },\n",
    "    \"conversation\": {\n",
    "        \"dataset_name\": \"sharegpt\",\n",
    "        \"num_prompts\": 1000,\n",
    "        \"max_concurrency\": 50,\n",
    "        \"sharegpt_output_len\": 200,\n",
    "        \"description\": \"Conversational AI test\"\n",
    "    },\n",
    "    \"creative_writing\": {\n",
    "        \"dataset_name\": \"sonnet\",\n",
    "        \"num_prompts\": 500,\n",
    "        \"max_concurrency\": 30,\n",
    "        \"sonnet_input_len\": 300,\n",
    "        \"sonnet_output_len\": 500,\n",
    "        \"description\": \"Creative writing test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_preset_benchmark(endpoint, \n",
    "                        use_dedicated_endpoint: bool,\n",
    "                        preset_name: str,\n",
    "                        model_name: str = \"unknown\",\n",
    "                        **kwargs) -> List[BenchmarkResult]:\n",
    "    \"\"\"Run a preset benchmark configuration\"\"\"\n",
    "    \n",
    "    if preset_name not in BENCHMARK_PRESETS:\n",
    "        available = \", \".join(BENCHMARK_PRESETS.keys())\n",
    "        raise ValueError(f\"Unknown preset '{preset_name}'. Available: {available}\")\n",
    "    \n",
    "    preset_config = BENCHMARK_PRESETS[preset_name].copy()\n",
    "    description = preset_config.pop(\"description\", \"\")\n",
    "    \n",
    "    # Override with any provided kwargs\n",
    "    preset_config.update(kwargs)\n",
    "    \n",
    "    print(f\"🎯 Running preset benchmark: {preset_name}\")\n",
    "    print(f\"📝 Description: {description}\")\n",
    "    \n",
    "    return run_vllm_style_benchmark(\n",
    "        endpoint=endpoint,\n",
    "        use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "        model_name=model_name,\n",
    "        **preset_config\n",
    "    )\n",
    "\n",
    "def run_comprehensive_benchmark_suite(endpoint,\n",
    "                                    use_dedicated_endpoint: bool,\n",
    "                                    model_name: str = \"unknown\",\n",
    "                                    save_results: bool = True) -> Dict[str, List[BenchmarkResult]]:\n",
    "    \"\"\"Run a comprehensive benchmark suite with multiple configurations\"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting Comprehensive TPU Benchmark Suite\")\n",
    "    print(f\"📋 Model: {model_name}\")\n",
    "    print(f\"🎯 Running {len(BENCHMARK_PRESETS)} preset configurations\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for preset_name in BENCHMARK_PRESETS.keys():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔄 Running preset: {preset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            results = run_preset_benchmark(\n",
    "                endpoint=endpoint,\n",
    "                use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "                preset_name=preset_name,\n",
    "                model_name=model_name,\n",
    "                save_result=save_results,\n",
    "                result_filename=f\"comprehensive_{preset_name}_{model_name.replace('/', '_')}\"\n",
    "            )\n",
    "            all_results[preset_name] = results\n",
    "            print(f\"✅ Completed preset: {preset_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed preset {preset_name}: {e}\")\n",
    "            all_results[preset_name] = []\n",
    "    \n",
    "    # Generate comparative report\n",
    "    if save_results:\n",
    "        generate_comparative_suite_report(all_results, model_name)\n",
    "    \n",
    "    print(f\"\\n🎉 Comprehensive benchmark suite completed!\")\n",
    "    print(f\"📊 Results available for {len([k for k, v in all_results.items() if v])} configurations\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def generate_comparative_suite_report(all_results: Dict[str, List[BenchmarkResult]], \n",
    "                                    model_name: str):\n",
    "    \"\"\"Generate a comparative report across all benchmark configurations\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"benchmark_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    report_file = os.path.join(output_dir, f\"comprehensive_report_{model_name.replace('/', '_')}_{timestamp}.md\")\n",
    "    \n",
    "    # Analyze each configuration\n",
    "    config_summaries = {}\n",
    "    for preset_name, results in all_results.items():\n",
    "        if results:\n",
    "            analyzer = BenchmarkAnalyzer(results)\n",
    "            config_summaries[preset_name] = analyzer.generate_summary()\n",
    "    \n",
    "    # Generate markdown report\n",
    "    md_content = f\"\"\"# Comprehensive TPU Benchmark Report\n",
    "\n",
    "**Model:** {model_name}  \n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Configurations Tested:** {len(config_summaries)}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents comprehensive benchmarking results across multiple test configurations for the TPU-hosted model.\n",
    "\n",
    "## Configuration Comparison\n",
    "\n",
    "| Configuration | Requests/s | Output tok/s | TTFT p95 (ms) | TPOT p95 (ms) | Success Rate |\n",
    "|---------------|------------|--------------|---------------|---------------|--------------|\n",
    "\"\"\"\n",
    "    \n",
    "    # Add comparison table rows\n",
    "    for preset_name, summary in config_summaries.items():\n",
    "        if \"error\" not in summary:\n",
    "            success_rate = summary['successful_requests'] / summary['total_requests'] * 100\n",
    "            md_content += f\"| {preset_name} | {summary['request_throughput']:.2f} | {summary['output_token_throughput']:.2f} | {summary['ttft_percentiles'][95]:.1f} | {summary['tpot_percentiles'][95]:.1f} | {success_rate:.1f}% |\\n\"\n",
    "    \n",
    "    md_content += f\"\"\"\n",
    "\n",
    "## Detailed Results by Configuration\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add detailed results for each configuration\n",
    "    for preset_name, summary in config_summaries.items():\n",
    "        preset_config = BENCHMARK_PRESETS[preset_name]\n",
    "        md_content += f\"\"\"\n",
    "### {preset_name.replace('_', ' ').title()}\n",
    "\n",
    "**Description:** {preset_config.get('description', 'N/A')}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        if \"error\" in summary:\n",
    "            md_content += f\"❌ **Status:** Failed - {summary['error']}\\n\\n\"\n",
    "            continue\n",
    "        \n",
    "        md_content += f\"\"\"**Status:** ✅ Successful\n",
    "\n",
    "**Request Statistics:**\n",
    "- Total requests: {summary['total_requests']}\n",
    "- Successful requests: {summary['successful_requests']}\n",
    "- Failed requests: {summary['failed_requests']}\n",
    "- Success rate: {summary['successful_requests'] / summary['total_requests'] * 100:.1f}%\n",
    "- Duration: {summary['benchmark_duration']:.2f}s\n",
    "\n",
    "**Throughput:**\n",
    "- Request throughput: {summary['request_throughput']:.2f} req/s\n",
    "- Input token throughput: {summary['input_token_throughput']:.2f} tok/s\n",
    "- Output token throughput: {summary['output_token_throughput']:.2f} tok/s\n",
    "\n",
    "**Latency (ms):**\n",
    "- TTFT p95: {summary['ttft_percentiles'][95]:.1f}ms\n",
    "- TPOT p95: {summary['tpot_percentiles'][95]:.1f}ms\n",
    "- ITL p95: {summary['itl_percentiles'][95]:.1f}ms\n",
    "- E2E p95: {summary['e2e_latency_percentiles'][95]:.1f}ms\n",
    "\n",
    "**Token Statistics:**\n",
    "- Total input tokens: {summary['total_input_tokens']:,}\n",
    "- Total output tokens: {summary['total_output_tokens']:,}\n",
    "- Avg input tokens/req: {summary['total_input_tokens'] / summary['successful_requests']:.1f}\n",
    "- Avg output tokens/req: {summary['total_output_tokens'] / summary['successful_requests']:.1f}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add recommendations\n",
    "    md_content += f\"\"\"\n",
    "## Performance Analysis & Recommendations\n",
    "\n",
    "### Best Performing Configurations\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Find best configurations by different metrics\n",
    "    if config_summaries:\n",
    "        # Best throughput\n",
    "        best_throughput = max(config_summaries.items(), \n",
    "                            key=lambda x: x[1].get('output_token_throughput', 0) if 'error' not in x[1] else 0)\n",
    "        \n",
    "        # Best latency\n",
    "        best_latency = min(config_summaries.items(),\n",
    "                          key=lambda x: x[1].get('ttft_percentiles', {}).get(95, float('inf')) if 'error' not in x[1] else float('inf'))\n",
    "        \n",
    "        md_content += f\"\"\"\n",
    "**🚀 Highest Throughput:** {best_throughput[0]} ({best_throughput[1].get('output_token_throughput', 0):.2f} tok/s)  \n",
    "**⚡ Lowest Latency:** {best_latency[0]} ({best_latency[1].get('ttft_percentiles', {}).get(95, 0):.1f}ms TTFT p95)\n",
    "\n",
    "### Use Case Recommendations\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Provide recommendations based on results\n",
    "        for preset_name, summary in config_summaries.items():\n",
    "            if \"error\" in summary:\n",
    "                continue\n",
    "                \n",
    "            preset_config = BENCHMARK_PRESETS[preset_name]\n",
    "            use_case = \"\"\n",
    "            \n",
    "            if preset_name == \"quick_test\":\n",
    "                use_case = \"Development and testing\"\n",
    "            elif preset_name == \"latency_test\":\n",
    "                use_case = \"Low-latency applications requiring fast response times\"\n",
    "            elif preset_name == \"throughput_test\":\n",
    "                use_case = \"High-volume batch processing\"\n",
    "            elif preset_name == \"long_context\":\n",
    "                use_case = \"Document analysis and long-form content processing\"\n",
    "            elif preset_name == \"conversation\":\n",
    "                use_case = \"Interactive chatbots and conversational AI\"\n",
    "            elif preset_name == \"creative_writing\":\n",
    "                use_case = \"Content generation and creative applications\"\n",
    "            \n",
    "            success_rate = summary['successful_requests'] / summary['total_requests'] * 100\n",
    "            suitability = \"✅ Excellent\" if success_rate > 95 and summary['output_token_throughput'] > 10 else \"⚠️ Good\" if success_rate > 90 else \"❌ Needs optimization\"\n",
    "            \n",
    "            md_content += f\"\"\"\n",
    "**{preset_name.replace('_', ' ').title()}:** {suitability}  \n",
    "*Use case:* {use_case}  \n",
    "*Performance:* {summary['output_token_throughput']:.1f} tok/s, {summary['ttft_percentiles'][95]:.1f}ms TTFT p95  \n",
    "\"\"\"\n",
    "    \n",
    "    md_content += f\"\"\"\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "**Model:** {model_name}  \n",
    "**Test Framework:** vLLM-style TPU Benchmark Suite  \n",
    "**Timestamp:** {timestamp}  \n",
    "\n",
    "---\n",
    "*Report generated automatically by TPU Benchmark Suite*\n",
    "\"\"\"\n",
    "    \n",
    "    # Save the report\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(md_content)\n",
    "    \n",
    "    print(f\"\\n📊 Comprehensive report saved: {report_file}\")\n",
    "\n",
    "# Example usage functions\n",
    "def benchmark_examples():\n",
    "    \"\"\"Show example usage patterns\"\"\"\n",
    "    print(\"\"\"\n",
    "🚀 vLLM-Style TPU Benchmark Examples\n",
    "\n",
    "# 1. Quick test (your existing endpoint setup)\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# 2. Run quick benchmark\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e_base\",\n",
    "    dataset_name=\"random\",\n",
    "    num_prompts=100,\n",
    "    max_concurrency=10,\n",
    "    random_input_len=512,\n",
    "    random_output_len=128\n",
    ")\n",
    "\n",
    "# 3. Run preset benchmarks\n",
    "results = run_preset_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    preset_name=\"throughput_test\",\n",
    "    model_name=\"llama3.3_tpuv6e_base\"\n",
    ")\n",
    "\n",
    "# 4. Run comprehensive suite\n",
    "all_results = run_comprehensive_benchmark_suite(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e_base\"\n",
    ")\n",
    "\n",
    "# 5. Custom configuration\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"your-model-name\",\n",
    "    dataset_name=\"sharegpt\",  # or \"sonnet\"\n",
    "    num_prompts=1000,\n",
    "    max_concurrency=50,\n",
    "    request_rate=10.0,  # 10 req/s instead of unlimited\n",
    "    temperature=0.7,\n",
    "    save_result=True,\n",
    "    metadata={\"version\": \"1.0\", \"notes\": \"production test\"}\n",
    ")\n",
    "\n",
    "Available presets: \"\"\" + \", \".join(BENCHMARK_PRESETS.keys()) + \"\"\"\n",
    "Available datasets: random, sharegpt, sonnet\n",
    "\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if we're in a notebook environment\n",
    "        get_ipython()\n",
    "        # If in notebook, show examples\n",
    "        benchmark_examples()\n",
    "    except NameError:\n",
    "        # If not in notebook, run CLI\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 500\n",
      "   Concurrency: 50\n",
      "   Temperature: 0.7\n",
      "   Input length: 80\n",
      "   Output length: 350\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 500 requests...\n",
      "Starting benchmark with 500 requests...\n",
      "Max concurrency: 50\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 500/500 [03:03<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 183.88 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 500\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 185.19s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 2.70 req/s\n",
      "   Input token throughput: 215.99 tok/s\n",
      "   Output token throughput: 1308.82 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT         34.5    43.1    48.8    73.6\n",
      "   ITL          34.5    43.1    48.8    73.6\n",
      "   E2E       18394.7 18547.7 18986.9 19043.9\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 40,000\n",
      "   Total output tokens: 242,385\n",
      "   Avg input tokens/req: 80.0\n",
      "   Avg output tokens/req: 484.8\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/vllm_tpu_random_your-model-name_summary_20250708_024154.json\n",
      "   Detailed: benchmark_results/vllm_tpu_random_your-model-name_detailed_20250708_024154.csv\n",
      "🎯 Running preset benchmark: throughput_test\n",
      "📝 Description: High-throughput test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 2000\n",
      "   Concurrency: 200\n",
      "   Temperature: 0.7\n",
      "   Input length: 512\n",
      "   Output length: 128\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 2000 requests...\n",
      "Starting benchmark with 2000 requests...\n",
      "Max concurrency: 200\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 2000/2000 [04:36<00:00,  7.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 276.89 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 2000\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 290.44s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 6.89 req/s\n",
      "   Input token throughput: 3525.64 tok/s\n",
      "   Output token throughput: 1583.21 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT        115.4   128.9   130.0   142.5\n",
      "   ITL         115.4   128.9   130.0   142.5\n",
      "   E2E       27177.6 30329.2 30343.4 32854.7\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 1,024,000\n",
      "   Total output tokens: 459,834\n",
      "   Avg input tokens/req: 512.0\n",
      "   Avg output tokens/req: 229.9\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/vllm_tpu_random_your-model-name_summary_20250708_024631.json\n",
      "   Detailed: benchmark_results/vllm_tpu_random_your-model-name_detailed_20250708_024631.csv\n",
      "🚀 Starting Comprehensive TPU Benchmark Suite\n",
      "📋 Model: your-model-name\n",
      "🎯 Running 6 preset configurations\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: quick_test\n",
      "============================================================\n",
      "🎯 Running preset benchmark: quick_test\n",
      "📝 Description: Quick test with small load\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 100\n",
      "   Concurrency: 10\n",
      "   Temperature: 0.7\n",
      "   Input length: 512\n",
      "   Output length: 64\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 100 requests...\n",
      "Starting benchmark with 100 requests...\n",
      "Max concurrency: 10\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 100/100 [00:36<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 36.49 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 100\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 36.52s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 2.74 req/s\n",
      "   Input token throughput: 1401.82 tok/s\n",
      "   Output token throughput: 404.53 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        364.4   366.0   366.7   367.6\n",
      "   TPOT         22.1    22.2    22.3    23.0\n",
      "   ITL          22.1    22.2    22.3    23.0\n",
      "   E2E        3644.2  3659.6  3667.1  3676.4\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 51,200\n",
      "   Total output tokens: 14,775\n",
      "   Avg input tokens/req: 512.0\n",
      "   Avg output tokens/req: 147.8\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_quick_test_your-model-name_summary_20250708_024708.json\n",
      "   Detailed: benchmark_results/comprehensive_quick_test_your-model-name_detailed_20250708_024708.csv\n",
      "✅ Completed preset: quick_test\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: latency_test\n",
      "============================================================\n",
      "🎯 Running preset benchmark: latency_test\n",
      "📝 Description: Single-request latency measurement\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 500\n",
      "   Concurrency: 1\n",
      "   Temperature: 0.7\n",
      "   Input length: 1024\n",
      "   Output length: 128\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 500 requests...\n",
      "Starting benchmark with 500 requests...\n",
      "Max concurrency: 1\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 500/500 [40:13<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 2413.36 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 500\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 2413.53s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 0.21 req/s\n",
      "   Input token throughput: 212.14 tok/s\n",
      "   Output token throughput: 47.46 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        484.6   489.1   490.3   492.8\n",
      "   TPOT         18.9    19.1    19.1    22.1\n",
      "   ITL          18.9    19.1    19.1    22.1\n",
      "   E2E        4846.5  4890.8  4902.7  4928.5\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 512,000\n",
      "   Total output tokens: 114,536\n",
      "   Avg input tokens/req: 1024.0\n",
      "   Avg output tokens/req: 229.1\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_latency_test_your-model-name_summary_20250708_032721.json\n",
      "   Detailed: benchmark_results/comprehensive_latency_test_your-model-name_detailed_20250708_032721.csv\n",
      "✅ Completed preset: latency_test\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: throughput_test\n",
      "============================================================\n",
      "🎯 Running preset benchmark: throughput_test\n",
      "📝 Description: High-throughput test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 2000\n",
      "   Concurrency: 200\n",
      "   Temperature: 0.7\n",
      "   Input length: 512\n",
      "   Output length: 128\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 2000 requests...\n",
      "Starting benchmark with 2000 requests...\n",
      "Max concurrency: 200\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 2000/2000 [04:31<00:00,  7.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 272.09 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 2000\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 286.17s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 6.99 req/s\n",
      "   Input token throughput: 3578.31 tok/s\n",
      "   Output token throughput: 1604.27 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT        115.3   117.2   131.2   143.5\n",
      "   ITL         115.3   117.2   131.2   143.5\n",
      "   E2E       27188.6 27552.0 29420.4 33229.0\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 1,024,000\n",
      "   Total output tokens: 459,092\n",
      "   Avg input tokens/req: 512.0\n",
      "   Avg output tokens/req: 229.5\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_throughput_test_your-model-name_summary_20250708_033154.json\n",
      "   Detailed: benchmark_results/comprehensive_throughput_test_your-model-name_detailed_20250708_033154.csv\n",
      "✅ Completed preset: throughput_test\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: long_context\n",
      "============================================================\n",
      "🎯 Running preset benchmark: long_context\n",
      "📝 Description: Long context processing test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 100\n",
      "   Concurrency: 20\n",
      "   Temperature: 0.7\n",
      "   Input length: 4096\n",
      "   Output length: 256\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 100 requests...\n",
      "Starting benchmark with 100 requests...\n",
      "Max concurrency: 20\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 100/100 [00:01<00:00, 83.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 1.23 seconds\n",
      "❌ Benchmark failed: No successful requests\n",
      "Total requests: 100\n",
      "Failed requests: 100\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_long_context_your-model-name_summary_20250708_033155.json\n",
      "   Detailed: benchmark_results/comprehensive_long_context_your-model-name_detailed_20250708_033155.csv\n",
      "✅ Completed preset: long_context\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: conversation\n",
      "============================================================\n",
      "🎯 Running preset benchmark: conversation\n",
      "📝 Description: Conversational AI test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: sharegpt\n",
      "   Requests: 1000\n",
      "   Concurrency: 50\n",
      "   Temperature: 0.7\n",
      "   Output length: 200\n",
      "\n",
      "📝 Generating 1000 requests...\n",
      "Starting benchmark with 1000 requests...\n",
      "Max concurrency: 50\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 1000/1000 [03:47<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 228.05 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 1000\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 228.40s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 4.38 req/s\n",
      "   Input token throughput: 69.09 tok/s\n",
      "   Output token throughput: 1074.59 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT         44.1    48.6    50.6    55.4\n",
      "   ITL          44.1    48.6    50.6    55.4\n",
      "   E2E       11391.3 11451.7 11470.6 11484.8\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 15,781\n",
      "   Total output tokens: 245,437\n",
      "   Avg input tokens/req: 15.8\n",
      "   Avg output tokens/req: 245.4\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_conversation_your-model-name_summary_20250708_033543.json\n",
      "   Detailed: benchmark_results/comprehensive_conversation_your-model-name_detailed_20250708_033543.csv\n",
      "✅ Completed preset: conversation\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: creative_writing\n",
      "============================================================\n",
      "🎯 Running preset benchmark: creative_writing\n",
      "📝 Description: Creative writing test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: sonnet\n",
      "   Requests: 500\n",
      "   Concurrency: 30\n",
      "   Temperature: 0.7\n",
      "   Input length: 300\n",
      "   Output length: 500\n",
      "\n",
      "📝 Generating 500 requests...\n",
      "Starting benchmark with 500 requests...\n",
      "Max concurrency: 30\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 500/500 [05:37<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 337.76 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 500\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 339.94s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 1.47 req/s\n",
      "   Input token throughput: 441.25 tok/s\n",
      "   Output token throughput: 895.52 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT         31.4    33.7    34.3    36.3\n",
      "   ITL          31.4    33.7    34.3    36.3\n",
      "   E2E       19995.0 20089.9 20115.1 20153.7\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 150,000\n",
      "   Total output tokens: 304,426\n",
      "   Avg input tokens/req: 300.0\n",
      "   Avg output tokens/req: 608.9\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_creative_writing_your-model-name_summary_20250708_034121.json\n",
      "   Detailed: benchmark_results/comprehensive_creative_writing_your-model-name_detailed_20250708_034121.csv\n",
      "✅ Completed preset: creative_writing\n",
      "\n",
      "📊 Comprehensive report saved: benchmark_results/comprehensive_report_your-model-name_20250708_034121.md\n",
      "\n",
      "🎉 Comprehensive benchmark suite completed!\n",
      "📊 Results available for 6 configurations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Your existing endpoint setup\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# 1. Quick test (like your use cases)\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    dataset_name=\"random\",\n",
    "    num_prompts=500,\n",
    "    max_concurrency=50,\n",
    "    random_input_len=80,      # Email use case\n",
    "    random_output_len=350,\n",
    "    save_result=True\n",
    ")\n",
    "\n",
    "# 2. Run preset configurations\n",
    "results = run_preset_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    preset_name=\"throughput_test\",\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")\n",
    "\n",
    "# 3. Comprehensive suite (all configurations)\n",
    "all_results = run_comprehensive_benchmark_suite(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 250 concurrency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 500\n",
      "   Concurrency: 250\n",
      "   Temperature: 0.7\n",
      "   Input length: 80\n",
      "   Output length: 350\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 500 requests...\n",
      "Starting benchmark with 500 requests...\n",
      "Max concurrency: 250\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 500/500 [01:58<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 119.84 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 500\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 121.14s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 4.13 req/s\n",
      "   Input token throughput: 330.20 tok/s\n",
      "   Output token throughput: 2003.58 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT        114.8   142.6   185.0   251.0\n",
      "   ITL         114.8   142.6   185.0   251.0\n",
      "   E2E       59790.7 60596.3 60615.0 60643.6\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 40,000\n",
      "   Total output tokens: 242,713\n",
      "   Avg input tokens/req: 80.0\n",
      "   Avg output tokens/req: 485.4\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/vllm_tpu_random_your-model-name_summary_20250708_034321.json\n",
      "   Detailed: benchmark_results/vllm_tpu_random_your-model-name_detailed_20250708_034321.csv\n",
      "🎯 Running preset benchmark: throughput_test\n",
      "📝 Description: High-throughput test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 2000\n",
      "   Concurrency: 200\n",
      "   Temperature: 0.7\n",
      "   Input length: 512\n",
      "   Output length: 128\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 2000 requests...\n",
      "Starting benchmark with 2000 requests...\n",
      "Max concurrency: 200\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 2000/2000 [04:32<00:00,  7.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 272.79 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 2000\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 288.80s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 6.93 req/s\n",
      "   Input token throughput: 3545.69 tok/s\n",
      "   Output token throughput: 1591.11 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT        114.6   123.7   136.5   151.0\n",
      "   ITL         114.6   123.7   136.5   151.0\n",
      "   E2E       27014.2 27453.9 31341.4 35173.8\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 1,024,000\n",
      "   Total output tokens: 459,515\n",
      "   Avg input tokens/req: 512.0\n",
      "   Avg output tokens/req: 229.8\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/vllm_tpu_random_your-model-name_summary_20250708_034754.json\n",
      "   Detailed: benchmark_results/vllm_tpu_random_your-model-name_detailed_20250708_034754.csv\n",
      "🚀 Starting Comprehensive TPU Benchmark Suite\n",
      "📋 Model: your-model-name\n",
      "🎯 Running 6 preset configurations\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: quick_test\n",
      "============================================================\n",
      "🎯 Running preset benchmark: quick_test\n",
      "📝 Description: Quick test with small load\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 100\n",
      "   Concurrency: 10\n",
      "   Temperature: 0.7\n",
      "   Input length: 512\n",
      "   Output length: 64\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 100 requests...\n",
      "Starting benchmark with 100 requests...\n",
      "Max concurrency: 10\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 36.34 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 100\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 36.37s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 2.75 req/s\n",
      "   Input token throughput: 1407.64 tok/s\n",
      "   Output token throughput: 405.71 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        362.9   365.5   366.2   366.6\n",
      "   TPOT         22.0    22.2    22.3    24.2\n",
      "   ITL          22.0    22.2    22.3    24.2\n",
      "   E2E        3629.0  3654.8  3661.8  3665.9\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 51,200\n",
      "   Total output tokens: 14,757\n",
      "   Avg input tokens/req: 512.0\n",
      "   Avg output tokens/req: 147.6\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_quick_test_your-model-name_summary_20250708_034831.json\n",
      "   Detailed: benchmark_results/comprehensive_quick_test_your-model-name_detailed_20250708_034831.csv\n",
      "✅ Completed preset: quick_test\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: latency_test\n",
      "============================================================\n",
      "🎯 Running preset benchmark: latency_test\n",
      "📝 Description: Single-request latency measurement\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 500\n",
      "   Concurrency: 1\n",
      "   Temperature: 0.7\n",
      "   Input length: 1024\n",
      "   Output length: 128\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 500 requests...\n",
      "Starting benchmark with 500 requests...\n",
      "Max concurrency: 1\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 500/500 [40:14<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 2414.23 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 500\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 2414.41s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 0.21 req/s\n",
      "   Input token throughput: 212.06 tok/s\n",
      "   Output token throughput: 47.48 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        484.7   488.8   490.1   492.2\n",
      "   TPOT         18.9    19.1    19.1    21.7\n",
      "   ITL          18.9    19.1    19.1    21.7\n",
      "   E2E        4846.7  4887.6  4900.8  4922.1\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 512,000\n",
      "   Total output tokens: 114,628\n",
      "   Avg input tokens/req: 1024.0\n",
      "   Avg output tokens/req: 229.3\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_latency_test_your-model-name_summary_20250708_042845.json\n",
      "   Detailed: benchmark_results/comprehensive_latency_test_your-model-name_detailed_20250708_042845.csv\n",
      "✅ Completed preset: latency_test\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: throughput_test\n",
      "============================================================\n",
      "🎯 Running preset benchmark: throughput_test\n",
      "📝 Description: High-throughput test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 2000\n",
      "   Concurrency: 200\n",
      "   Temperature: 0.7\n",
      "   Input length: 512\n",
      "   Output length: 128\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 2000 requests...\n",
      "Starting benchmark with 2000 requests...\n",
      "Max concurrency: 200\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 2000/2000 [04:31<00:00,  7.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 271.91 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 1991\n",
      "   Failed requests: 9\n",
      "   Success rate: 99.6%\n",
      "   Benchmark duration: 285.10s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 6.98 req/s\n",
      "   Input token throughput: 3575.52 tok/s\n",
      "   Output token throughput: 1605.87 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT        115.0   117.2   129.8   141.8\n",
      "   ITL         115.0   117.2   129.8   141.8\n",
      "   E2E       27105.1 27474.1 29262.5 33069.4\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 1,019,392\n",
      "   Total output tokens: 457,839\n",
      "   Avg input tokens/req: 512.0\n",
      "   Avg output tokens/req: 230.0\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_throughput_test_your-model-name_summary_20250708_043317.json\n",
      "   Detailed: benchmark_results/comprehensive_throughput_test_your-model-name_detailed_20250708_043317.csv\n",
      "✅ Completed preset: throughput_test\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: long_context\n",
      "============================================================\n",
      "🎯 Running preset benchmark: long_context\n",
      "📝 Description: Long context processing test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: random\n",
      "   Requests: 100\n",
      "   Concurrency: 20\n",
      "   Temperature: 0.7\n",
      "   Input length: 4096\n",
      "   Output length: 256\n",
      "   Range ratio: 0.0\n",
      "\n",
      "📝 Generating 100 requests...\n",
      "Starting benchmark with 100 requests...\n",
      "Max concurrency: 20\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 100/100 [00:01<00:00, 74.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 1.37 seconds\n",
      "❌ Benchmark failed: No successful requests\n",
      "Total requests: 100\n",
      "Failed requests: 100\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_long_context_your-model-name_summary_20250708_043319.json\n",
      "   Detailed: benchmark_results/comprehensive_long_context_your-model-name_detailed_20250708_043319.csv\n",
      "✅ Completed preset: long_context\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: conversation\n",
      "============================================================\n",
      "🎯 Running preset benchmark: conversation\n",
      "📝 Description: Conversational AI test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: sharegpt\n",
      "   Requests: 1000\n",
      "   Concurrency: 50\n",
      "   Temperature: 0.7\n",
      "   Output length: 200\n",
      "\n",
      "📝 Generating 1000 requests...\n",
      "Starting benchmark with 1000 requests...\n",
      "Max concurrency: 50\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 1000/1000 [03:47<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 227.42 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 1000\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 227.99s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 4.39 req/s\n",
      "   Input token throughput: 68.95 tok/s\n",
      "   Output token throughput: 1074.08 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT         43.7    48.7    50.4    59.9\n",
      "   ITL          43.7    48.7    50.4    59.9\n",
      "   E2E       11344.9 11441.4 11493.3 11535.7\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 15,719\n",
      "   Total output tokens: 244,875\n",
      "   Avg input tokens/req: 15.7\n",
      "   Avg output tokens/req: 244.9\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_conversation_your-model-name_summary_20250708_043707.json\n",
      "   Detailed: benchmark_results/comprehensive_conversation_your-model-name_detailed_20250708_043707.csv\n",
      "✅ Completed preset: conversation\n",
      "\n",
      "============================================================\n",
      "🔄 Running preset: creative_writing\n",
      "============================================================\n",
      "🎯 Running preset benchmark: creative_writing\n",
      "📝 Description: Creative writing test\n",
      "🚀 Starting vLLM-style TPU Benchmark\n",
      "📋 Configuration:\n",
      "   Model: your-model-name\n",
      "   Dataset: sonnet\n",
      "   Requests: 500\n",
      "   Concurrency: 30\n",
      "   Temperature: 0.7\n",
      "   Input length: 300\n",
      "   Output length: 500\n",
      "\n",
      "📝 Generating 500 requests...\n",
      "Starting benchmark with 500 requests...\n",
      "Max concurrency: 30\n",
      "Temperature: 0.7\n",
      "Request rate: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing requests: 100%|██████████| 500/500 [05:38<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed in 338.62 seconds\n",
      "\n",
      "==================================================\n",
      "📊 BENCHMARK RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "📈 Request Statistics:\n",
      "   Successful requests: 500\n",
      "   Failed requests: 0\n",
      "   Success rate: 100.0%\n",
      "   Benchmark duration: 340.31s\n",
      "\n",
      "🚀 Throughput:\n",
      "   Request throughput: 1.47 req/s\n",
      "   Input token throughput: 440.78 tok/s\n",
      "   Output token throughput: 894.93 tok/s\n",
      "\n",
      "⏱️ Latency Metrics (ms):\n",
      "   Metric        p50     p90     p95     p99\n",
      "   ---------------------------------------------\n",
      "   TTFT        500.0   500.0   500.0   500.0\n",
      "   TPOT         31.4    34.1    34.9    38.4\n",
      "   ITL          31.4    34.1    34.9    38.4\n",
      "   E2E       20024.0 20086.8 20117.2 20127.6\n",
      "\n",
      "📊 Token Statistics:\n",
      "   Total input tokens: 150,000\n",
      "   Total output tokens: 304,550\n",
      "   Avg input tokens/req: 300.0\n",
      "   Avg output tokens/req: 609.1\n",
      "\n",
      "💾 Results saved:\n",
      "   Summary: benchmark_results/comprehensive_creative_writing_your-model-name_summary_20250708_044245.json\n",
      "   Detailed: benchmark_results/comprehensive_creative_writing_your-model-name_detailed_20250708_044245.csv\n",
      "✅ Completed preset: creative_writing\n",
      "\n",
      "📊 Comprehensive report saved: benchmark_results/comprehensive_report_your-model-name_20250708_044245.md\n",
      "\n",
      "🎉 Comprehensive benchmark suite completed!\n",
      "📊 Results available for 6 configurations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Your existing endpoint setup\n",
    "endpoint_name = \"1029620071644790784\"\n",
    "aip_endpoint_name = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "\n",
    "# 1. Quick test (like your use cases)\n",
    "results = run_vllm_style_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\",\n",
    "    dataset_name=\"random\",\n",
    "    num_prompts=500,\n",
    "    max_concurrency=250,\n",
    "    random_input_len=80,      # Email use case\n",
    "    random_output_len=350,\n",
    "    save_result=True\n",
    ")\n",
    "\n",
    "# 2. Run preset configurations\n",
    "results = run_preset_benchmark(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    preset_name=\"throughput_test\",\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")\n",
    "\n",
    "# 3. Comprehensive suite (all configurations)\n",
    "all_results = run_comprehensive_benchmark_suite(\n",
    "    endpoint=endpoint,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    model_name=\"llama3.3_tpuv6e\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxaKC69ypQds"
   },
   "source": [
    "## Clean up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "HYNmBz8PdcvJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # @title Delete the models and endpoints\n",
    "# # @markdown  Delete the experiment models and endpoints to recycle the resources\n",
    "# # @markdown  and avoid unnecessary continuous charges that may incur.\n",
    "\n",
    "# # Undeploy model and delete endpoint.\n",
    "# for endpoint in endpoints.values():\n",
    "#     endpoint.delete(force=True)\n",
    "\n",
    "# # Delete models.\n",
    "# for model in models.values():\n",
    "#     model.delete()\n",
    "\n",
    "# delete_bucket = F   # @param {type:\"boolean\"}\n",
    "# if delete_bucket:\n",
    "#     ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_1_qwen3_deployment_tpu.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
